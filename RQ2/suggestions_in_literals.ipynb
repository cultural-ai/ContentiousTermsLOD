{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from thefuzz import process\n",
    "import itertools\n",
    "from itertools import groupby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# suggestion can be found in the same properties as query terms\n",
    "# {\"suggestions\":{\"pref\": [('intersex', 90)] }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_suggestions(resource:str, lang:str, resource_file_path:str) -> dict:\n",
    "    '''\n",
    "    Finding suggestions from WM in the literal values of entities based on fuzzy string matching (thefuzz library);\n",
    "    there can be an exact match (score = 100) or near match (score > 90), matches with lower scores are not included;\n",
    "    \n",
    "    resource: str, 'wikidata', 'aat', 'pwn', or 'odwn'\n",
    "    lang: str, 'en' or 'nl'\n",
    "    resource_file_path: str, path to the file with query results of a resoure (related matches, subset, all results)\n",
    "    Returns dict per resource per lang adding the found suggestions to every hit:\n",
    "        for example, {'suggestions':{'prefLabel':[(\"match\",100)]}};\n",
    "    '''\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # load suggestions per lang\n",
    "    if lang == \"en\":\n",
    "        # change path\n",
    "        with open('/Users/anesterov/reps/LODlit/RQ2/en_suggestions.json','r') as jf:\n",
    "            suggestions_dict = json.load(jf)\n",
    "    if lang == \"nl\":\n",
    "        with open('/Users/anesterov/reps/LODlit/RQ2/nl_suggestions.json','r') as jf:\n",
    "            suggestions_dict = json.load(jf)\n",
    "    \n",
    "    # load the resource data\n",
    "    with open(resource_file_path,'r') as jf:\n",
    "        resource_data = json.load(jf)\n",
    "\n",
    "    for term, hits in resource_data.items():\n",
    "        hits_with_suggestions = []\n",
    "        \n",
    "        for hit in hits:\n",
    "            # searhing suggestions in literals of every hit\n",
    "            # this will allow to compare in which properties the query terms and suggestions were found\n",
    "            suggestions = {}\n",
    "\n",
    "            # check resource\n",
    "            # AAT\n",
    "            if resource == \"aat\":\n",
    "\n",
    "                # prefLabel\n",
    "                # filtering matches by scores\n",
    "                # excluding suggestions that are equal to query terms\n",
    "                matches_pref = [tup for tup in process.extract(hit['prefLabel'], suggestions_dict[term]) if tup[1] > 90\\\n",
    "                                and tup[0].lower() != hit[\"query_term\"]]\n",
    "\n",
    "                # altLabels\n",
    "                matches_alt = []\n",
    "                for alt in hit[\"altLabel\"]:\n",
    "                    matches_alt.extend([tup for tup in process.extract(alt, suggestions_dict[term]) if tup[1] > 90 \\\n",
    "                                       and tup[0].lower() != hit[\"query_term\"]])\n",
    "\n",
    "                # scopeNotes\n",
    "                matches_scopeNote = [tup for tup in process.extract(hit[\"scopeNote\"], suggestions_dict[term]) if tup[1] > 90 \\\n",
    "                                    and tup[0].lower() != hit[\"query_term\"]]\n",
    "\n",
    "                # prefLabel comment\n",
    "                matches_pref_comment = [tup for tup in process.extract(hit['prefLabel_comment'], suggestions_dict[term]) if tup[1] > 90 \\\n",
    "                                       and tup[0].lower() != hit[\"query_term\"]]\n",
    "                \n",
    "                # altLabels comments\n",
    "                matches_alt_comment = []\n",
    "                for alt_comment in hit[\"altLabel_comment\"]:\n",
    "                    matches_alt_comment.extend([tup for tup in process.extract(alt_comment, suggestions_dict[term]) if tup[1] > 90 \\\n",
    "                                               and tup[0].lower() != hit[\"query_term\"]])\n",
    "                    \n",
    "                # group by suggestions, take unique sugestions with max score\n",
    "                suggestions['prefLabel'] = [max(score) for l,score in itertools.groupby(sorted(matches_pref), key=lambda t: t[0])]\n",
    "                suggestions['altLabel'] = [max(score) for l,score in itertools.groupby(sorted(matches_alt), key=lambda t: t[0])]\n",
    "                suggestions['scopeNote'] = [max(score) for l,score in itertools.groupby(sorted(matches_scopeNote), key=lambda t: t[0])]\n",
    "                suggestions['prefLabel_comment'] = [max(score) for l,score in itertools.groupby(sorted(matches_pref_comment), key=lambda t: t[0])]\n",
    "                suggestions['altLabel_comment'] = [max(score) for l,score in itertools.groupby(sorted(matches_alt_comment), key=lambda t: t[0])]\n",
    "\n",
    "                hit[\"suggestions\"] = suggestions\n",
    "\n",
    "                # if there are suggestions found, append it\n",
    "                if len(matches_pref) > 0 or len(matches_alt) > 0 or len(matches_scopeNote) > 0 \\\n",
    "                or len(matches_pref_comment) > 0 or len(matches_alt_comment) > 0:\n",
    "                    hits_with_suggestions.append(hit)\n",
    "            \n",
    "            # Wikidata        \n",
    "            if resource == \"wd\":\n",
    "                # prefLabel\n",
    "                if type(hit['prefLabel']) == str:\n",
    "                    matches_pref = [tup for tup in process.extract(hit['prefLabel'], suggestions_dict[term]) if tup[1] > 90 \\\n",
    "                                   and tup[0].lower() != hit[\"query_term\"]]\n",
    "\n",
    "                # aliases\n",
    "                matches_alt = []\n",
    "                if hit[\"aliases\"]:\n",
    "                    for alt in hit[\"aliases\"]:\n",
    "                        matches_alt.extend([tup for tup in process.extract(alt, suggestions_dict[term]) if tup[1] > 90 \\\n",
    "                                           and tup[0].lower() != hit[\"query_term\"]])\n",
    "\n",
    "                # description\n",
    "                matches_desc = []\n",
    "                # check desc type\n",
    "                if type(hit[\"description\"]) == list:\n",
    "                    for d in hit[\"description\"]:\n",
    "                        matches_desc.extend([tup for tup in process.extract(d, suggestions_dict[term]) if tup[1] > 90 \\\n",
    "                                            and tup[0].lower() != hit[\"query_term\"]])\n",
    "                if type(hit[\"description\"]) == str:\n",
    "                    matches_desc = [tup for tup in process.extract(hit[\"description\"], suggestions_dict[term]) if tup[1] > 90 \\\n",
    "                                   and tup[0].lower() != hit[\"query_term\"]]\n",
    "                    \n",
    "                # group by suggestions, take unique sugestions with max score\n",
    "                suggestions['prefLabel'] = [max(score) for l,score in itertools.groupby(sorted(matches_pref), key=lambda t: t[0])]\n",
    "                suggestions['aliases'] = [max(score) for l,score in itertools.groupby(sorted(matches_alt), key=lambda t: t[0])]\n",
    "                suggestions['description'] = [max(score) for l,score in itertools.groupby(sorted(matches_desc), key=lambda t: t[0])]\n",
    "                \n",
    "                hit[\"suggestions\"] = suggestions\n",
    "                \n",
    "                # if there are suggestions found, append it\n",
    "                if len(matches_pref) > 0 or len(matches_alt) > 0 or len(matches_desc) > 0:\n",
    "                    hits_with_suggestions.append(hit)\n",
    "            \n",
    "            # PWN\n",
    "            if resource == \"pwn\":\n",
    "                # lemmata\n",
    "                matches_lemma = []\n",
    "                for l in hit[\"lemmata\"]:\n",
    "                    matches_lemma.extend([tup for tup in process.extract(l, suggestions_dict[term]) if tup[1] > 90 \\\n",
    "                                         and tup[0].lower() != hit[\"query_term\"]])\n",
    "                    \n",
    "                # definitions\n",
    "                matches_def = [tup for tup in process.extract(hit['definition'], suggestions_dict[term]) if tup[1] > 90 \\\n",
    "                              and tup[0].lower() != hit[\"query_term\"]]\n",
    "                \n",
    "                # examples\n",
    "                matches_ex = []\n",
    "                for e in hit[\"examples\"]:\n",
    "                    matches_ex.extend([tup for tup in process.extract(e, suggestions_dict[term]) if tup[1] > 90 \\\n",
    "                                      and tup[0].lower() != hit[\"query_term\"]])\n",
    "                    \n",
    "                # group by suggestions, take unique sugestions with max score\n",
    "                suggestions['lemmata'] = [max(score) for l,score in itertools.groupby(sorted(matches_lemma), key=lambda t: t[0])]\n",
    "                suggestions['definition'] = [max(score) for l,score in itertools.groupby(sorted(matches_def), key=lambda t: t[0])]\n",
    "                suggestions['examples'] = [max(score) for l,score in itertools.groupby(sorted(matches_ex), key=lambda t: t[0])]\n",
    "                \n",
    "                hit[\"suggestions\"] = suggestions\n",
    "                \n",
    "                # if there are suggestions found, append it\n",
    "                if len(matches_lemma) > 0 or len(matches_def) > 0 or len(matches_ex) > 0:\n",
    "                    hits_with_suggestions.append(hit)\n",
    "                    \n",
    "            # ODWN\n",
    "            if resource == \"odwn\":\n",
    "                # le_written_form\n",
    "                # optional\n",
    "                matches_le = []\n",
    "                if hit.get('le_written_form'):\n",
    "                    matches_le = [tup for tup in process.extract(hit['le_written_form'], suggestions_dict[term]) if tup[1] > 90 \\\n",
    "                                 and tup[0].lower() != hit[\"query_term\"]]\n",
    "                \n",
    "                # sense_definition\n",
    "                # optional\n",
    "                matches_sense_def = []\n",
    "                if hit.get('sense_definition'):\n",
    "                    matches_sense_def = [tup for tup in process.extract(hit['sense_definition'], suggestions_dict[term]) if tup[1] > 90 \\\n",
    "                                        and tup[0].lower() != hit[\"query_term\"]]\n",
    "                \n",
    "                # synset def\n",
    "                matches_synset_def = []\n",
    "                for synset_def in hit['synset_definitions']:\n",
    "                    matches_synset_def.extend([tup for tup in process.extract(synset_def, suggestions_dict[term]) if tup[1] > 90 \\\n",
    "                                              and tup[0].lower() != hit[\"query_term\"]])\n",
    "            \n",
    "                # sense ex\n",
    "                # optional\n",
    "                matches_sense_ex = []\n",
    "                if hit.get(\"sense_examples\"):\n",
    "                    for sense_ex in hit[\"sense_examples\"]:\n",
    "                        matches_sense_ex.extend([tup for tup in process.extract(sense_ex, suggestions_dict[term]) if tup[1] > 90 \\\n",
    "                                                and tup[0].lower() != hit[\"query_term\"]])\n",
    "                \n",
    "                # group by suggestions, take unique sugestions with max score\n",
    "                suggestions['le_written_form'] = [max(score) for l,score in itertools.groupby(sorted(matches_le), key=lambda t: t[0])]\n",
    "                suggestions['sense_definition'] = [max(score) for l,score in itertools.groupby(sorted(matches_sense_def), key=lambda t: t[0])]\n",
    "                suggestions['synset_definitions'] = [max(score) for l,score in itertools.groupby(sorted(matches_synset_def), key=lambda t: t[0])]\n",
    "                suggestions['sense_examples'] = [max(score) for l,score in itertools.groupby(sorted(matches_sense_ex), key=lambda t: t[0])]\n",
    "\n",
    "                \n",
    "                hit[\"suggestions\"] = suggestions\n",
    "                \n",
    "                # if there are suggestions found, append it\n",
    "                if len(matches_le) > 0 or len(matches_sense_def) > 0 or len(matches_synset_def) > 0 \\\n",
    "                or len(matches_sense_ex) > 0:\n",
    "                    hits_with_suggestions.append(hit)\n",
    "                \n",
    "        results[term] = hits_with_suggestions\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sug_all_wd_nl = find_suggestions('wd','nl','/Users/anesterov/LODlit_local/wd/jan31/results_clean_nl.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exporting the json file\n",
    "with open('sug_all_wd_nl.json', 'w') as jf:\n",
    "    json.dump(sug_all_wd_nl, jf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Suggestions stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# related matches\n",
    "with open('/Users/anesterov/reps/LODlit/RQ2/sug_rm_wd_en.json','r') as jf:\n",
    "    sug_rm_wd_en = json.load(jf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_suggestions_count(resource:str, path_to_dataset:str) -> dict:\n",
    "    '''\n",
    "    Counting N of entities and hits with exact and near match suggestions\n",
    "    resource: str, \"wd\", \"aat\", \"pwn\", or \"odwn\"\n",
    "    path_to_dataset: str, path to a file with hits\n",
    "    Returns dict\n",
    "    '''\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    with open(path_to_dataset,'r') as jf:\n",
    "        data = json.load(jf)\n",
    "        \n",
    "    # check resource\n",
    "    if resource == \"wd\":\n",
    "        entity_id_key = \"QID\"\n",
    "    if resource == \"aat\":\n",
    "        entity_id_key = \"aat_uri\"\n",
    "    if resource == \"pwn\":\n",
    "        entity_id_key = \"synset_id\"\n",
    "        \n",
    "    # how many exact match suggestions?\n",
    "    entities_exact = []\n",
    "    entities_near = []\n",
    "    hits_exact = 0\n",
    "    hits_near = 0\n",
    "\n",
    "    for term, sug_hits in data.items():\n",
    "        for hit in sug_hits:\n",
    "            for prop, matches in hit[\"suggestions\"].items():\n",
    "                for m in matches:\n",
    "                    if m[1] == 100:\n",
    "                        hits_exact += 1\n",
    "                        entities_exact.append(hit)\n",
    "                    else:\n",
    "                        hits_near += 1\n",
    "                        entities_near.append(hit)\n",
    "    # ODWN\n",
    "    if resource == \"odwn\":\n",
    "        n_entities_exact_d = []\n",
    "        for hit in entities_exact:\n",
    "            if hit.get(\"synset_id\"):\n",
    "                n_entities_exact_d.append(hit[\"synset_id\"])\n",
    "            else:\n",
    "                n_entities_exact_d.append(hit[\"le_id\"])\n",
    "        n_entities_exact = len(set(n_entities_exact_d))\n",
    "\n",
    "        n_entities_near_d = []\n",
    "        for hit in entities_near:\n",
    "            if hit.get(\"synset_id\"):\n",
    "                n_entities_near_d.append(hit[\"synset_id\"])\n",
    "            else:\n",
    "                n_entities_near_d.append(hit[\"le_id\"])\n",
    "        n_entities_near = len(set(n_entities_near_d))\n",
    "        \n",
    "    else:\n",
    "        n_entities_exact = len(set([hit[entity_id_key] for hit in entities_exact]))\n",
    "        n_entities_near = len(set([hit[entity_id_key] for hit in entities_near]))\n",
    "\n",
    "    results[\"n_entities_exact\"] = n_entities_exact\n",
    "    results[\"n_entities_near\"] = n_entities_near\n",
    "    results[\"hits_exact\"] = hits_exact\n",
    "    results[\"hits_near\"] = hits_near\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_suggestions_count(\"odwn\",\"/Users/anesterov/reps/LODlit/RQ2/sug_all_odwn.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check unique entities with suggestions in the subset and all search results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_entities_with_suggestions(resource:str,lang:str,to_compare:tuple) -> dict:\n",
    "    '''\n",
    "    to_compare: tuple; (\"rm\",\"subset\") or (\"subset\",\"all\")\n",
    "    '''\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    lang = \"_\" + lang\n",
    "    if resource == \"odwn\" or resource == \"pwn\":\n",
    "        lang = ''\n",
    "    \n",
    "    with open(f\"/Users/anesterov/reps/LODlit/RQ2/sug_{to_compare[0]}_{resource}{lang}.json\",'r') as jf:\n",
    "        resource_1 = json.load(jf)\n",
    "        \n",
    "    with open(f\"/Users/anesterov/reps/LODlit/RQ2/sug_{to_compare[1]}_{resource}{lang}.json\",'r') as jf:\n",
    "        resource_2 = json.load(jf)\n",
    "        \n",
    "    # check resource\n",
    "    # special conditions for ODWN\n",
    "    if resource == \"odwn\":\n",
    "        e_with_sug_1 = []\n",
    "        e_with_sug_2 = []\n",
    "        \n",
    "        for term, sug_hits in resource_1.items():\n",
    "            for hit in sug_hits:\n",
    "                if hit.get(\"synset_id\"):\n",
    "                    e_with_sug_1.append(hit[\"synset_id\"])\n",
    "                else:\n",
    "                    e_with_sug_1.append(hit[\"le_id\"])\n",
    "\n",
    "        for term, sug_hits in resource_2.items():\n",
    "            for hit in sug_hits:\n",
    "                if hit.get(\"synset_id\"):\n",
    "                    e_with_sug_2.append(hit[\"synset_id\"])\n",
    "                else:\n",
    "                    e_with_sug_2.append(hit[\"le_id\"])\n",
    "    else:\n",
    "    \n",
    "        if resource == \"wd\":\n",
    "            entity_id_key = \"QID\"\n",
    "        if resource == \"aat\":\n",
    "            entity_id_key = \"aat_uri\"\n",
    "        if resource == \"pwn\":\n",
    "            entity_id_key = \"synset_id\"\n",
    "\n",
    "        e_with_sug_1 = []\n",
    "        for term, sug_hits in resource_1.items():\n",
    "            for hit in sug_hits:\n",
    "                e_with_sug_1.append(hit[entity_id_key])\n",
    "\n",
    "        e_with_sug_2 = []\n",
    "        for term, sug_hits in resource_2.items():\n",
    "            for hit in sug_hits:\n",
    "                e_with_sug_2.append(hit[entity_id_key])\n",
    "            \n",
    "    unique_e = [e for e in list(set(e_with_sug_2)) if e not in list(set(e_with_sug_1))]\n",
    "    \n",
    "    results[f\"{resource}{lang}\"] = unique_e\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ent = get_unique_entities_with_suggestions(\"wd\",\"en\",(\"subset\",\"all\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ent[\"wd_en\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Are sugestions used when contentious terms are found in prefLabel?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# related matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/anesterov/reps/LODlit/RQ2/sug_rm_wd_en.json','r') as jf:\n",
    "    sug_rm_wd_en = json.load(jf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_suggestions(resource:str,lang:str,level:str,search_suggestions_in:str) -> dict:\n",
    "    '''\n",
    "    resource: 'wd', 'aat'\n",
    "    level: 'rm', 'subset', 'all'\n",
    "    search_suggestions_in: 'pref', 'alt'\n",
    "    '''\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # check resource\n",
    "    if resource == \"wd\":\n",
    "        pref = \"prefLabel\"\n",
    "        alt = \"aliases\"\n",
    "        entity_key = \"QID\"\n",
    "    if resource == \"aat\":\n",
    "        pref = \"prefLabel\"\n",
    "        alt = \"altLabel\"\n",
    "        entity_key = \"aat_uri\"\n",
    "    \n",
    "    with open(f\"/Users/anesterov/reps/LODlit/RQ2/sug_{level}_{resource}_{lang}.json\",'r') as jf:\n",
    "        hits_with_suggestions = json.load(jf)\n",
    "    \n",
    "    n_hits = 0\n",
    "    for term, sug_hits in hits_with_suggestions.items():\n",
    "        hits_per_term = []\n",
    "        for hit in sug_hits:\n",
    "            # query terms are found in aliases and suggestions are used as prefLabel\n",
    "            if search_suggestions_in == 'pref':\n",
    "                if hit[\"found_in\"] == alt and len(hit[\"suggestions\"][pref]) > 0:\n",
    "                    hits_per_term.append(hit)\n",
    "                    n_hits += 1\n",
    "                    \n",
    "            if search_suggestions_in == 'alt':\n",
    "                if hit[\"found_in\"] == pref and len(hit[\"suggestions\"][alt]) > 0:\n",
    "                    hits_per_term.append(hit)\n",
    "                    n_hits += 1\n",
    "                \n",
    "        if len(hits_per_term) > 0:\n",
    "            results[term] = hits_per_term\n",
    "    \n",
    "    # get N of unique entities\n",
    "    n_e_unique = []\n",
    "    for term, hits in results.items():\n",
    "        for hit in hits:\n",
    "            n_e_unique.append(hit[entity_key])\n",
    "    \n",
    "    string_to_return = f\"Suggestions in {search_suggestions_in}. N unique e: {len(set(n_e_unique))} {(set(n_e_unique))}, N hits all: {n_hits}\"\n",
    "        \n",
    "    return string_to_return,results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"Suggestions in pref. N unique e: 3 {'Q1156', 'Q17884', 'Q12773225'}, N hits all: 5\",\n",
       " {'bombay': [{'query_term': 'bombay',\n",
       "    'lang': 'en',\n",
       "    'QID': 'Q1156',\n",
       "    'prefLabel': 'Mumbai',\n",
       "    'aliases': ['Bombay'],\n",
       "    'description': ['capital city in Maharashtra, India'],\n",
       "    'index': 1,\n",
       "    'found_in': 'aliases',\n",
       "    'instance_of': ['megacity',\n",
       "     'state capital',\n",
       "     'city',\n",
       "     'million city',\n",
       "     'locality',\n",
       "     'metropolis',\n",
       "     'second largest city',\n",
       "     'business cluster',\n",
       "     'big city'],\n",
       "    'subclass_of': [],\n",
       "    'suggestions': {'prefLabel': [['Mumbai', 100]],\n",
       "     'aliases': [],\n",
       "     'description': []}}],\n",
       "  'queer': [{'query_term': 'queer',\n",
       "    'lang': 'en',\n",
       "    'QID': 'Q17884',\n",
       "    'prefLabel': 'LGBT',\n",
       "    'aliases': ['GLBT',\n",
       "     'LGBTI',\n",
       "     'LGBTIQ',\n",
       "     'LGBTQ',\n",
       "     'LGB',\n",
       "     'QUILTBAG',\n",
       "     'LGBTQIA',\n",
       "     'LGBT+',\n",
       "     'LGBTQIAPN',\n",
       "     'LGBTQIA+',\n",
       "     'LGBTQIAP+',\n",
       "     'LGBT person',\n",
       "     'LGBT people',\n",
       "     'LGBTQ person',\n",
       "     'LGBTQ people',\n",
       "     'GLBT person',\n",
       "     'GLBT people',\n",
       "     'queer person',\n",
       "     'sexual and gender minority',\n",
       "     'sexual and gender minorities',\n",
       "     'queer people',\n",
       "     'LGBT persons',\n",
       "     'LGBTQ persons',\n",
       "     'GLBT persons',\n",
       "     'queer persons',\n",
       "     'LGBTA',\n",
       "     'LGBTA+',\n",
       "     'non-heterosexual people',\n",
       "     'non-heterosexual person',\n",
       "     'non-heterosexual persons',\n",
       "     'non-heterosexual',\n",
       "     'non-heterosexuals'],\n",
       "    'description': ['lesbian, gay, bisexual, and transgender persons'],\n",
       "    'index': 1,\n",
       "    'found_in': 'aliases',\n",
       "    'instance_of': ['community',\n",
       "     'group of humans',\n",
       "     'minority group',\n",
       "     'LGBT slang'],\n",
       "    'subclass_of': ['sexual minority', 'gender minority'],\n",
       "    'suggestions': {'prefLabel': [['LGBT', 100]],\n",
       "     'aliases': [['LGBT', 100]],\n",
       "     'description': []}},\n",
       "   {'query_term': 'queer',\n",
       "    'lang': 'en',\n",
       "    'QID': 'Q17884',\n",
       "    'prefLabel': 'LGBT',\n",
       "    'aliases': ['GLBT',\n",
       "     'LGBTI',\n",
       "     'LGBTIQ',\n",
       "     'LGBTQ',\n",
       "     'LGB',\n",
       "     'QUILTBAG',\n",
       "     'LGBTQIA',\n",
       "     'LGBT+',\n",
       "     'LGBTQIAPN',\n",
       "     'LGBTQIA+',\n",
       "     'LGBTQIAP+',\n",
       "     'LGBT person',\n",
       "     'LGBT people',\n",
       "     'LGBTQ person',\n",
       "     'LGBTQ people',\n",
       "     'GLBT person',\n",
       "     'GLBT people',\n",
       "     'queer person',\n",
       "     'sexual and gender minority',\n",
       "     'sexual and gender minorities',\n",
       "     'queer people',\n",
       "     'LGBT persons',\n",
       "     'LGBTQ persons',\n",
       "     'GLBT persons',\n",
       "     'queer persons',\n",
       "     'LGBTA',\n",
       "     'LGBTA+',\n",
       "     'non-heterosexual people',\n",
       "     'non-heterosexual person',\n",
       "     'non-heterosexual persons',\n",
       "     'non-heterosexual',\n",
       "     'non-heterosexuals'],\n",
       "    'description': ['lesbian, gay, bisexual, and transgender persons'],\n",
       "    'index': 1,\n",
       "    'found_in': 'aliases',\n",
       "    'instance_of': ['community',\n",
       "     'group of humans',\n",
       "     'minority group',\n",
       "     'LGBT slang'],\n",
       "    'subclass_of': ['sexual minority', 'gender minority'],\n",
       "    'suggestions': {'prefLabel': [['LGBT', 100]],\n",
       "     'aliases': [['LGBT', 100]],\n",
       "     'description': []}},\n",
       "   {'query_term': 'queer',\n",
       "    'lang': 'en',\n",
       "    'QID': 'Q17884',\n",
       "    'prefLabel': 'LGBT',\n",
       "    'aliases': ['GLBT',\n",
       "     'LGBTI',\n",
       "     'LGBTIQ',\n",
       "     'LGBTQ',\n",
       "     'LGB',\n",
       "     'QUILTBAG',\n",
       "     'LGBTQIA',\n",
       "     'LGBT+',\n",
       "     'LGBTQIAPN',\n",
       "     'LGBTQIA+',\n",
       "     'LGBTQIAP+',\n",
       "     'LGBT person',\n",
       "     'LGBT people',\n",
       "     'LGBTQ person',\n",
       "     'LGBTQ people',\n",
       "     'GLBT person',\n",
       "     'GLBT people',\n",
       "     'queer person',\n",
       "     'sexual and gender minority',\n",
       "     'sexual and gender minorities',\n",
       "     'queer people',\n",
       "     'LGBT persons',\n",
       "     'LGBTQ persons',\n",
       "     'GLBT persons',\n",
       "     'queer persons',\n",
       "     'LGBTA',\n",
       "     'LGBTA+',\n",
       "     'non-heterosexual people',\n",
       "     'non-heterosexual person',\n",
       "     'non-heterosexual persons',\n",
       "     'non-heterosexual',\n",
       "     'non-heterosexuals'],\n",
       "    'description': ['lesbian, gay, bisexual, and transgender persons'],\n",
       "    'index': 1,\n",
       "    'found_in': 'aliases',\n",
       "    'instance_of': ['community',\n",
       "     'group of humans',\n",
       "     'minority group',\n",
       "     'LGBT slang'],\n",
       "    'subclass_of': ['sexual minority', 'gender minority'],\n",
       "    'suggestions': {'prefLabel': [['LGBT', 100]],\n",
       "     'aliases': [['LGBT', 100]],\n",
       "     'description': []}}],\n",
       "  'slave': [{'query_term': 'slave',\n",
       "    'lang': 'en',\n",
       "    'QID': 'Q12773225',\n",
       "    'prefLabel': 'enslaved person',\n",
       "    'aliases': ['enslaved', 'slave'],\n",
       "    'description': ['person in a state of slavery'],\n",
       "    'index': 29,\n",
       "    'found_in': 'aliases',\n",
       "    'instance_of': ['social class', 'occupation', 'condition'],\n",
       "    'subclass_of': ['personal property', 'prisoner', 'product'],\n",
       "    'suggestions': {'prefLabel': [['enslaved person', 100]],\n",
       "     'aliases': [['Enslaved', 100]],\n",
       "     'description': []}}]})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_suggestions(\"wd\",\"en\",\"subset\",\"pref\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

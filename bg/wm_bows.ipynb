{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting backround information about query terms from the Words Matter knowledge graph\n",
    "* – getting Contentious Issues description texts\n",
    "* – getting suggested terms for contentious terms\n",
    "* – making files with descriptions and suggestions for every lemma of query terms\n",
    "* – getting top terms from the description text based on their TF-IDF scores\n",
    "* – shaping the files with WM bag of words for every lemma\n",
    "* – this notebook produces the following files:\n",
    "    * (1) 'CI_description.json'\n",
    "    * (2) 'suggested_terms_bows.json'\n",
    "    * (3) 'en_lemmas_wm_info.json'\n",
    "    * (4) 'nl_lemmas_wm_info.json'\n",
    "    * (5) 'en_wm_bows.json'\n",
    "    * (6) 'nl_wm_bows.json'\n",
    "    * (7) 'en_wm_bows_tf_idf.json'\n",
    "    * (8) 'nl_wm_bows_tf_idf.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "from nltk.corpus import stopwords\n",
    "import simplemma\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import rdflib\n",
    "from rdflib import Graph\n",
    "from rdflib.namespace import Namespace\n",
    "from rdflib.namespace import SKOS, RDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wnl = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Collecting WM description texts: querying WM KG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting custom namespaces\n",
    "culco = Namespace(\"https://w3id.org/culco#\")\n",
    "skosxl = Namespace(\"http://www.w3.org/2008/05/skos-xl#\")\n",
    "dcterms = Namespace(\"http://purl.org/dc/terms/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change path\n",
    "path_to_wm = '/Users/anesterov/reps/wordsmatter/glossary.ttl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the graph\n",
    "wm = Graph()\n",
    "wm.parse(path_to_wm, format=\"turtle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPARQL to get Contentious Issues descriptions\n",
    "\n",
    "descr_text = wm.query(\n",
    "    \"\"\"        \n",
    "    SELECT ?CI ?descr_text (GROUP_CONCAT(?cont_label_uri;SEPARATOR=\",\") AS ?cont_label_list)\n",
    "\n",
    "    WHERE {\n",
    "\n",
    "      ?CI dcterms:description ?descr_text ;\n",
    "            culco:hasContentiousLabel ?cont_label_uri .\n",
    "    }\n",
    "    GROUP BY ?CI\n",
    "    \"\"\",\n",
    "    \n",
    "    initNs={'culco': culco, 'dcterms':dcterms}\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descr_text_dict = {}\n",
    "for row in descr_text:\n",
    "    prefix = \"https://w3id.org/culco/wordsmatter/\"\n",
    "    cont_labels = row.cont_label_list.split(\",\")\n",
    "    cl = [l.replace(prefix,\"\") for l in cont_labels]\n",
    "    descr_text_dict[row.CI.replace(prefix,\"\")] = {\"descr\":str(row.descr_text), \"cont_labels\":cl}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exporting\n",
    "with open('CI_description.json', 'w') as jf:\n",
    "    json.dump(descr_text_dict, jf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Getting suggested terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suggested = wm.query(\n",
    "    \"\"\"        \n",
    "    SELECT ?cont_label (GROUP_CONCAT(?sug_label_lit;SEPARATOR=\" \") AS ?sug_label_lit_list)\n",
    "\n",
    "    WHERE {\n",
    "\n",
    "      ?Suggestion culco:suggestedFor ?cont_label ;\n",
    "                  culco:hasSuggestedLabel ?sug_label .\n",
    "                  \n",
    "      ?sug_label skosxl:literalForm ?sug_label_lit .\n",
    "    }\n",
    "    GROUP BY ?cont_label\n",
    "    \"\"\",\n",
    "    \n",
    "    initNs={'culco': culco, 'skosxl':skosxl}\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suggested_labels = {}\n",
    "for row in suggested:\n",
    "    label_id = row.cont_label.replace(\"https://w3id.org/culco/wordsmatter/\",\"\")\n",
    "    suggested_labels[label_id] = str(row.sug_label_lit_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label_uri, sug in suggested_labels.items():\n",
    "    sug_list = sug.lower().replace(\"-\",\" \").replace(\"(\",\"\").replace(\")\",\"\").replace(\"\\xad\",\"\").split(\" \")\n",
    "    no_stop_words = [s for s in sug_list if s not in stopwords.words('dutch') \\\n",
    "                     and s not in stopwords.words('english')]\n",
    "    suggested_labels[label_uri] = list(set(no_stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exporting\n",
    "with open('suggested_terms_bows.json', 'w') as jf:\n",
    "    json.dump(suggested_labels, jf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Generating files with WM info per lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CI with labels and descriptions\n",
    "with open(\"/Users/anesterov/reps/LODlit/CI_description.json\",'r') as jf:\n",
    "    wm_descr = json.load(jf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# suggestions\n",
    "with open(\"/Users/anesterov/reps/LODlit/suggested_terms_bows.json\",'r') as jf:\n",
    "    wm_suggestions = json.load(jf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing lemmas with label URIs\n",
    "with open('/Users/anesterov/reps/LODlit/en_lemmas_with_label_uris.json','r') as jf:\n",
    "    en_lemmas_with_label_uris = json.load(jf)\n",
    "    \n",
    "with open('/Users/anesterov/reps/LODlit/nl_lemmas_with_label_uris.json','r') as jf:\n",
    "    nl_lemmas_with_label_uris = json.load(jf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EN: file with lemmas, their corresponding labels, WM text, suggestions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# {'lemma': {'wm_text':[''], 'suggestions':[''], 'label_uris':['']}}\n",
    "\n",
    "en_lemmas_wm_text = {}\n",
    "\n",
    "for lemma, label_uris in en_lemmas_with_label_uris.items():\n",
    "    dict_per_lemma = {}\n",
    "    descr_list_per_lemma = []\n",
    "    for label in label_uris:\n",
    "        for CI, info in wm_descr.items():\n",
    "            if label in info[\"cont_labels\"]:\n",
    "                descr_list_per_lemma.append(info[\"descr\"])\n",
    "    \n",
    "    dict_per_lemma[\"wm_text\"] = list(set(descr_list_per_lemma))\n",
    "    dict_per_lemma[\"label_uris\"] = label_uris\n",
    "    en_lemmas_wm_text[lemma] = dict_per_lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding suggestions\n",
    "\n",
    "for lemma, info in en_lemmas_wm_text.items():\n",
    "    suggestions_per_lemma = []\n",
    "    for label_uri in info[\"label_uris\"]:\n",
    "        suggestion_list = wm_suggestions.get(label_uri)\n",
    "        if suggestion_list != None:\n",
    "            suggestions_per_lemma.extend(suggestion_list)\n",
    "    info[\"suggestions\"] = suggestions_per_lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exporting\n",
    "with open('en_lemmas_wm_info.json', 'w') as jf:\n",
    "\tjson.dump(en_lemmas_wm_text, jf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NL: file with lemmas, their corresponding labels, WM text, suggestions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# {'lemma': {'wm_text':[''], 'suggestions':[''], 'label_uris':['']}}\n",
    "\n",
    "nl_lemmas_wm_text = {}\n",
    "\n",
    "for lemma, label_uris in nl_lemmas_with_label_uris.items():\n",
    "    dict_per_lemma = {}\n",
    "    descr_list_per_lemma = []\n",
    "    for label in label_uris:\n",
    "        for CI, info in wm_descr.items():\n",
    "            # checking if CI is in Dutch (has _nl suffix)\n",
    "            if \"_nl\" in CI and label in info[\"cont_labels\"]:\n",
    "                descr_list_per_lemma.append(info[\"descr\"])\n",
    "    \n",
    "    dict_per_lemma[\"wm_text\"] = list(set(descr_list_per_lemma)) # taking only unque desr texts\n",
    "    dict_per_lemma[\"label_uris\"] = label_uris\n",
    "    nl_lemmas_wm_text[lemma] = dict_per_lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding NL suggestions\n",
    "for lemma, info in nl_lemmas_wm_text.items():\n",
    "    suggestions_per_lemma = []\n",
    "    for label_uri in info[\"label_uris\"]:\n",
    "        suggestion_list = wm_suggestions.get(label_uri)\n",
    "        if suggestion_list != None:\n",
    "            suggestions_per_lemma.extend(suggestion_list)\n",
    "    info[\"suggestions\"] = suggestions_per_lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exporting\n",
    "with open('nl_lemmas_wm_info.json', 'w') as jf:\n",
    "    json.dump(nl_lemmas_wm_text, jf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### tokenise, lower-case, remove non-word characters, lemmatise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_bow(text:list, lang:str, merge_bows=False) -> list:\n",
    "    '''\n",
    "    Makes a BoW from a list of str:\n",
    "    removes non-word charachters (incl. punctuation, numbers),\n",
    "    removes stop-words (nltk),\n",
    "    lowercases, tokenises (split by space), lemmatises (NLTK lemmatiser for EN; simplemma for NL),\n",
    "    removes tokens with fewer than 3 characters;\n",
    "    text: a list of str to make BoWs from\n",
    "    lang: str, language of strings, 'en' or 'nl'\n",
    "    merge_bows: bool, if there are multiple texts, merge them in one BoW (True) or not (False), default False \n",
    "    Returns a BoW (list of lists)\n",
    "    '''\n",
    "    joint_bow = []\n",
    "    \n",
    "    for t in text:\n",
    "        no_w_text = re.sub('(\\W|\\d)',' ',t)\n",
    "        text_bow = no_w_text.split(' ')\n",
    "        \n",
    "        # checking lang\n",
    "        if lang == 'en':\n",
    "            text_bow_clean = [wnl.lemmatize(token.lower()) for token in text_bow if token.lower() not in stopwords.words('english') \\\n",
    "                                 and token != '' and len(token) > 2]\n",
    "        if lang == 'nl':\n",
    "            # Dutch lemmatizer can output uppercase lemmas\n",
    "            text_bow_clean_not_lowercased = [simplemma.lemmatize(token.lower(),lang='nl') for token in text_bow if token.lower() not in stopwords.words('dutch') \\\n",
    "                                 and token != '' and len(token) > 2]\n",
    "            text_bow_clean = [t.lower() for t in text_bow_clean_not_lowercased]\n",
    "            \n",
    "        if merge_bows == True:\n",
    "            joint_bow.extend(text_bow_clean)\n",
    "        else:\n",
    "            joint_bow.append(text_bow_clean)\n",
    "            \n",
    "    return joint_bow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EN: make a file with WM bows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lemma, wm_info in en_lemmas_wm_text.items():\n",
    "    bow = []\n",
    "    bow.extend(make_bow(wm_info[\"wm_text\"],\"en\"))\n",
    "    wm_info[\"bow\"] = bow\n",
    "    \n",
    "    # suggestions should be lemmatised\n",
    "    lem_sug = [wnl.lemmatize(s) for s in wm_info[\"suggestions\"]]\n",
    "    wm_info[\"suggestions\"] = lem_sug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exporting EN json file\n",
    "with open('en_wm_bows.json', 'w') as jf:\n",
    "    json.dump(en_lemmas_wm_text, jf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NL: make a file with WM bows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lemma, wm_info in nl_lemmas_wm_text.items():\n",
    "    bow = []\n",
    "    bow.extend(make_bow(wm_info[\"wm_text\"],\"nl\"))\n",
    "    wm_info[\"bow\"] = bow\n",
    "    \n",
    "    # suggestions should be lemmatised\n",
    "    lem_sug = [simplemma.lemmatize(s,lang='nl') for s in wm_info[\"suggestions\"]]\n",
    "    wm_info[\"suggestions\"] = [s.lower() for s in lem_sug]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exporting NL json file\n",
    "with open('nl_wm_bows.json', 'w') as jf:\n",
    "    json.dump(nl_lemmas_wm_text, jf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Getting WM BoWs with TF-IDF scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing WM text EN\n",
    "with open(\"/Users/anesterov/reps/LODlit/en_wm_bows.json\",\"r\") as jf:\n",
    "    en_wm_bows = json.load(jf)\n",
    "    \n",
    "# importing WM text NL\n",
    "with open(\"/Users/anesterov/reps/LODlit/nl_wm_bows.json\",\"r\") as jf:\n",
    "    nl_wm_bows = json.load(jf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf(doc:list, token:str) -> float:\n",
    "    '''\n",
    "    Calculates term frequency\n",
    "    doc: list, alll documents\n",
    "    token: str, a token to get the TF score of\n",
    "    Returns float\n",
    "    '''\n",
    "    n_found = len([t for t in doc if t == token])\n",
    "    tf_score = n_found / len(doc)\n",
    "    \n",
    "    return tf_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idf(token:str, doc_freq:dict, n_docs:int) -> float:\n",
    "    '''\n",
    "    Calculates inverse document frequency:\n",
    "        adds 1 to DF to avoid zero division\n",
    "    token: str, a token to get the IDF score of\n",
    "    doc_freq: dict, document frequency, in how many documents tokens appear\n",
    "    n_docs: int, a number of documents\n",
    "    Returns float\n",
    "    '''\n",
    "    idf_score = math.log(n_docs / (doc_freq[token] + 1))\n",
    "        \n",
    "    return idf_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_tokens_tfidf(bow:list,doc_freq:dict,n_docs:int) -> list:\n",
    "    '''\n",
    "    Getting top tokens based on their TF-IDF weighting in one BoW\n",
    "    Depends on the tf and idf functions\n",
    "    bow: list of str, tokens in one BoW\n",
    "    doc_freq: dict, document frequency, in how many documents tokens appear\n",
    "    n_docs: int, a number of documents\n",
    "    Returns list: Top 10 tokens in a bow by their TF-IDF scores\n",
    "    '''\n",
    "    top_tokens = []\n",
    "    tf_idf_scores = {}\n",
    "    \n",
    "    for token in bow:\n",
    "        tf_idf = tf(bow,token) * idf(token,doc_freq,n_docs)\n",
    "        tf_idf_scores[token] = tf_idf\n",
    "        \n",
    "    tokens_scores = sorted(tf_idf_scores.items(), key=lambda x:x[1], reverse=True)\n",
    "    \n",
    "    if len(tokens_scores) < 10:\n",
    "        top_tokens = [t[0] for t in tokens_scores]\n",
    "    else:\n",
    "        #cut_off_score = tokens_scores[9][1] # taking top 10 scores\n",
    "        #top_tokens = [t[0] for t in tokens_scores if t[1] >= cut_off_score]\n",
    "        #if len(top_tokens) > 10:\n",
    "        top_tokens = [t[0] for t in tokens_scores[0:10]]\n",
    "    \n",
    "    return top_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_tokens_and_docs(source:dict) -> tuple:\n",
    "    '''\n",
    "    Gets unique tokens and documents in a file\n",
    "    source: dict\n",
    "    Prints N of unique tokens\n",
    "    Returns a tuple, where 0: list of tokens (str), 1: list of documents (list)\n",
    "    '''\n",
    "    all_docs = []\n",
    "    all_tokens = []\n",
    "    \n",
    "    for value in source.values():\n",
    "        # taking only unique bows\n",
    "        for bow in value[\"bow\"]:\n",
    "            if bow not in all_docs:\n",
    "                all_docs.append(bow)\n",
    "                # collecting all unique tokens\n",
    "                for token in bow:\n",
    "                    if token not in all_tokens:\n",
    "                        all_tokens.append(token)\n",
    "    \n",
    "    print(f\"Unique tokens: {len(all_tokens)}\")\n",
    "    \n",
    "    tokens_docs = (all_tokens, all_docs) \n",
    "    \n",
    "    return tokens_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of unique tokens and number of documents in WM EN\n",
    "tokens_docs_en = get_unique_tokens_and_docs(en_wm_bows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens_en = tokens_docs_en[0]\n",
    "all_docs_en = tokens_docs_en[1]\n",
    "n_docs_en = len(all_docs_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making a dict with document frequency (DF) scores for every unique token\n",
    "en_df = {}\n",
    "for token in all_tokens_en:\n",
    "    token_count = 0\n",
    "    for bow in all_docs_en:\n",
    "        if token in bow:\n",
    "            token_count += 1\n",
    "    en_df[token] = token_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding two new bows to the file 'en_wm_bows':\n",
    "# (1) top tokens based on TF-IDF; (2) joint bow with the 1 + suggestions\n",
    "\n",
    "for value in en_wm_bows.values():\n",
    "    \n",
    "    # there can be muttiple bows for one term\n",
    "    top_tokens = [] \n",
    "    \n",
    "    for bow in value[\"bow\"]:\n",
    "        top_tokens.extend(get_top_tokens_tfidf(bow,en_df,n_docs_en))\n",
    "        \n",
    "    value[\"bow_tf_idf\"] = top_tokens\n",
    "    \n",
    "    # metging top tokens and suggestions\n",
    "    joint_bow = []\n",
    "    joint_bow.extend(top_tokens)\n",
    "    joint_bow.extend(value[\"suggestions\"])\n",
    "    \n",
    "    value[\"bow_joint\"] = joint_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exporting\n",
    "with open('en_wm_bows_tf_idf.json', 'w') as jf:\n",
    "    json.dump(en_wm_bows, jf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of unique tokens and number of documents in WM NL\n",
    "tokens_docs_nl = get_unique_tokens_and_docs(nl_wm_bows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens_nl = tokens_docs_nl[0]\n",
    "all_docs_nl = tokens_docs_nl[1]\n",
    "n_docs_nl = len(all_docs_nl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making a dict with document frequency (DF) scores for every unique token\n",
    "nl_df = {}\n",
    "for token in all_tokens_nl:\n",
    "    token_count = 0\n",
    "    for bow in all_docs_nl:\n",
    "        if token in bow:\n",
    "            token_count += 1\n",
    "    nl_df[token] = token_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding two new bows to the file 'nl_wm_bows':\n",
    "# (1) top tokens based on TF-IDF; (2) joint bow with the 1 + suggestions\n",
    "\n",
    "for value in nl_wm_bows.values():\n",
    "    \n",
    "    # there can be muttiple bows for one term\n",
    "    top_tokens = [] \n",
    "    \n",
    "    for bow in value[\"bow\"]:\n",
    "        top_tokens.extend(get_top_tokens_tfidf(bow,nl_df,n_docs_nl))\n",
    "        \n",
    "    value[\"bow_tf_idf\"] = top_tokens\n",
    "    \n",
    "    # metging top tokens and suggestions\n",
    "    joint_bow = []\n",
    "    joint_bow.extend(top_tokens)\n",
    "    joint_bow.extend(value[\"suggestions\"])\n",
    "    \n",
    "    value[\"bow_joint\"] = joint_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exporting\n",
    "with open('nl_wm_bows_tf_idf.json', 'w') as jf:\n",
    "    json.dump(nl_wm_bows, jf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting backround information about query terms from the Words Matter knowledge graph\n",
    "* – getting Contentious Issues description texts\n",
    "* – getting suggested terms for contentious terms\n",
    "* – making files with descriptions and suggestions for every lemma of query terms\n",
    "* – getting top terms from the description text based on their TF-IDF scores\n",
    "* – shaping the files with WM bag of words for every lemma\n",
    "* – this notebook generates the following files:\n",
    "    * (1) 'CI_description.json'\n",
    "    * (2) 'suggested_terms_bows.json'\n",
    "    * (3) 'en_lemmas_wm_info.json'\n",
    "    * (4) 'nl_lemmas_wm_info.json'\n",
    "    * (5) 'en_wm_bows.json'\n",
    "    * (6) 'nl_wm_bows.json'\n",
    "    * (7) 'en_wm_bows_tf_idf.json'\n",
    "    * (8) 'nl_wm_bows_tf_idf.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "from nltk.corpus import stopwords\n",
    "import simplemma\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import rdflib\n",
    "from rdflib import Graph\n",
    "from rdflib.namespace import Namespace\n",
    "from rdflib.namespace import SKOS, RDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wnl = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing LODlitParser bows module \n",
    "# this code is taken from\n",
    "# https://stackoverflow.com/questions/67631/how-can-i-import-a-module-dynamically-given-the-full-path\n",
    "import importlib.util\n",
    "import sys\n",
    "spec = importlib.util.spec_from_file_location(\"LODlitParser.bows\", \"/Users/anesterov/reps/LODlit/LODlitParser/bows.py\")\n",
    "bows = importlib.util.module_from_spec(spec)\n",
    "sys.modules[\"LODlitParser.bows\"] = bows\n",
    "spec.loader.exec_module(bows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Collecting WM description texts: querying WM KG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting custom namespaces\n",
    "culco = Namespace(\"https://w3id.org/culco#\")\n",
    "skosxl = Namespace(\"http://www.w3.org/2008/05/skos-xl#\")\n",
    "dcterms = Namespace(\"http://purl.org/dc/terms/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change path\n",
    "path_to_wm = '/Users/anesterov/reps/wordsmatter/glossary.ttl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the graph\n",
    "wm = Graph()\n",
    "wm.parse(path_to_wm, format=\"turtle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPARQL to get Contentious Issues descriptions\n",
    "\n",
    "descr_text = wm.query(\n",
    "    \"\"\"        \n",
    "    SELECT ?CI ?descr_text (GROUP_CONCAT(?cont_label_uri;SEPARATOR=\",\") AS ?cont_label_list)\n",
    "\n",
    "    WHERE {\n",
    "\n",
    "      ?CI dcterms:description ?descr_text ;\n",
    "            culco:hasContentiousLabel ?cont_label_uri .\n",
    "    }\n",
    "    GROUP BY ?CI\n",
    "    \"\"\",\n",
    "    \n",
    "    initNs={'culco': culco, 'dcterms':dcterms}\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descr_text_dict = {}\n",
    "for row in descr_text:\n",
    "    prefix = \"https://w3id.org/culco/wordsmatter/\"\n",
    "    cont_labels = row.cont_label_list.split(\",\")\n",
    "    cl = [l.replace(prefix,\"\") for l in cont_labels]\n",
    "    descr_text_dict[row.CI.replace(prefix,\"\")] = {\"descr\":str(row.descr_text), \"cont_labels\":cl}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exporting\n",
    "with open('CI_description.json', 'w') as jf:\n",
    "    json.dump(descr_text_dict, jf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Getting suggested terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suggested = wm.query(\n",
    "    \"\"\"        \n",
    "    SELECT ?cont_label (GROUP_CONCAT(?sug_label_lit;SEPARATOR=\" \") AS ?sug_label_lit_list)\n",
    "\n",
    "    WHERE {\n",
    "\n",
    "      ?Suggestion culco:suggestedFor ?cont_label ;\n",
    "                  culco:hasSuggestedLabel ?sug_label .\n",
    "                  \n",
    "      ?sug_label skosxl:literalForm ?sug_label_lit .\n",
    "    }\n",
    "    GROUP BY ?cont_label\n",
    "    \"\"\",\n",
    "    \n",
    "    initNs={'culco': culco, 'skosxl':skosxl}\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suggested_labels = {}\n",
    "for row in suggested:\n",
    "    label_id = row.cont_label.replace(\"https://w3id.org/culco/wordsmatter/\",\"\")\n",
    "    suggested_labels[label_id] = str(row.sug_label_lit_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label_uri, sug in suggested_labels.items():\n",
    "    sug_list = sug.lower().replace(\"-\",\" \").replace(\"(\",\"\").replace(\")\",\"\").replace(\"\\xad\",\"\").split(\" \")\n",
    "    no_stop_words = [s for s in sug_list if s not in stopwords.words('dutch') \\\n",
    "                     and s not in stopwords.words('english')]\n",
    "    suggested_labels[label_uri] = list(set(no_stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exporting\n",
    "with open('suggested_terms_bows.json', 'w') as jf:\n",
    "    json.dump(suggested_labels, jf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Generating files with WM info per lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CI with labels and descriptions\n",
    "with open(\"/Users/anesterov/reps/LODlit/bg/CI_description.json\",'r') as jf:\n",
    "    wm_descr = json.load(jf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# suggestions\n",
    "with open(\"/Users/anesterov/reps/LODlit/bg/suggested_terms_bows.json\",'r') as jf:\n",
    "    wm_suggestions = json.load(jf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing lemmas with label URIs\n",
    "with open('/Users/anesterov/reps/LODlit/en_lemmas_with_label_uris.json','r') as jf:\n",
    "    en_lemmas_with_label_uris = json.load(jf)\n",
    "    \n",
    "with open('/Users/anesterov/reps/LODlit/nl_lemmas_with_label_uris.json','r') as jf:\n",
    "    nl_lemmas_with_label_uris = json.load(jf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EN: file with lemmas, their corresponding labels, WM text, suggestions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# {'lemma': {'wm_text':[''], 'suggestions':[''], 'label_uris':['']}}\n",
    "\n",
    "en_lemmas_wm_text = {}\n",
    "\n",
    "for lemma, label_uris in en_lemmas_with_label_uris.items():\n",
    "    dict_per_lemma = {}\n",
    "    descr_list_per_lemma = []\n",
    "    for label in label_uris:\n",
    "        for CI, info in wm_descr.items():\n",
    "            if label in info[\"cont_labels\"]:\n",
    "                descr_list_per_lemma.append(info[\"descr\"])\n",
    "    \n",
    "    dict_per_lemma[\"wm_text\"] = list(set(descr_list_per_lemma))\n",
    "    dict_per_lemma[\"label_uris\"] = label_uris\n",
    "    en_lemmas_wm_text[lemma] = dict_per_lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding suggestions\n",
    "\n",
    "for lemma, info in en_lemmas_wm_text.items():\n",
    "    suggestions_per_lemma = []\n",
    "    for label_uri in info[\"label_uris\"]:\n",
    "        suggestion_list = wm_suggestions.get(label_uri)\n",
    "        if suggestion_list != None:\n",
    "            suggestions_per_lemma.extend(suggestion_list)\n",
    "    info[\"suggestions\"] = suggestions_per_lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exporting\n",
    "with open('en_lemmas_wm_info.json', 'w') as jf:\n",
    "    json.dump(en_lemmas_wm_text, jf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NL: file with lemmas, their corresponding labels, WM text, suggestions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# {'lemma': {'wm_text':[''], 'suggestions':[''], 'label_uris':['']}}\n",
    "\n",
    "nl_lemmas_wm_text = {}\n",
    "\n",
    "for lemma, label_uris in nl_lemmas_with_label_uris.items():\n",
    "    dict_per_lemma = {}\n",
    "    descr_list_per_lemma = []\n",
    "    for label in label_uris:\n",
    "        for CI, info in wm_descr.items():\n",
    "            # checking if CI is in Dutch (has _nl suffix)\n",
    "            if \"_nl\" in CI and label in info[\"cont_labels\"]:\n",
    "                descr_list_per_lemma.append(info[\"descr\"])\n",
    "    \n",
    "    dict_per_lemma[\"wm_text\"] = list(set(descr_list_per_lemma)) # taking only unque desr texts\n",
    "    dict_per_lemma[\"label_uris\"] = label_uris\n",
    "    nl_lemmas_wm_text[lemma] = dict_per_lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding NL suggestions\n",
    "for lemma, info in nl_lemmas_wm_text.items():\n",
    "    suggestions_per_lemma = []\n",
    "    for label_uri in info[\"label_uris\"]:\n",
    "        suggestion_list = wm_suggestions.get(label_uri)\n",
    "        if suggestion_list != None:\n",
    "            suggestions_per_lemma.extend(suggestion_list)\n",
    "    info[\"suggestions\"] = suggestions_per_lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exporting\n",
    "with open('nl_lemmas_wm_info.json', 'w') as jf:\n",
    "    json.dump(nl_lemmas_wm_text, jf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### tokenise, lower-case, remove non-word characters, lemmatise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EN: make a file with WM bows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lemma, wm_info in en_lemmas_wm_text.items():\n",
    "    bow = []\n",
    "    bow.extend(bows.make_bows(wm_info[\"wm_text\"],\"en\"))\n",
    "    wm_info[\"bow\"] = bow\n",
    "    \n",
    "    # suggestions should be lemmatised\n",
    "    lem_sug = [wnl.lemmatize(s) for s in wm_info[\"suggestions\"]]\n",
    "    wm_info[\"suggestions\"] = lem_sug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exporting EN json file\n",
    "with open('en_wm_bows.json', 'w') as jf:\n",
    "    json.dump(en_lemmas_wm_text, jf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NL: make a file with WM bows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lemma, wm_info in nl_lemmas_wm_text.items():\n",
    "    bow = []\n",
    "    bow.extend(bows.make_bows(wm_info[\"wm_text\"],\"nl\"))\n",
    "    wm_info[\"bow\"] = bow\n",
    "    \n",
    "    # suggestions should be lemmatised\n",
    "    lem_sug = [simplemma.lemmatize(s,lang='nl') for s in wm_info[\"suggestions\"]]\n",
    "    wm_info[\"suggestions\"] = [s.lower() for s in lem_sug]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exporting NL json file\n",
    "with open('nl_wm_bows.json', 'w') as jf:\n",
    "    json.dump(nl_lemmas_wm_text, jf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Getting WM BoWs with TF-IDF scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing WM text EN\n",
    "with open(\"/Users/anesterov/reps/LODlit/bg/en_wm_bows.json\",\"r\") as jf:\n",
    "    en_wm_bows = json.load(jf)\n",
    "    \n",
    "# importing WM text NL\n",
    "with open(\"/Users/anesterov/reps/LODlit/bg/nl_wm_bows.json\",\"r\") as jf:\n",
    "    nl_wm_bows = json.load(jf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_tokens_and_docs(source:dict) -> tuple:\n",
    "    '''\n",
    "    Gets unique tokens and documents in a file\n",
    "    source: dict\n",
    "    Prints N of unique tokens\n",
    "    Returns a tuple, where 0: list of tokens (str), 1: list of documents (list)\n",
    "    '''\n",
    "    all_docs = []\n",
    "    all_tokens = []\n",
    "    \n",
    "    for value in source.values():\n",
    "        # taking only unique bows\n",
    "        for bow in value[\"bow\"]:\n",
    "            if bow not in all_docs:\n",
    "                all_docs.append(bow)\n",
    "                # collecting all unique tokens\n",
    "                for token in bow:\n",
    "                    if token not in all_tokens:\n",
    "                        all_tokens.append(token)\n",
    "    \n",
    "    print(f\"Unique tokens: {len(all_tokens)}\")\n",
    "    \n",
    "    tokens_docs = (all_tokens, all_docs) \n",
    "    \n",
    "    return tokens_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of unique tokens and number of documents in WM EN\n",
    "tokens_docs_en = get_unique_tokens_and_docs(en_wm_bows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens_en = tokens_docs_en[0]\n",
    "all_docs_en = tokens_docs_en[1]\n",
    "n_docs_en = len(all_docs_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making a dict with document frequency (DF) scores for every unique token\n",
    "en_df = {}\n",
    "for token in all_tokens_en:\n",
    "    token_count = 0\n",
    "    for bow in all_docs_en:\n",
    "        if token in bow:\n",
    "            token_count += 1\n",
    "    en_df[token] = token_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding two new bows to the file 'en_wm_bows':\n",
    "# (1) top tokens based on TF-IDF; (2) joint bow with the 1 + suggestions\n",
    "\n",
    "for value in en_wm_bows.values():\n",
    "    \n",
    "    # there can be muttiple bows for one term\n",
    "    top_tokens = [] \n",
    "    \n",
    "    for bow in value[\"bow\"]:\n",
    "        top_tokens.extend(bows.get_top_tokens_tfidf(bow,en_df,n_docs_en))\n",
    "        \n",
    "    value[\"bow_tf_idf\"] = top_tokens\n",
    "    \n",
    "    # merging top tokens and suggestions\n",
    "    joint_bow = []\n",
    "    joint_bow.extend(top_tokens)\n",
    "    joint_bow.extend(value[\"suggestions\"])\n",
    "    \n",
    "    value[\"bow_joint\"] = joint_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exporting\n",
    "with open('en_wm_bows_tf_idf.json', 'w') as jf:\n",
    "    json.dump(en_wm_bows, jf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of unique tokens and number of documents in WM NL\n",
    "tokens_docs_nl = get_unique_tokens_and_docs(nl_wm_bows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens_nl = tokens_docs_nl[0]\n",
    "all_docs_nl = tokens_docs_nl[1]\n",
    "n_docs_nl = len(all_docs_nl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making a dict with document frequency (DF) scores for every unique token\n",
    "nl_df = {}\n",
    "for token in all_tokens_nl:\n",
    "    token_count = 0\n",
    "    for bow in all_docs_nl:\n",
    "        if token in bow:\n",
    "            token_count += 1\n",
    "    nl_df[token] = token_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding two new bows to the file 'nl_wm_bows':\n",
    "# (1) top tokens based on TF-IDF; (2) joint bow with the 1 + suggestions\n",
    "\n",
    "for value in nl_wm_bows.values():\n",
    "    \n",
    "    # there can be muttiple bows for one term\n",
    "    top_tokens = [] \n",
    "    \n",
    "    for bow in value[\"bow\"]:\n",
    "        top_tokens.extend(bows.get_top_tokens_tfidf(bow,nl_df,n_docs_nl))\n",
    "        \n",
    "    value[\"bow_tf_idf\"] = top_tokens\n",
    "    \n",
    "    # metging top tokens and suggestions\n",
    "    joint_bow = []\n",
    "    joint_bow.extend(top_tokens)\n",
    "    joint_bow.extend(value[\"suggestions\"])\n",
    "    \n",
    "    value[\"bow_joint\"] = joint_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exporting\n",
    "with open('nl_wm_bows_tf_idf.json', 'w') as jf:\n",
    "    json.dump(nl_wm_bows, jf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

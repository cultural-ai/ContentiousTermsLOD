{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "import requests\n",
    "import io\n",
    "import time\n",
    "import zipfile\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing EN query terms\n",
    "with open('query_terms_cont_en.json','r') as jf:\n",
    "    query_terms_cont_en = json.load(jf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list_of_terms_en = []\n",
    "\n",
    "for stem, form in query_terms_cont_en.items():\n",
    "    list_of_terms_en.append(stem)\n",
    "    list_of_terms_en.extend(form)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list_of_terms_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing NL query terms\n",
    "with open('query_terms_cont_nl.json','r') as jf:\n",
    "    query_terms_cont_nl = json.load(jf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_terms_nl = []\n",
    "\n",
    "for stem, form in query_terms_cont_nl.items():\n",
    "    list_of_terms_nl.append(stem)\n",
    "    list_of_terms_nl.extend(form)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list_of_terms_nl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating total hits per term (without filtering on keywords and properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'query' with 'search' generator: constant params\n",
    "url = \"https://www.wikidata.org/w/api.php\"\n",
    "params_en = {\"action\":\"query\",\n",
    "          \"prop\":\"entityterms\",\n",
    "          \"wbetlanguage\":\"en\", # English\n",
    "          \"generator\":\"search\",\n",
    "          \"gsrsearch\":'', # term goes here (quotes for stemming off)\n",
    "          \"gsrlimit\":\"1\", # getting all results\n",
    "          \"gsrinfo\":\"totalhits\",\n",
    "          \"format\":\"json\"} \n",
    "headers = {\"user-agent\":\"bot getting labels aliases and descriptions of the requested pages (CWI; Human-Centered Data Analytics; nesterov@cwi.nl)\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### EN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterating over the list of terms\n",
    "# updating the 'srsearch' param\n",
    "with open (\"total_hits_no_filter_en.csv\",\"w\") as csv_file:\n",
    "    header = ['query_term', 'total_hits']\n",
    "    writer = csv.writer(csv_file)\n",
    "    writer.writerow(header)\n",
    "    \n",
    "    for term in list_of_terms_en:\n",
    "        params_en[\"gsrsearch\"] = f'\"{term}\"'\n",
    "        r = requests.get(url,params=params_en,headers=headers)\n",
    "        hits = r.json()['query']['searchinfo']['totalhits']\n",
    "        data = [term,hits]\n",
    "        \n",
    "        writer.writerow(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### NL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'query' with 'search' generator: constant params\n",
    "url = \"https://www.wikidata.org/w/api.php\"\n",
    "params_nl = {\"action\":\"query\",\n",
    "          \"prop\":\"entityterms\",\n",
    "          \"wbetlanguage\":\"nl\", # Dutch\n",
    "          \"generator\":\"search\",\n",
    "          \"gsrsearch\":'', # term goes here (quotes for stemming off)\n",
    "          \"gsrlimit\":\"1\", # getting all results\n",
    "          \"gsrinfo\":\"totalhits\",\n",
    "          \"format\":\"json\"} \n",
    "headers = {\"user-agent\":\"bot getting labels aliases and descriptions of the requested pages (CWI; Human-Centered Data Analytics; nesterov@cwi.nl)\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterating over the list of terms\n",
    "# updating the 'srsearch' param\n",
    "with open (\"total_hits_no_filter_nl.csv\",\"w\") as csv_file:\n",
    "    header = ['query_term', 'total_hits']\n",
    "    writer = csv.writer(csv_file)\n",
    "    writer.writerow(header)\n",
    "    \n",
    "    for term in list_of_terms_nl:\n",
    "        params_nl[\"gsrsearch\"] = f'\"{term}\"'\n",
    "        r = requests.get(url,params=params_nl,headers=headers)\n",
    "        hits = r.json()['query']['searchinfo']['totalhits']\n",
    "        data = [term,hits]\n",
    "        \n",
    "        writer.writerow(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating total hits per term with filtering on keywords and properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating the search strings (srsearch) with filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# json file with properties to exlude\n",
    "with open(\"/Users/anesterov/reps/LODlit/Wikidata/to_exlude.json\",\"r\") as json_file:\n",
    "     to_exclude = json.load(json_file)\n",
    "        \n",
    "property_list = ['P31','P279']\n",
    "filter_str_en = \"-scientific -scholarly -article\"\n",
    "filter_str_nl = \"-wetenschappelijk -artikel\"\n",
    "\n",
    "# writing strings for \n",
    "for p in property_list:\n",
    "    for k in to_exclude.keys():\n",
    "        add_str = f\" -haswbstatement:{p}={k}\"\n",
    "        filter_str_en = filter_str_en + add_str\n",
    "        filter_str_nl = filter_str_nl + add_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### EN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterating over the list of terms\n",
    "# updating the 'srsearch' param\n",
    "with open (\"total_hits_filter_en.csv\",\"w\") as csv_file:\n",
    "    header = ['query_term', 'total_hits']\n",
    "    writer = csv.writer(csv_file)\n",
    "    writer.writerow(header)\n",
    "    \n",
    "    for term in list_of_terms_en:\n",
    "        params_en[\"gsrsearch\"] = f'\"{term}\" ' + filter_str_en\n",
    "        r = requests.get(url,params=params_en,headers=headers)\n",
    "        hits = r.json()['query']['searchinfo']['totalhits']\n",
    "        data = [term,hits]\n",
    "        \n",
    "        writer.writerow(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### NL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterating over the list of terms\n",
    "# updating the 'srsearch' param\n",
    "with open (\"total_hits_filter_nl.csv\",\"w\") as csv_file:\n",
    "    header = ['query_term', 'total_hits']\n",
    "    writer = csv.writer(csv_file)\n",
    "    writer.writerow(header)\n",
    "    \n",
    "    for term in list_of_terms_nl:\n",
    "        params_nl[\"gsrsearch\"] = f'\"{term}\" ' + filter_str_nl\n",
    "        r = requests.get(url,params=params_nl,headers=headers)\n",
    "        hits = r.json()['query']['searchinfo']['totalhits']\n",
    "        data = [term,hits]\n",
    "        \n",
    "        writer.writerow(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Searching EN terms and getting labels, aliases, and descriptions for every found entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 'query' with 'search' generator: constant params\n",
    "url = \"https://www.wikidata.org/w/api.php\"\n",
    "params = {\"action\":\"query\",\n",
    "          \"prop\":\"entityterms\",\n",
    "          \"wbetlanguage\":\"en\", #English\n",
    "          \"generator\":\"search\",\n",
    "          \"gsrsearch\":'', # term goes here (quotes for stemming off)\n",
    "          \"gsrlimit\":\"max\", # getting all results\n",
    "          \"gsroffset\":\"0\", # offset\n",
    "          \"gsrinfo\":\"totalhits\",\n",
    "          \"gsrsort\":\"incoming_links_desc\", # sorting results by incoming links\n",
    "          \"format\":\"json\",} \n",
    "headers = {\"user-agent\":\"bot getting labels aliases and descriptions of the requested pages (CWI; Human-Centered Data Analytics; nesterov@cwi.nl)\"}\n",
    "\n",
    "results = {} # dict to store the results\n",
    "\n",
    "with open(\"wikidata_search_results_en.json\", 'w') as results_file:\n",
    "    \n",
    "    for term in list_of_terms_en:\n",
    "        # counter for offset\n",
    "        gsroffset = 0\n",
    "        params[\"gsroffset\"] = gsroffset\n",
    "        # filtering the search results\n",
    "        params[\"gsrsearch\"] = f'\"{term}\" ' + filter_str_en # quotes for stemming off\n",
    "        # sending a request\n",
    "        w = requests.get(url,params=params,headers=headers)\n",
    "        wikidata_json = w.json()\n",
    "        time.sleep(2) # to prevent 502\n",
    "        \n",
    "        # checking the number of hits\n",
    "        hits = wikidata_json['query']['searchinfo']['totalhits']\n",
    "        print(\"term:\",term,\"|\",\"hits:\",hits)\n",
    "        \n",
    "        # if there are no results\n",
    "        if hits == 0:\n",
    "            results[term] = wikidata_json['query']\n",
    "            loops = 0\n",
    "            \n",
    "        # saving results for every term from the first query (the first loop)    \n",
    "        else:\n",
    "            results[term] = wikidata_json['query']['pages']\n",
    "            \n",
    "        # if there are less than 500 hits for a term, this will be the resulting dataset\n",
    "        if hits < 500:\n",
    "            loops = 0\n",
    "            print(\"saved\")\n",
    "            \n",
    "# - CONDITIONS - #\n",
    "\n",
    "        # 10K is max; and if hits > 500, offset is needed\n",
    "        if 10000 > hits > 500 and hits % 500 > 0:\n",
    "            loops = hits // 500\n",
    "            \n",
    "        # minus one loop if there's no remainder \n",
    "        if 10000 > hits > 500 and hits % 500 == 0:\n",
    "            loops = hits // 500 - 1\n",
    "            \n",
    "        # as the first loop is already done, max = 19\n",
    "        if hits > 10000:\n",
    "            loops = 19 \n",
    "\n",
    "# - REQUEST LOOPS - #   \n",
    "\n",
    "        for i in range(0,loops):\n",
    "            gsroffset = gsroffset + 500\n",
    "\n",
    "            # setting the offset and sending a new request\n",
    "            params[\"gsroffset\"] = gsroffset\n",
    "            w_i = requests.get(url,params=params,headers=headers)\n",
    "            wikidata_json_i = w_i.json()\n",
    "            \n",
    "            # saving the results\n",
    "            results[term].update(wikidata_json_i['query']['pages'])\n",
    "            time.sleep(2)\n",
    "            print(\"offset:\",gsroffset,\"saved\")\n",
    "        \n",
    "    json.dump(results, results_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Searching NL terms and getting labels, aliases, and descriptions for every found entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 'query' with 'search' generator: constant params\n",
    "url = \"https://www.wikidata.org/w/api.php\"\n",
    "params = {\"action\":\"query\",\n",
    "          \"prop\":\"entityterms\",\n",
    "          \"wbetlanguage\":\"nl\", # Dutch\n",
    "          \"generator\":\"search\",\n",
    "          \"gsrsearch\":'', # term goes here (quotes for stemming off)\n",
    "          \"gsrlimit\":\"max\", # getting all results\n",
    "          \"gsroffset\":\"0\", # offset\n",
    "          \"gsrinfo\":\"totalhits\",\n",
    "          \"gsrsort\":\"incoming_links_desc\", # sorting results by incoming links\n",
    "          \"format\":\"json\",} \n",
    "headers = {\"user-agent\":\"bot getting labels aliases and descriptions of the requested pages (CWI; Human-Centered Data Analytics; nesterov@cwi.nl)\"}\n",
    "\n",
    "results = {} # dict to store the results\n",
    "\n",
    "with open(\"wikidata_search_results_nl.json\", 'w') as results_file:\n",
    "    \n",
    "    for term in list_of_terms_nl:\n",
    "        # counter for offset\n",
    "        gsroffset = 0\n",
    "        params[\"gsroffset\"] = gsroffset\n",
    "        # filtering the search results\n",
    "        params[\"gsrsearch\"] = f'\"{term}\" ' + filter_str_nl # quotes for stemming off\n",
    "        # sending a request\n",
    "        w = requests.get(url,params=params,headers=headers)\n",
    "        wikidata_json = w.json()\n",
    "        time.sleep(2) # to prevent 502\n",
    "        \n",
    "        # checking the number of hits\n",
    "        hits = wikidata_json['query']['searchinfo']['totalhits']\n",
    "        print(\"term:\",term,\"|\",\"hits:\",hits)\n",
    "        \n",
    "        # if there are no results\n",
    "        if hits == 0:\n",
    "            results[term] = wikidata_json['query']\n",
    "            loops = 0\n",
    "            \n",
    "        # saving results for every term from the first query (the first loop)    \n",
    "        else:\n",
    "            results[term] = wikidata_json['query']['pages']\n",
    "            \n",
    "        # if there are less than 500 hits for a term, this will be the resulting dataset\n",
    "        if hits < 500:\n",
    "            loops = 0\n",
    "            print(\"saved\")\n",
    "            \n",
    "# - CONDITIONS - #\n",
    "\n",
    "        # 10K is max; and if hits > 500, offset is needed\n",
    "        if 10000 > hits > 500 and hits % 500 > 0:\n",
    "            loops = hits // 500\n",
    "            \n",
    "        # one loop less if there's no remainder \n",
    "        if 10000 > hits > 500 and hits % 500 == 0:\n",
    "            loops = hits // 500 - 1\n",
    "            \n",
    "        # as the first loop is already done, max = 19\n",
    "        if hits > 10000:\n",
    "            loops = 19 \n",
    "\n",
    "# - REQUEST LOOPS - #   \n",
    "\n",
    "        for i in range(0,loops):\n",
    "            gsroffset = gsroffset + 500\n",
    "\n",
    "            # setting the offset and sending a new request\n",
    "            params[\"gsroffset\"] = gsroffset\n",
    "            w_i = requests.get(url,params=params,headers=headers)\n",
    "            wikidata_json_i = w_i.json()\n",
    "            \n",
    "            # saving the results\n",
    "            results[term].update(wikidata_json_i['query']['pages'])\n",
    "            time.sleep(2)\n",
    "            print(\"offset:\",gsroffset,\"saved\")\n",
    "        \n",
    "    json.dump(results, results_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

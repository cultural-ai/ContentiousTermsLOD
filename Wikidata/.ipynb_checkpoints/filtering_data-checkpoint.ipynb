{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "import zipfile\n",
    "import re\n",
    "import requests\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step1. Excluding entities â€“ based on 'instance of', 'subclass of'\n",
    "* Entities that have certain values of \"instance of\" and \"subclass of\" (\"to_exclude.json\")\n",
    "* Entities without any entity terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the file with the P31 and P279 values to exlude\n",
    "# the file was created manually based on the analysis of the search results\n",
    "\n",
    "with open('to_exlude.json') as jf_exlude:\n",
    "    to_exclude = json.load(jf_exlude)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unzipping the search results to a local directory\n",
    "with zipfile.ZipFile('wikidata_search_results_en.json.zip','r') as unzip:\n",
    "    # set your path (~62 MB)\n",
    "    unzip.extractall('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the results file\n",
    "# set your path\n",
    "with open('/wikidata_search_results.json') as jf:\n",
    "    wd_search_results_en = json.load(jf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the file with the query terms and putting them in a list\n",
    "\n",
    "with open('wikidata_query_terms_en.txt','r') as txt_file:\n",
    "    terms_en = txt_file.read()\n",
    "    \n",
    "queried_terms_en = [s.strip(\"\\'\") for s in terms_en.lstrip(\"[\").rstrip(\"]\").split(\", \")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing all the entities to exclude from the results in a list\n",
    "\n",
    "entities_to_exlude = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# iterating over the claim files\n",
    "# claims for every term are stored in separate files (they were saved in the previous step, 'query_wikidata_en_nl.ipynb')\n",
    "# claims are stored in list of dicts\n",
    "# the number of dicts corresponds to the number of requests made to retrieve claims\n",
    "\n",
    "for term in queried_terms_en:\n",
    "    print(term)\n",
    "    # set your path to the claims directory\n",
    "    with open(f'/claims_en/{term}_claims.json','r') as jf:\n",
    "        claims_file = json.load(jf)\n",
    "        \n",
    "    if claims_file[\"entities\"] != []: # if there are claims\n",
    "\n",
    "        for i in claims_file[\"entities\"]:\n",
    "            for e_id, values in i.items():\n",
    "                if 'claims' in values:\n",
    "                    for v in values['claims'].values():\n",
    "                        for l in v:\n",
    "                            if 'datavalue' in l['mainsnak']:\n",
    "                                # if the values of P31 or P279 match the ids to exclude,\n",
    "                                # add them to the list 'entities_to_exlude'\n",
    "                                if (l['mainsnak']['property'] == 'P31' or l['mainsnak']['property'] == 'P279') \\\n",
    "                                and l['mainsnak']['datavalue']['value']['id'] in to_exclude.keys():\n",
    "                                    entities_to_exlude.append(e_id)\n",
    "        print(len(entities_to_exlude),\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ~ 69K entities\n",
    "len(entities_to_exlude)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unzipping the search results to a local directory\n",
    "with zipfile.ZipFile('wikidata_search_results_nl.json.zip','r') as unzip:\n",
    "    # set your path (~39 MB)\n",
    "    unzip.extractall('/Users/anesterov/reps/testing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the results file\n",
    "# set your path\n",
    "with open('/wikidata_search_results_nl.json','r') as jf:\n",
    "    wd_search_results_nl = json.load(jf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the file with the query terms and putting them in a list\n",
    "\n",
    "with open('wikidata_query_terms_nl_with_hits.txt','r') as txt_file:\n",
    "    terms_nl = txt_file.read()\n",
    "    \n",
    "queried_terms_nl = terms_nl.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing all the NL entities to exclude from the results in a list\n",
    "\n",
    "entities_to_exclude_nl = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# iterating over the claim files\n",
    "# claims for every term are stored in separate files (they were saved in the previous step, 'query_wikidata_en_nl.ipynb')\n",
    "# claims are stored in list of dicts\n",
    "# the number of dicts corresponds to the number of requests made to retrieve claims\n",
    "\n",
    "for term in queried_terms_nl:\n",
    "    print(term)\n",
    "    # set your path to the claims directory\n",
    "    with open(f'/claims_nl/{term}_claims.json','r') as jf:\n",
    "        claims_file = json.load(jf)\n",
    "        \n",
    "    if claims_file[\"entities\"] != []: # if there are claims\n",
    "\n",
    "        for i in claims_file[\"entities\"]:\n",
    "            for e_id, values in i.items():\n",
    "                if 'claims' in values:\n",
    "                    for v in values['claims'].values():\n",
    "                        for l in v:\n",
    "                            if 'datavalue' in l['mainsnak']:\n",
    "                                # if the values of P31 or P279 match the ids to exclude,\n",
    "                                # add them to the list 'entities_to_exlude_nl'\n",
    "                                if (l['mainsnak']['property'] == 'P31' or l['mainsnak']['property'] == 'P279') \\\n",
    "                                and l['mainsnak']['datavalue']['value']['id'] in to_exclude.keys():\n",
    "                                    entities_to_exclude_nl.append(e_id)\n",
    "        print(len(entities_to_exclude_nl),\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ~ 16K entities\n",
    "len(entities_to_exclude_nl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1.1. Saving the clean version\n",
    "Also excluding entities without entity terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_results_en = {}\n",
    "for key,value in wd_search_results_en.items():\n",
    "    sub_dict = {}\n",
    "    for k,v in value.items():\n",
    "        if 'title' in v and 'entityterms' in v and v['title'] not in entities_to_exlude:\n",
    "            sub_dict[k] = v\n",
    "    filtered_results_en[key] = sub_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set your path (~47 MB)\n",
    "with open('/wikidata_search_results_en_clean.json','w') as jf:\n",
    "    json.dump(filtered_results_en, jf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging the lists of EN and NL entities to exclude\n",
    "exclude_all = entities_to_exlude + entities_to_exclude_nl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_results_nl = {}\n",
    "for key,value in wd_search_results_nl.items():\n",
    "    sub_dict = {}\n",
    "    for k,v in value.items():\n",
    "        if 'title' in v and 'entityterms' in v and v['title'] not in exclude_all:\n",
    "            sub_dict[k] = v\n",
    "    filtered_results_nl[key] = sub_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set your path (~28 MB)\n",
    "with open('/wikidata_search_results_nl_clean.json','w') as jf:\n",
    "    json.dump(filtered_results_nl, jf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1.2. Requesting additional claims\n",
    "* Not all the entities have claims, additional requests are needed\n",
    "* Check if an entity has claims, if not send a request "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### checking which entities have missing claims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the cleaned file or reuse the dict 'filtered_results_en'\n",
    "with open('/wikidata_search_results_en_clean.json','r') as jf:\n",
    "    wd_en_clean = json.load(jf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of all entities with claims (across all the files)\n",
    "\n",
    "all_entities_with_claims = []\n",
    "\n",
    "for query_term in wd_en_clean.keys():\n",
    "    # set your path to the EN claims directory\n",
    "    with open(f'/claims_en/{query_term}_claims.json','r') as jf:\n",
    "        claims_file = json.load(jf)\n",
    "        \n",
    "    for claim_list in claims_file[\"entities\"]:\n",
    "        all_entities_with_claims.extend(list(claim_list.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# dict {\"query_word\":[entities_without_claims]}\n",
    "\n",
    "no_claims = {}\n",
    "\n",
    "for query_term,value in wd_en_clean.items():\n",
    "    # list of entities per query word\n",
    "    list_of_entities_per_term = [i['title'] for i in value.values()] \n",
    "    print(\"term: \", query_term, \"|\", \"total entities: \", len(list_of_entities_per_term))\n",
    "    \n",
    "    # checking which entities do not have claims\n",
    "    entities_without_claims = []\n",
    "    for i in list_of_entities_per_term:\n",
    "        if i not in list(set(all_entities_with_claims)):\n",
    "            entities_without_claims.extend([i])\n",
    "    \n",
    "    print(\"no claims: \", len(entities_without_claims))\n",
    "    \n",
    "    if entities_without_claims != []:\n",
    "        no_claims[query_term] = entities_without_claims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 22 terms have missing claims\n",
    "# this file was created for documentation, and was not used further\n",
    "with open('no_claims.json','w') as json_write:\n",
    "    json.dump(no_claims, json_write)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### checking which entities have missing claims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the cleaned file or reuse the dict 'filtered_results_nl'\n",
    "with open('/wikidata_search_results_nl_clean.json','r') as jf:\n",
    "    wd_nl_clean = json.load(jf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of all entities with claims (across all the files)\n",
    "\n",
    "all_entities_with_claims_nl = []\n",
    "\n",
    "for query_term in wd_nl_clean.keys():\n",
    "    # set your path to the NL claims directory\n",
    "    with open(f'/claims_nl/{query_term}_claims.json','r') as jf:\n",
    "        claims_file = json.load(jf)\n",
    "        \n",
    "    for claim_list in claims_file[\"entities\"]:\n",
    "        all_entities_with_claims_nl.extend(list(claim_list.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# dict {\"query_word\":[entities_without_claims]}\n",
    "\n",
    "no_claims_nl = {}\n",
    "\n",
    "for query_term,value in wd_nl_clean.items():\n",
    "    # list of entities per query word\n",
    "    list_of_entities_per_term = [i['title'] for i in value.values()] \n",
    "    print(\"term: \", query_term, \"|\", \"total entities: \", len(list_of_entities_per_term))\n",
    "    \n",
    "    # checking which entities do not have claims\n",
    "    entities_without_claims = []\n",
    "    for i in list_of_entities_per_term:\n",
    "        if i not in list(set(all_entities_with_claims_nl)):\n",
    "            entities_without_claims.extend([i])\n",
    "    \n",
    "    print(\"no claims: \", len(entities_without_claims))\n",
    "    \n",
    "    if entities_without_claims != []:\n",
    "        no_claims_nl[query_term] = entities_without_claims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# checking if the NL claims are not in the EN claims also\n",
    "\n",
    "no_claims_nl_request = {}\n",
    "for key,list_of_entities in no_claims_nl.items():\n",
    "    not_found = [i for i in list_of_entities if i not in all_entities_with_claims]\n",
    "    if not_found != []:\n",
    "        no_claims_nl_request[key] = not_found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 19 NL terms are missing claims\n",
    "len(no_claims_nl_request)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requesting the missing claims"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'wbgetentities': constant params\n",
    "url = \"https://www.wikidata.org/w/api.php\"\n",
    "params = {\"action\":\"wbgetentities\",\n",
    "          \"ids\":\"\", # string of entities (max=50) goes here\n",
    "          \"props\":\"claims\",\n",
    "          \"languages\":\"en\",\n",
    "          \"format\":\"json\"\n",
    "         }\n",
    "# set your header\n",
    "headers = {\"user-agent\":\"bot getting claims of the requested entities (CWI; Human-Centered Data Analytics; nesterov@cwi.nl)\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_requested_entities = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "additional_claims = {}\n",
    "\n",
    "for term, no_claims_list in no_claims.items():\n",
    "    \n",
    "    params[\"ids\"] = \"\"\n",
    "    \n",
    "    # for debugging\n",
    "    \n",
    "    print(\"term:\",term,\"|\",\"entities:\",len(no_claims_list))\n",
    "\n",
    "# - CONDITIONS - #\n",
    "\n",
    "    # if len of the list > 50 and there's a remainder\n",
    "    if len(no_claims_list) > 50 and len(no_claims_list) % 50 > 0:\n",
    "        loops = len(no_claims_list) // 50 + 1 # add another loop for requests\n",
    "        \n",
    "    # if len of the list > 50 and no remainder    \n",
    "    if len(no_claims_list) > 50 and len(no_claims_list) % 50 == 0:\n",
    "        loops = len(no_claims_list) // 50\n",
    "        \n",
    "    # otherwise loops = 1 (len < 50 or len = 50)    \n",
    "    if len(no_claims_list) < 50:\n",
    "        loops = 1\n",
    "        \n",
    "    # for debugging\n",
    "    \n",
    "    print(\"loops:\",loops)\n",
    "        \n",
    "# - REQUEST LOOPS - #   \n",
    "        \n",
    "    # counters to slice list_of_entities\n",
    "    start = 0 \n",
    "    end = 0\n",
    "    query_result_list = []\n",
    "    \n",
    "    for i in range(0,loops):\n",
    "        ids_string = \"\" # putting Qs in one string\n",
    "        end = end + 50\n",
    "\n",
    "        for q in no_claims_list[start:end]:\n",
    "            if q not in list_of_requested_entities:\n",
    "                ids_string = ids_string + f\"{q}|\"\n",
    "                list_of_requested_entities.append(q) # remebering requested entities to prevent duplicates\n",
    "\n",
    "        start = start + 50\n",
    "\n",
    "        # updating params\n",
    "\n",
    "        params[\"ids\"] = ids_string.rstrip(\"|\")\n",
    "\n",
    "        # sending a request\n",
    "        d = requests.get(url,params=params,headers=headers)\n",
    "        claims = d.json() # claims per request\n",
    "        \n",
    "        if 'entities' in claims:\n",
    "            query_result_list.append(claims['entities']) # saving all claims\n",
    "    \n",
    "    # for debugging        \n",
    "    print(\"actual_results:\",len(query_result_list),[len(i) for i in query_result_list])\n",
    "\n",
    "# - SAVING RESULTS - #\n",
    "        \n",
    "    additional_claims[term] = query_result_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging EN claims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set your path to the EN claims directory \n",
    "path_to_json = '/claims_en/'\n",
    "json_files = [path_to_json + jf for jf in os.listdir(path_to_json) if jf.endswith('.json')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_en = {}\n",
    "for jf in json_files:\n",
    "    with open(jf,'r') as claim_file:\n",
    "        claims_per_term = json.load(claim_file)\n",
    "        merged_en[jf.split('/')[-1].split('_')[0]] = claims_per_term['entities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 104\n",
    "len(merged_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging additional claims with all the claims\n",
    "for term, claims in additional_claims.items():\n",
    "    if claims != []:\n",
    "        merged_en[term].extend(claims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving all the claims\n",
    "# set your path (~3,89 GB)\n",
    "with open('/claims_en/merged_claims_en.json', 'w') as output_file:\n",
    "    json.dump(merged_en, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'wbgetentities': constant params\n",
    "url = \"https://www.wikidata.org/w/api.php\"\n",
    "params = {\"action\":\"wbgetentities\",\n",
    "          \"ids\":\"\", # string of entities (max=50) goes here\n",
    "          \"props\":\"claims\",\n",
    "          \"languages\":\"en\",\n",
    "          \"format\":\"json\"\n",
    "         }\n",
    "# set your header\n",
    "headers = {\"user-agent\":\"bot getting claims of the requested entities (CWI; Human-Centered Data Analytics; nesterov@cwi.nl)\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_requested_entities_nl = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "additional_claims_nl = {}\n",
    "\n",
    "for term, no_claims_list in no_claims_nl_request.items():\n",
    "    \n",
    "    params[\"ids\"] = \"\"\n",
    "    \n",
    "    # for debugging\n",
    "    \n",
    "    print(\"term:\",term,\"|\",\"entities:\",len(no_claims_list))\n",
    "\n",
    "# - CONDITIONS - #\n",
    "\n",
    "    # if len of the list > 50 and there's a remainder\n",
    "    if len(no_claims_list) > 50 and len(no_claims_list) % 50 > 0:\n",
    "        loops = len(no_claims_list) // 50 + 1 # add another loop for requests\n",
    "        \n",
    "    # if len of the list > 50 and no remainder    \n",
    "    if len(no_claims_list) > 50 and len(no_claims_list) % 50 == 0:\n",
    "        loops = len(no_claims_list) // 50\n",
    "        \n",
    "    # otherwise loops = 1 (len < 50 or len = 50)    \n",
    "    if len(no_claims_list) < 50:\n",
    "        loops = 1\n",
    "        \n",
    "    # for debugging\n",
    "    \n",
    "    print(\"loops:\",loops)\n",
    "        \n",
    "# - REQUEST LOOPS - #   \n",
    "        \n",
    "    # counters to slice list_of_entities\n",
    "    start = 0 \n",
    "    end = 0\n",
    "    query_result_list = []\n",
    "    \n",
    "    for i in range(0,loops):\n",
    "        ids_string = \"\" # putting Qs in one string\n",
    "        end = end + 50\n",
    "\n",
    "        for q in no_claims_list[start:end]:\n",
    "            if q not in list_of_requested_entities_nl:\n",
    "                ids_string = ids_string + f\"{q}|\"\n",
    "                list_of_requested_entities_nl.append(q) # remebering requested entities to prevent duplicates\n",
    "\n",
    "        start = start + 50\n",
    "\n",
    "        # updating params\n",
    "\n",
    "        params[\"ids\"] = ids_string.rstrip(\"|\")\n",
    "\n",
    "        # sending a request\n",
    "        d = requests.get(url,params=params,headers=headers)\n",
    "        claims = d.json() # claims per request\n",
    "        \n",
    "        if 'entities' in claims:\n",
    "            query_result_list.append(claims['entities']) # saving all claims\n",
    "    \n",
    "    # for debugging        \n",
    "    print(\"actual_results:\",len(query_result_list),[len(i) for i in query_result_list])\n",
    "\n",
    "# - SAVING RESULTS - #\n",
    "        \n",
    "    additional_claims_nl[term] = query_result_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging NL claims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set your path to the NL claims directory \n",
    "path_to_json = '/claims_nl/'\n",
    "json_files = [path_to_json + jf for jf in os.listdir(path_to_json) if jf.endswith('.json')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_nl = {}\n",
    "for jf in json_files:\n",
    "    with open(jf,'r') as claim_file:\n",
    "        claims_per_term = json.load(claim_file)\n",
    "        merged_nl[jf.split('/')[-1].split('_')[0]] = claims_per_term['entities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 104\n",
    "len(merged_nl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging additional claims with all the claims\n",
    "for term, claims in additional_claims_nl.items():\n",
    "    if claims != []:\n",
    "        merged_nl[term].extend(claims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving all the claims\n",
    "# set your path (~1,24 GB)\n",
    "with open('/claims_nl/merged_claims_nl.json', 'w') as output_file:\n",
    "    json.dump(merged_nl, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1.3. Re-running Step 1 with new claims"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for term, lists in additional_claims_nl.items():\n",
    "    for i in lists:\n",
    "        for e_id, claims in i.items():\n",
    "            for v in values['claims'].values():\n",
    "                for l in v:\n",
    "                    if 'datavalue' in l['mainsnak']:\n",
    "                        if (l['mainsnak']['property'] == 'P31' or l['mainsnak']['property'] == 'P279') \\\n",
    "                        and l['mainsnak']['datavalue']['value']['id'] in to_exclude.keys():\n",
    "                            entities_to_exlude.append(e_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(entities_to_exlude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the list of entities that will be filtered out\n",
    "# this file was created for documentation, and was not used further\n",
    "\n",
    "with open('exclude_entities_en.txt','w') as txt_file:\n",
    "    for i in entities_to_exlude:\n",
    "        txt_file.writelines(f\"{i},\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for term, lists in additional_claims_nl.items():\n",
    "    for i in lists:\n",
    "        for e_id, claims in i.items():\n",
    "            for v in claims['claims'].values():\n",
    "                for l in v:\n",
    "                    if 'datavalue' in l['mainsnak']:\n",
    "                        if (l['mainsnak']['property'] == 'P31' or l['mainsnak']['property'] == 'P279') \\\n",
    "                        and l['mainsnak']['datavalue']['value']['id'] in to_exclude.keys():\n",
    "                            entities_to_exclude_nl.append(e_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(entities_to_exclude_nl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the list of entities that will be filtered out\n",
    "# this file was created for documentation, and was not used further\n",
    "\n",
    "with open('exclude_entities_nl.txt','w') as txt_file:\n",
    "    for i in entities_to_exclude_nl:\n",
    "        txt_file.writelines(f\"{i},\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1.4 Rewriting the clean version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_results_en = {}\n",
    "for key,value in wd_search_results_en.items():\n",
    "    sub_dict = {}\n",
    "    for k,v in value.items():\n",
    "        if 'title' in v and 'entityterms' in v and v['title'] not in entities_to_exlude:\n",
    "            sub_dict[k] = v\n",
    "    filtered_results_en[key] = sub_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set your path\n",
    "with open('/wikidata_search_results_en_clean.json','w') as jf:\n",
    "    json.dump(filtered_results_en, jf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging the lists of EN and NL entities to exclude\n",
    "exclude_all = entities_to_exlude + entities_to_exclude_nl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_results_nl = {}\n",
    "for key,value in wd_search_results_nl.items():\n",
    "    sub_dict = {}\n",
    "    for k,v in value.items():\n",
    "        if 'title' in v and 'entityterms' in v and v['title'] not in exclude_all:\n",
    "            sub_dict[k] = v\n",
    "    filtered_results_nl[key] = sub_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set your path\n",
    "with open('/wikidata_search_results_nl_clean.json','w') as jf:\n",
    "    json.dump(filtered_results_nl, jf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2. Excluding entities â€“ proper names\n",
    "* Excluding names: if the target word is used in a label with uppercase AND if an entity has the properties \"family name\" and \"given name\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Files with entity literals:\n",
    "# filtered_results_en\n",
    "# filtered_results_nl\n",
    "\n",
    "# Files with claims:\n",
    "# all_claims â€“ English\n",
    "# all_claims_nl â€“ Duth\n",
    "\n",
    "# properties\n",
    "# P735 â€“ \"given name\"\n",
    "# P734 â€“ \"family name\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_en = []\n",
    "\n",
    "# iterating over the \"filtered_results_en\" dict\n",
    "# that was exported to a file 'wikidata_search_results_en_clean.json' in the previous step\n",
    "\n",
    "for target_word, entityterms in filtered_results_en.items():\n",
    "\n",
    "    # taking unigrams only\n",
    "    # n-grams are put in \"\", and complex words have '-'\n",
    "    if '\"' not in target_word and '-' not in target_word:\n",
    "        \n",
    "        for page_id, values in entityterms.items():\n",
    "            if 'label' in values['entityterms'] and \\\n",
    "            re.search(target_word.capitalize(),values['entityterms']['label'][0]) != None:\n",
    "                names_en.append(values['title'])\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 70K entities have capitalized target terms in their labels\n",
    "len(names_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_to_filter = []\n",
    "\n",
    "for target_word, claims_lists in all_claims.items():\n",
    "    for claims in claims_lists:\n",
    "        for e_id in claims.keys():\n",
    "            # if there are the properties \"family name\" or \"given name\"\n",
    "            if e_id in names_en and ('P734' in claims[e_id]['claims'].keys() or \\\n",
    "                                     'P735' in claims[e_id]['claims'].keys()):\n",
    "                names_to_filter.append(e_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# excluding the entities from the results\n",
    "\n",
    "clean_en = {}\n",
    "for target_word, entityterms in filtered_results_en.items():\n",
    "    sub_dict = {}\n",
    "    for page_id, values in entityterms.items():\n",
    "        if values['title'] not in names_to_filter:\n",
    "            sub_dict[page_id] = values\n",
    "            \n",
    "    clean_en[target_word] = sub_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rewriting the clean file\n",
    "# set your path\n",
    "with open('/wikidata_search_results_en_clean.json','w') as jf:\n",
    "    json.dump(clean_en, jf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_nl = []\n",
    "\n",
    "# iterating over the \"filtered_results_en\" dict\n",
    "# that was exported to a file 'wikidata_search_results_nl_clean.json' in the previous step\n",
    "\n",
    "for target_word, entityterms in filtered_results_nl.items():\n",
    "\n",
    "    # taking unigrams only\n",
    "    # n-grams are put in \"\", and complex words have '-'\n",
    "    if '\"' not in target_word and '-' not in target_word:\n",
    "        \n",
    "        for page_id, values in entityterms.items():\n",
    "            if 'label' in values['entityterms'] and \\\n",
    "            re.search(target_word.capitalize(),values['entityterms']['label'][0]) != None:\n",
    "                names_nl.append(values['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NL entities with capitalized target terms\n",
    "len(names_nl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_to_filter_nl = []\n",
    "\n",
    "# checking both EN and NL claims\n",
    "\n",
    "# English claims\n",
    "\n",
    "for target_word, claims_lists in all_claims.items():\n",
    "    for claims in claims_lists:\n",
    "        for e_id in claims.keys():\n",
    "            # if there are the properties \"family name\" or \"given name\"\n",
    "            if e_id in names_nl and ('P734' in claims[e_id]['claims'].keys() or \\\n",
    "                                     'P735' in claims[e_id]['claims'].keys()):\n",
    "                names_to_filter_nl.append(e_id)\n",
    "                \n",
    "# Dutch claims\n",
    "                \n",
    "for target_word, claims_lists in all_claims_nl.items():\n",
    "    for claims in claims_lists:\n",
    "        for e_id in claims.keys():\n",
    "            # if there are the properties \"family name\" or \"given name\"\n",
    "            if e_id in names_nl and ('P734' in claims[e_id]['claims'].keys() or \\\n",
    "                                     'P735' in claims[e_id]['claims'].keys()):\n",
    "                names_to_filter_nl.append(e_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(names_to_filter_nl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# excluding the entities from the results\n",
    "\n",
    "clean_nl = {}\n",
    "for target_word, entityterms in filtered_results_nl.items():\n",
    "    sub_dict = {}\n",
    "    for page_id, values in entityterms.items():\n",
    "        if values['title'] not in names_to_filter_nl:\n",
    "            sub_dict[page_id] = values\n",
    "            \n",
    "    clean_nl[target_word] = sub_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rewriting the clean file\n",
    "# set your path\n",
    "with open('/wikidata_search_results_nl_clean.json','w') as jf:\n",
    "    json.dump(clean_nl, jf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

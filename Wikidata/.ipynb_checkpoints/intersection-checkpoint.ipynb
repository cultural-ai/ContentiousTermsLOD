{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the 'intersection' module\n",
    "\n",
    "import intersection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing necessary files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing entities from the clean results\n",
    "\n",
    "with zipfile.ZipFile('wikidata_search_results_en_clean.json.zip','r') as unzip:\n",
    "    unzip.extractall('') # set your path (~47 MB)\n",
    "\n",
    "# set your path\n",
    "with open('/wikidata_search_results_en_clean.json','r') as json_file:\n",
    "    wd_en = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imported simplified claims\n",
    "\n",
    "with zipfile.ZipFile('simple_claims_en.json.zip','r') as unzip:\n",
    "    unzip.extractall('') # set your path (~15 MB)\n",
    "\n",
    "with open('/simple_claims_en.json','r') as json_file:\n",
    "    claims_en = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv file with close match entities (was created manually)\n",
    "\n",
    "close_match_en = pd.read_csv('close_match_en.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# json file with close match concepts' literals from AAT\n",
    "\n",
    "with open('aat_close_match_bow_en.json') as json_file:\n",
    "    aat_close_match_en = json.load(json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating intersection\n",
    "output -> 'intersection_all_en.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set your path (~37 MB)\n",
    "with open('/intersection_all_en.csv','w') as csv_file:\n",
    "    \n",
    "    header = ['target_term', 'entity_id', 'matching_synsets', 'synset_score','core_entity_score','aat_score','label','descr','aliases','P31','P31_id','P279','P279_id','match']\n",
    "    writer = csv.writer(csv_file)\n",
    "    writer.writerow(header)\n",
    "\n",
    "    for term, value in wd_en.items():\n",
    "\n",
    "        # STEP 1. Wikidata close match BoW\n",
    "\n",
    "        wd_close_match_id = close_match_en.loc[close_match_en['target_term_en'].isin([term])]['wikidata_close_match'].tolist()[0]\n",
    "\n",
    "        if wd_close_match_id != 'none':\n",
    "            # getting literals of the Wikidata entity\n",
    "            wd_literals = intersection.get_close_match_literals(wd_en,claims_en,wd_close_match_id)\n",
    "            # lemmatizing and cleaning\n",
    "            wd_bow = intersection.lemmatize_and_clean_en(wd_literals, term)     \n",
    "        else:\n",
    "            wd_bow = []\n",
    "        \n",
    "\n",
    "        # STEP 2. WordNet synsets BoWs\n",
    "\n",
    "        wn_synsets_bows = intersection.wordnet_tokens_en(term) # can be empty {}\n",
    "\n",
    "        # STEP 3. AAT BoWs\n",
    "\n",
    "        if term in aat_close_match_en.keys():\n",
    "            aat_bow = intersection.lemmatize_and_clean_en(aat_close_match_en[term], term)\n",
    "        else:\n",
    "            aat_bow = []\n",
    "\n",
    "        # STEP 4. Getting BoW of an entity to disambiguate\n",
    "        for entity_info in value.values():\n",
    "            entity_terms = intersection.get_entity_literals_by_target(wd_en,claims_en,term,entity_info['title'],True)\n",
    "            entity_bow = intersection.lemmatize_and_clean_en(entity_terms,term)\n",
    "\n",
    "            # STEP 5: checking if the entity BoW intersects with (1) WordNet BoWs, (2) Wikidata BoW,\n",
    "                    # (3) AAT BoW\n",
    "\n",
    "            # WN synsets BoWs\n",
    "            synset_scores = {}\n",
    "            for synset, tokens in wn_synsets_bows.items():\n",
    "                score = 0\n",
    "                for token in tokens:\n",
    "                    if token in entity_bow:\n",
    "                        synset_scores[synset] = score + 1\n",
    "\n",
    "            # selecting synsets with max score\n",
    "            matching_synsets = []\n",
    "            for k,v in synset_scores.items():\n",
    "                if v == max(synset_scores.values()):\n",
    "                    matching_synsets.append(k)\n",
    "\n",
    "            # Wikidata BoW and AAT BoW\n",
    "\n",
    "            wd_score = 0\n",
    "            aat_score = 0\n",
    "            for token in entity_bow:\n",
    "                if token in wd_bow:\n",
    "                    wd_score = wd_score + 1\n",
    "                if token in aat_bow:\n",
    "                    aat_score = aat_score + 1\n",
    "\n",
    "            # Does entity match?\n",
    "\n",
    "            match = False\n",
    "\n",
    "            target_synsets = close_match_en.loc[close_match_en['target_term_en'].isin([term])]['wn_close_match'].tolist()[0]\n",
    "\n",
    "            for synset in matching_synsets:\n",
    "                if synset in target_synsets:\n",
    "                    match = True\n",
    "\n",
    "            if wd_score > 0 or aat_score > 0:\n",
    "                match = True\n",
    "\n",
    "            # synset_score\n",
    "\n",
    "            if list(synset_scores.values()) == []:\n",
    "                synset_score = 0\n",
    "            else:\n",
    "                synset_score = max(synset_scores.values())\n",
    "\n",
    "            # adding literals to the file\n",
    "            \n",
    "            label = 'none'\n",
    "            if 'label' in entity_info['entityterms']:\n",
    "                label = entity_info['entityterms']['label'][0]\n",
    "\n",
    "            aliases = 'none'\n",
    "            if 'alias' in entity_info['entityterms']:\n",
    "                aliases = entity_info['entityterms']['alias']\n",
    "\n",
    "            descr = 'none'\n",
    "            if 'description' in entity_info['entityterms']:\n",
    "                descr = entity_info['entityterms']['description'][0]\n",
    "\n",
    "            P31 = 'none'\n",
    "            P279 = 'none'\n",
    "            P31_id = 'none'\n",
    "            P279_id = 'none'\n",
    "\n",
    "            if entity_info['title'] in claims_en:\n",
    "                P31 = list(claims_en[entity_info['title']]['P31'].values())\n",
    "                P279 = list(claims_en[entity_info['title']]['P279'].values())\n",
    "                P31_id = list(claims_en[entity_info['title']]['P31'].keys())\n",
    "                P279_id = list(claims_en[entity_info['title']]['P279'].keys())\n",
    "\n",
    "            data = [term,entity_info['title'],matching_synsets,synset_score,wd_score,aat_score,label,descr,aliases,P31,P31_id,P279,P279_id,match]\n",
    "            writer.writerow(data)\n",
    "                \n",
    "        print(term, 'Done')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

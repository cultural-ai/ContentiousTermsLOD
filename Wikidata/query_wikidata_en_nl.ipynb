{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "import requests\n",
    "import time\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# English"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making a list of query terms\n",
    "Exports \"wikidata_query_terms_en.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading all the wordforms of the EN query terms\n",
    "# the imported file is the result of 'labels_in_LOD/getting_word_forms.ipynb'\n",
    "# see README in labels_in_LOD\n",
    "\n",
    "path_en_wordforms = 'https://raw.githubusercontent.com/cultural-ai/LODlit/main/en_wordforms.json'\n",
    "\n",
    "parse = requests.get(path_en_wordforms)\n",
    "wordforms_en = json.loads(parse.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full text search of MediaWiki captures word forms for English:\n",
    "# so, for one-word terms, get only lemmas\n",
    "# for compound terms (with space ' ' characters), get all wordforms, because they will be queried in quotes (\"\"), and\n",
    "# stemming in MediaWiki in this case won't be applied\n",
    "\n",
    "all_lemmas_en = []\n",
    "\n",
    "for key, value in wordforms_en.items():\n",
    "    if value['lemmata'] != []: # if there is lemmata\n",
    "        for i in value['lemmata']: # get lemmas for every PoS\n",
    "            \n",
    "            # if compound (with ' ') and noun\n",
    "            if ' ' in i['lemma'] and i['pos'] == 'noun': \n",
    "                # add all word forms for compound nouns; do not add adjective forms\n",
    "                all_lemmas_en = all_lemmas_en + [w for w in i['wordforms']]\n",
    "            \n",
    "            # only get lemmas for (1) compound adjectives and (2) one-word terms   \n",
    "            else:\n",
    "                all_lemmas_en.append(i['lemma'])\n",
    "                \n",
    "    # if there is no lemmata, just add the initial term to the list         \n",
    "    else:\n",
    "        all_lemmas_en.append(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_lemmas_en = [word.lower() for word in all_lemmas_en] # lowercase\n",
    "unique_lemmas_en = list(set(all_lemmas_en)) # only unique lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding quotes to compound terms; one-word terms are without quotes\n",
    "en_query_terms = [f'\"{l}\"' if ' ' in l else l for l in unique_lemmas_en]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 104 English query terms\n",
    "len(en_query_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writing a txt file\n",
    "with open(\"wikidata_query_terms_en.txt\", \"w\") as txt_file:\n",
    "    txt_file.write(str([w for w in en_query_terms]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting total hits for every term\n",
    "complex search without the words \"scientific scholarly article\" in the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query to get total hits for every term\n",
    "# constant params\n",
    "url = \"https://www.wikidata.org/w/api.php\"\n",
    "params = {\"action\":\"query\",\n",
    "             \"list\":\"search\",\n",
    "             \"srsearch\":\"\", # term goes here\n",
    "             \"srlimit\":\"1\", # 1 result per term is enough to get meta on totalhits\n",
    "             \"srinfo\":\"totalhits\",\n",
    "             \"srprop\":\"titlesnippet\",\n",
    "             \"format\":\"json\"}\n",
    "\n",
    "# adjust header\n",
    "headers = {\"user-agent\":\"bot getting totalhits for the search terms (CWI; Human-Centered Data Analytics; nesterov@cwi.nl)\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# iterating over the list of terms\n",
    "# updating the 'srsearch' param\n",
    "with open (\"total_hits_en.csv\",\"w\") as csv_file:\n",
    "    header = ['query_term', 'total_hits']\n",
    "    writer = csv.writer(csv_file)\n",
    "    writer.writerow(header)\n",
    "    \n",
    "    \n",
    "    for term in en_query_terms:\n",
    "        params[\"srsearch\"] = f\"{term} -scientific -scholarly -article\"\n",
    "        r = requests.get(url,params=params,headers=headers)\n",
    "        hits = r.json()['query']['searchinfo']['totalhits']\n",
    "        data = [term,hits]\n",
    "        \n",
    "        writer.writerow(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Searching en terms and getting labels, aliases, and descriptions for every matched entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'query' with 'search' generator: constant params\n",
    "# MediaWiki API documentation: https://www.wikidata.org/w/api.php?action=help&modules=main\n",
    "\n",
    "url = \"https://www.wikidata.org/w/api.php\"\n",
    "params = {\"action\":\"query\",\n",
    "          \"prop\":\"entityterms\",\n",
    "          \"wbetlanguage\":\"en\", #English\n",
    "          \"generator\":\"search\",\n",
    "          \"gsrsearch\":\"\", # term goes here\n",
    "          \"gsrlimit\":\"max\", # getting all results\n",
    "          \"gsroffset\":\"0\", # offset\n",
    "          \"gsrinfo\":\"totalhits\",\n",
    "          \"gsrsort\":\"incoming_links_desc\", # sorting results by incoming links\n",
    "          \"format\":\"json\",} \n",
    "# adjust header\n",
    "headers = {\"user-agent\":\"bot getting labels aliases and descriptions of the requested pages (CWI; Human-Centered Data Analytics; nesterov@cwi.nl)\"}\n",
    "\n",
    "results = {} # dict to store the results\n",
    "\n",
    "\n",
    "# the resulting file is zipped on GitHub\n",
    "with open(\"wikidata_search_results_en.json\", 'w') as results_file:\n",
    "    \n",
    "    for term in en_query_terms:\n",
    "        # counter for offset\n",
    "        gsroffset = 0\n",
    "        params[\"gsroffset\"] = gsroffset\n",
    "        # excluding the words \"scientific\", \"scholarly\", and \"article\" from the search results\n",
    "        params[\"gsrsearch\"] = f\"{term} -scientific -scholarly -article\"\n",
    "        # sending a request\n",
    "        w = requests.get(url,params=params,headers=headers)\n",
    "        wikidata_json = w.json()\n",
    "        time.sleep(2) # to prevent 502\n",
    "        \n",
    "        # checking the number of hits\n",
    "        hits = wikidata_json['query']['searchinfo']['totalhits']\n",
    "        print(\"term:\",term,\"|\",\"hits:\",hits)\n",
    "        \n",
    "        # if there are no results\n",
    "        if hits == 0:\n",
    "            results[term] = wikidata_json['query']\n",
    "            loops = 0\n",
    "            \n",
    "        # saving results for every term from the first query (the first loop)    \n",
    "        else:\n",
    "            results[term] = wikidata_json['query']['pages']\n",
    "            \n",
    "        # if there are less than 500 hits for a term, this will be the resulting dataset\n",
    "        if hits < 500:\n",
    "            loops = 0\n",
    "            print(\"saved\")\n",
    "            \n",
    "# - CONDITIONS - #\n",
    "\n",
    "        # 10K is max; and if hits > 500, offset is needed\n",
    "        if 10000 > hits > 500 and hits % 500 > 0:\n",
    "            loops = hits // 500\n",
    "            \n",
    "        # minus one loop if there's no remainder \n",
    "        if 10000 > hits > 500 and hits % 500 == 0:\n",
    "            loops = hits // 500 - 1\n",
    "            \n",
    "        # as the first loop is already done, max = 19\n",
    "        if hits > 10000:\n",
    "            loops = 19 \n",
    "\n",
    "# - REQUEST LOOPS - #   \n",
    "\n",
    "        for i in range(0,loops):\n",
    "            gsroffset = gsroffset + 500\n",
    "\n",
    "            # setting the offset and sending a new request\n",
    "            params[\"gsroffset\"] = gsroffset\n",
    "            w_i = requests.get(url,params=params,headers=headers)\n",
    "            wikidata_json_i = w_i.json()\n",
    "            \n",
    "            # saving the results\n",
    "            results[term].update(wikidata_json_i['query']['pages'])\n",
    "            time.sleep(2)\n",
    "            print(\"offset:\",gsroffset,\"saved\")\n",
    "        \n",
    "    json.dump(results, results_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting claims for every entity to retrieve info about properties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading the results file\n",
    "the files were zipped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with zipfile.ZipFile(\"wikidata_search_results_en.json.zip\",\"r\") as unzip:\n",
    "    unzip.extractall(\"\") # set your path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"\") as jf: # set the path of the unzipped file\n",
    "    wd_results = json.load(jf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Querying claims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'wbgetentities': constant params\n",
    "url = \"https://www.wikidata.org/w/api.php\"\n",
    "params = {\"action\":\"wbgetentities\",\n",
    "          \"ids\":\"\", # string of entities (max=50) goes here\n",
    "          \"props\":\"claims\",\n",
    "          \"languages\":\"en\",\n",
    "          \"format\":\"json\"\n",
    "         }\n",
    "# adjust header\n",
    "headers = {\"user-agent\":\"bot getting claims of the requested entities (CWI; Human-Centered Data Analytics; nesterov@cwi.nl)\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_requested_entities = [] # run once before querying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for term in en_query_terms[:]: # use slice to prevent 502 (~50 terms at a time)\n",
    "    \n",
    "    # entities per term\n",
    "    list_of_entities = [v['title'] for v in wd_results[term].values() if 'title' in v]\n",
    "\n",
    "    params[\"ids\"] = \"\"\n",
    "    all_claims_per_term = {}\n",
    "    \n",
    "    # for debugging\n",
    "    \n",
    "    print(\"term:\",term,\"|\",\"entities:\",len(list_of_entities))\n",
    "\n",
    "# - CONDITIONS - #\n",
    "\n",
    "    # if len of the list > 50 and there's a remainder\n",
    "    if len(list_of_entities) > 50 and len(list_of_entities) % 50 > 0:\n",
    "        loops = len(list_of_entities) // 50 + 1 # add another loop for requests\n",
    "        \n",
    "    # if len of the list > 50 and no remainder    \n",
    "    if len(list_of_entities) > 50 and len(list_of_entities) % 50 == 0:\n",
    "        loops = len(list_of_entities) // 50\n",
    "        \n",
    "    # otherwise loops = 1 (len < 50 or len = 50)    \n",
    "    if len(list_of_entities) < 50:\n",
    "        loops = 1\n",
    "        \n",
    "    # for debugging\n",
    "    \n",
    "    print(\"loops:\",loops)\n",
    "        \n",
    "# - REQUEST LOOPS - #   \n",
    "        \n",
    "    # counters to slice list_of_entities\n",
    "    start = 0\n",
    "    end = 0\n",
    "    query_result_list = []\n",
    "    \n",
    "    for i in range(0,loops):\n",
    "        ids_string = \"\" # putting Qs in one string\n",
    "        end = end + 50\n",
    "\n",
    "        for q in list_of_entities[start:end]:\n",
    "            if q not in list_of_requested_entities:\n",
    "                ids_string = ids_string + f\"{q}|\"\n",
    "                list_of_requested_entities.append(q) # remebering requested entities to prevent duplicates\n",
    "\n",
    "        start = start + 50\n",
    "\n",
    "        # updating params\n",
    "\n",
    "        params[\"ids\"] = ids_string.rstrip(\"|\")\n",
    "\n",
    "        # sending a request\n",
    "        d = requests.get(url,params=params,headers=headers)\n",
    "        claims = d.json() # claims per request\n",
    "        \n",
    "        if 'entities' in claims:\n",
    "            query_result_list.append(claims['entities']) # saving all claims\n",
    "    \n",
    "    # for debugging        \n",
    "    print(\"actual_results:\",len(query_result_list),[len(i) for i in query_result_list])\n",
    "\n",
    "# - SAVING RESULTS - #\n",
    "        \n",
    "    all_claims_per_term['entities'] = query_result_list\n",
    "    \n",
    "    # set your path\n",
    "    # saving all the claims per term in a separate file\n",
    "    # there will be as many files as query terms\n",
    "    with open(f'/claims_en/{term}_claims.json', 'w') as json_file:\n",
    "        json.dump(all_claims_per_term, json_file)\n",
    "        \n",
    "    # for debugging\n",
    "    print(len(list_of_requested_entities),\"SAVED\",\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### saving the 'list_of_requested_entities' in a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"requested_entities_en.txt\",\"w\") as txt_file:\n",
    "    txt_file.write(str(list_of_requested_entities))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dutch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making a list of query terms\n",
    "Exports \"wikidata_query_terms_nl_all.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading all the wordforms of the NL query terms\n",
    "# the imported file is the result of 'labels_in_LOD/getting_word_forms.ipynb'\n",
    "# see README in labels_in_LOD\n",
    "\n",
    "path_nl_wordforms = 'https://raw.githubusercontent.com/cultural-ai/LODlit/main/nl_wordforms.json'\n",
    "\n",
    "parse = requests.get(path_nl_wordforms)\n",
    "wordforms_nl = json.loads(parse.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full text search of MediaWiki doesn't capture word forms in Dutch:\n",
    "# so, we're getting all the Dutch wordforms\n",
    "\n",
    "all_lemmas_nl = []\n",
    "\n",
    "for key, value in wordforms_nl.items():\n",
    "    if value['lemmata'] != []: # if there is lemmata\n",
    "        for i in value['lemmata']: # get lemmas for every PoS\n",
    "            all_lemmas_nl = all_lemmas_nl + [w for w in i['wordforms']]\n",
    "    else:\n",
    "        all_lemmas_nl.append(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_lemmas_nl = [word.lower() for word in all_lemmas_nl] # lowercase\n",
    "unique_lemmas_nl = list(set(all_lemmas_nl)) # only unique lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding quotes to compound terms; one-word terms are without quotes\n",
    "nl_query_terms = [f'\"{l}\"' if ' ' in l else l for l in unique_lemmas_nl]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 280 NL query terms\n",
    "len(nl_query_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writing a txt file\n",
    "with open(\"wikidata_query_terms_nl_all.txt\", \"w\") as txt_file:\n",
    "    txt_file.write(str([w for w in nl_query_terms]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting total hits for every term\n",
    "complex search without the words \"wetenschappelijk artikel\" in the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query to get total hits for every term\n",
    "# using the search generator to request nl language \n",
    "# constant params\n",
    "url = \"https://www.wikidata.org/w/api.php\"\n",
    "params = {\"format\":\"json\",\n",
    "          \"action\":\"query\",\n",
    "          \"prop\":\"entityterms\",\n",
    "          \"wbetlanguage\":\"nl\",\n",
    "          \"generator\":\"search\",\n",
    "          \"gsrsearch\":\"\", # term goes here\n",
    "          \"gsrlimit\":\"1\", # just 1 hit\n",
    "          \"gsrinfo\":\"totalhits\"}\n",
    "# adjust header\n",
    "headers = {\"user-agent\":\"bot getting totalhits for the Dutch search terms (CWI; Human-Centered Data Analytics; nesterov@cwi.nl)\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterating over the list of terms\n",
    "# updating the 'srsearch' param\n",
    "nl_hits = {}\n",
    "for term in nl_query_terms:\n",
    "    params[\"gsrsearch\"] = f\"{term} -wetenschappelijk -artikel\"\n",
    "    r = requests.get(url,params=params,headers=headers)\n",
    "    hits = r.json()['query']['searchinfo']['totalhits']\n",
    "    nl_hits[term] = hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take only terms with hits for queries; 74 terms were not included\n",
    "terms_with_hits = [t for t,hits in nl_hits.items() if hits > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writing a txt file\n",
    "with open(\"wikidata_query_terms_nl_with_hits.txt\", \"w\") as txt_file:\n",
    "    txt_file.write(str([w for w in terms_with_hits]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Searching nl terms and getting labels, aliases, and descriptions for every matched entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'query' with 'search' generator: constant params\n",
    "url = \"https://www.wikidata.org/w/api.php\"\n",
    "params = {\"action\":\"query\",\n",
    "          \"prop\":\"entityterms\",\n",
    "          \"wbetlanguage\":\"nl\", # Dutch\n",
    "          \"generator\":\"search\",\n",
    "          \"gsrsearch\":\"\", # term goes here\n",
    "          \"gsrlimit\":\"max\", # getting all results\n",
    "          \"gsroffset\":\"0\", # offset\n",
    "          \"gsrinfo\":\"totalhits\",\n",
    "          \"gsrsort\":\"incoming_links_desc\", # sorting results by incoming links\n",
    "          \"format\":\"json\"}\n",
    "# adjust header\n",
    "headers = {\"user-agent\":\"bot getting labels aliases and descriptions of the requested pages (CWI; Human-Centered Data Analytics; nesterov@cwi.nl)\"}\n",
    "\n",
    "results = {} # dict to store the results\n",
    "\n",
    "# the resulting file is zipped on GitHub\n",
    "with open(\"wikidata_search_results_nl.json\", 'w') as results_file:\n",
    "    \n",
    "    for term in terms_with_hits:\n",
    "        # counter for offset\n",
    "        gsroffset = 0\n",
    "        params[\"gsroffset\"] = gsroffset\n",
    "        # excluding the words \"wetenschappelijk\" and \"artikel\" from the search results\n",
    "        params[\"gsrsearch\"] = f\"{term} -wetenschappelijk -artikel\"\n",
    "        w = requests.get(url,params=params,headers=headers)\n",
    "        wikidata_json = w.json()\n",
    "        time.sleep(2) # to prevent 502\n",
    "        \n",
    "        # checking the number of hits\n",
    "        hits = wikidata_json['query']['searchinfo']['totalhits']\n",
    "        print(\"term:\",term,\"|\",\"hits:\",hits)\n",
    "        \n",
    "        # saving results for every term from the first query (the first loop)\n",
    "        results[term] = wikidata_json['query']['pages']\n",
    "        \n",
    "        # if there are less than 500 hits for a term, this will be the resulting dataset\n",
    "        if hits < 500:\n",
    "            loops = 0\n",
    "            print(\"saved\")\n",
    "\n",
    "# - CONDITIONS - #\n",
    "\n",
    "        # 10K is max; and if hits > 500, offset is needed\n",
    "        if 10000 > hits > 500 and hits % 500 > 0:\n",
    "            loops = hits // 500\n",
    "            \n",
    "        # one loop less if there's no remainder \n",
    "        if 10000 > hits > 500 and hits % 500 == 0:\n",
    "            loops = hits // 500 - 1\n",
    "            \n",
    "        # as the first loop is already done, max = 19\n",
    "        if hits > 10000:\n",
    "            loops = 19 \n",
    "        \n",
    "# - REQUEST LOOPS - #   \n",
    "\n",
    "        for i in range(0,loops):\n",
    "            gsroffset = gsroffset + 500\n",
    "\n",
    "            # setting the offset and sending a new request\n",
    "            params[\"gsroffset\"] = gsroffset\n",
    "            w_i = requests.get(url,params=params,headers=headers)\n",
    "            wikidata_json_i = w_i.json()\n",
    "            \n",
    "            # saving the results\n",
    "            results[term].update(wikidata_json_i['query']['pages'])\n",
    "            time.sleep(2)\n",
    "            print(\"offset:\",gsroffset,\"saved\")\n",
    "        \n",
    "    json.dump(results, results_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many (unique) entities are there in the search results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unzipping the results file from GitHub\n",
    "with zipfile.ZipFile(\"wikidata_search_results_nl.json.zip\",\"r\") as unzip:\n",
    "    unzip.extractall(\"\") # set your path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"\") as jf: # set your path to the unzipped file\n",
    "    wd_results_nl = json.load(jf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a list of all entities from the results\n",
    "\n",
    "all_entities_nl = []\n",
    "for value in wd_results_nl.values():\n",
    "    list_of_entities = [v['title'] for v in value.values() if 'title' in v]\n",
    "    for e in list_of_entities:\n",
    "        all_entities_nl.append(e)\n",
    "len(all_entities_nl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only unique entities\n",
    "unique_entities_nl = list(set(all_entities_nl))\n",
    "len(unique_entities_nl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many entities were not alredy queried in the requests for English terms?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the list of requested entities\n",
    "\n",
    "with open(\"requested_entities_en.txt\",\"r\") as txt_file:\n",
    "    txt = txt_file.read()\n",
    "    \n",
    "requested_entities_en = [s.strip('\"\\'') for s in txt.lstrip(\"[\").rstrip(\"]\").split(\", \")]\n",
    "len(requested_entities_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting only the entities that were not queried\n",
    "\n",
    "nl_to_query = [nl_entity for nl_entity in unique_entities_nl if nl_entity not in requested_entities_en]\n",
    "len(nl_to_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"wikidata_query_terms_nl_with_hits.txt\", \"r\") as txt_file:\n",
    "     txt = txt_file.read()\n",
    "nl_query_terms = [t for t in txt.split('\\n')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Querying the claims for Dutch entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'wbgetentities': constant params\n",
    "url = \"https://www.wikidata.org/w/api.php\"\n",
    "params = {\"action\":\"wbgetentities\",\n",
    "          \"ids\":\"\", # string of entities (max=50) goes here\n",
    "          \"props\":\"claims\",\n",
    "          \"languages\":\"nl\", # Dutch\n",
    "          \"format\":\"json\"\n",
    "         }\n",
    "# adjust header\n",
    "headers = {\"user-agent\":\"bot getting claims of the requested entities (CWI; Human-Centered Data Analytics; nesterov@cwi.nl)\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_requested_entities_nl = [] # run once before querying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for term in nl_query_terms[:]: # use slice to prevent 502 (~50 terms at a time)\n",
    "    \n",
    "    # entities per term\n",
    "    list_of_entities = [v['title'] for v in wd_results_nl[term].values() if 'title' in v]\n",
    "    \n",
    "    # take only entities that were not queried before in the English queries\n",
    "    list_of_entities_unique = [t for t in list_of_entities if t in nl_to_query]\n",
    "    \n",
    "    # query only entities that were not queried in the Dutch queries\n",
    "    to_request = []\n",
    "    \n",
    "    for i in list_of_entities_unique:\n",
    "        if i not in list_of_requested_entities_nl:\n",
    "            list_of_requested_entities_nl.append(i) # remebering requested entities to prevent duplicates\n",
    "\n",
    "            to_request.append(i)\n",
    "\n",
    "    params[\"ids\"] = \"\"\n",
    "    all_claims_per_term = {}\n",
    "    \n",
    "    # for debugging\n",
    "    \n",
    "    print(\"term:\",term,\"|\",\"entities:\",len(to_request))\n",
    "\n",
    "# - CONDITIONS - #\n",
    "\n",
    "    # if len of the list > 50 and there's a remainder\n",
    "    if len(to_request) > 50 and len(to_request) % 50 > 0:\n",
    "        loops = len(to_request) // 50 + 1 # add another loop for requests\n",
    "        \n",
    "    # if len of the list > 50 and no remainder    \n",
    "    if len(to_request) > 50 and len(to_request) % 50 == 0:\n",
    "        loops = len(to_request) // 50\n",
    "        \n",
    "    # otherwise loops = 1 (len < 50 or len = 50)    \n",
    "    if len(to_request) < 50:\n",
    "        loops = 1\n",
    "        \n",
    "    # for debugging\n",
    "    \n",
    "    print(\"loops:\",loops)\n",
    "        \n",
    "# - REQUEST LOOPS - #   \n",
    "        \n",
    "    # counters to slice list_of_entities\n",
    "    start = 0 \n",
    "    end = 0\n",
    "    query_result_list = []\n",
    "    \n",
    "    for i in range(0,loops):\n",
    "        ids_string = \"\" # putting Qs in one string\n",
    "        end = end + 50\n",
    "\n",
    "        for q in to_request[start:end]:\n",
    "            ids_string = ids_string + f\"{q}|\"\n",
    "            \n",
    "        start = start + 50\n",
    "\n",
    "        # updating params\n",
    "\n",
    "        params[\"ids\"] = ids_string.rstrip(\"|\")\n",
    "\n",
    "        # sending a request\n",
    "        d = requests.get(url,params=params,headers=headers)\n",
    "        print(i+1,d.ok)\n",
    "        claims = d.json() # claims per request\n",
    "        time.sleep(2)\n",
    "        \n",
    "        if 'entities' in claims:\n",
    "            query_result_list.append(claims['entities']) # saving all claims\n",
    "    \n",
    "    # for debugging        \n",
    "    print(\"actual_results:\",len(query_result_list),[len(i) for i in query_result_list])\n",
    "\n",
    "# - SAVING RESULTS - #\n",
    "        \n",
    "    all_claims_per_term['entities'] = query_result_list\n",
    "\n",
    "    # set your path\n",
    "    # saving all the claims per term in a separate file\n",
    "    # there will be as many files as query terms\n",
    "    with open(f'/claims_nl/{term}_claims.json', 'w') as json_file:\n",
    "        json.dump(all_claims_per_term, json_file)\n",
    "        \n",
    "    # for debugging\n",
    "    print((len(list_of_requested_entities_nl),\"SAVED\",\"\\n\\n\")\n",
    "    \n",
    "print(\"COMPLETED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### saving the 'list_of_requested_entities_nl' in a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"requested_entities_nl.txt\",\"w\") as txt_file:\n",
    "    txt_file.write(str(list_of_requested_entities_nl))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

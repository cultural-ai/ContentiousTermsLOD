{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lodlitparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import time\n",
    "import re\n",
    "import sys\n",
    "from io import BytesIO\n",
    "from zipfile import ZipFile\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "# install NLTK, download wordnet31 ('https://github.com/nltk/nltk_data/blob/gh-pages/packages/corpora/wordnet31.zip');\n",
    "# put the content of 'wordnet31' to 'wordnet' in 'nltk_data/corpora' (there are issues with importing wordnet31 from nltk.corpus)\n",
    "from nltk.corpus import wordnet as wn\n",
    "# download OpenDutchWordnet from 'https://github.com/cultural-ai/OpenDutchWordnet'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wikidata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wd(qids:list,lang:str,user_agent:str) -> dict:\n",
    "    \n",
    "    \"\"\"\n",
    "    Requesting labels, aliases, descriptions of Wikidata entities\n",
    "    qids: list of entity IDs (str) (requests 50 entities at a time slicing the list)\n",
    "    lang: str with language code; for example, 'en' or 'nl';\n",
    "    (see language codes in Wikidata: https://www.wikidata.org/w/api.php?action=help&modules=wbgetentities)\n",
    "    user_agent: str with user-agent's info; required by Wikidata (see: https://meta.wikimedia.org/wiki/User-Agent_policy)\n",
    "    Returns a dict: {'QID': {'type': '',\n",
    "                     'id': 'QID',\n",
    "                     'labels': {'lang': {'language': 'lang', 'value': ''}},\n",
    "                     'descriptions': {'lang': {'language': 'lang','value': ''}},\n",
    "                     'aliases': {'lang': [{'language': 'lang', 'value': ''}\n",
    "    \"\"\"\n",
    "    \n",
    "    # 'wbgetentities' constant params\n",
    "    url = \"https://www.wikidata.org/w/api.php\"\n",
    "    params = {\"action\":\"wbgetentities\",\n",
    "              \"ids\":\"\", # string of entities (max=50)\n",
    "              \"props\":\"labels|aliases|descriptions\",\n",
    "              \"languages\":lang,\n",
    "              \"format\":\"json\"}\n",
    "    headers = {\"user-agent\":user_agent}\n",
    "\n",
    "    # - N LOOPS - #\n",
    "\n",
    "    # if there's a remainder\n",
    "    if len(qids) % 50 > 0:\n",
    "        loops = len(qids) // 50 + 1 # add another loop for requests\n",
    "    else:\n",
    "        loops = len(qids) // 50\n",
    "\n",
    "    # - REQUEST LOOPS - #   \n",
    "\n",
    "    # counters to slice qids\n",
    "\n",
    "    start_quid_str = 0\n",
    "    end_quid_str = 0\n",
    "\n",
    "    for i in range(0,loops):\n",
    "        ids_string = \"\" # putting Qs in one string\n",
    "        end_quid_str = end_quid_str + 50 # max 50 entities per request\n",
    "\n",
    "        for q in qids[start_quid_str:end_quid_str]:\n",
    "            ids_string = ids_string + f\"{q}|\"\n",
    "\n",
    "        start_quid_str = start_quid_str + 50\n",
    "\n",
    "        # updating params\n",
    "\n",
    "        params[\"ids\"] = ids_string.rstrip(\"|\")\n",
    "\n",
    "        # sending a request\n",
    "        d = requests.get(url,params=params,headers=headers)\n",
    "        literals = d.json() # claims per request\n",
    "\n",
    "    return literals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aat(aat_uri:list,lang:str) -> dict:\n",
    "    '''\n",
    "    Querying prefLabel, altLabel, scopeNote, rdfs comments of concepts in AAT;\n",
    "    Sends SPARQL queries to the AAT endpoint via SPARQLwrapper \n",
    "    aat_uri: list of AAT concepts IDs (str) ['ID']\n",
    "    lang: str 'en' or 'nl'\n",
    "    Returns a dict with query results: {'ID':{'lang':'en',\n",
    "                                              'prefLabel':'',\n",
    "                                              'altLabels':[],\n",
    "                                              'scopeNote':'',\n",
    "                                              'prefLabel_comment':'',\n",
    "                                              'altLabel_comment':''}\n",
    "    '''\n",
    "    \n",
    "    sparql = SPARQLWrapper(\"http://vocab.getty.edu/sparql\")\n",
    "    \n",
    "    if lang == 'en':\n",
    "        lang_code = '300388277'\n",
    "    if lang == 'nl':\n",
    "        lang_code = '300388256'\n",
    "        \n",
    "    result_dict = {}\n",
    "        \n",
    "    for uri in aat_uri:\n",
    "        \n",
    "        result_dict[uri] = {}\n",
    "        \n",
    "        query_string = '''SELECT ?prefLabel (GROUP_CONCAT(?altLabel;SEPARATOR=\"#\") AS ?altLabels)\n",
    "        ?scopeNote ?prefLabel_comment ?altLabel_comment\n",
    "        WHERE {aat:''' + uri + ''' xl:prefLabel ?pL .?pL dcterms:language aat:''' + lang_code + ''';\n",
    "        xl:literalForm ?prefLabel .\n",
    "        OPTIONAL {?pL rdfs:comment ?prefLabel_comment . }\n",
    "        OPTIONAL {aat:''' + uri + ''' xl:altLabel ?aL .\n",
    "        ?aL dcterms:language aat:''' + lang_code + ''';\n",
    "        xl:literalForm ?altLabel . \n",
    "        OPTIONAL { ?aL rdfs:comment ?altLabel_comment . }}\n",
    "        OPTIONAL {aat:''' + uri + ''' skos:scopeNote / dcterms:language aat:'''+ lang_code + ''';\n",
    "        skos:scopeNote / rdf:value ?scopeNote . }}\n",
    "        GROUP BY ?prefLabel ?scopeNote ?prefLabel_comment ?altLabel_comment'''\n",
    "        \n",
    "        sparql.setQuery(query_string)\n",
    "        sparql.setReturnFormat(JSON)\n",
    "        results = sparql.query().convert()\n",
    "        \n",
    "        altLabels = []\n",
    "        scopeNote = None\n",
    "        prefLabel_comment = None\n",
    "        altLabel_comment = None\n",
    "        \n",
    "        for result in results['results']['bindings']:\n",
    "            \n",
    "            if 'altLabels' in result:\n",
    "                altLabels = result['altLabels']['value'].split('#')\n",
    "            if 'scopeNote' in result:\n",
    "                scopeNote = result['scopeNote']['value']\n",
    "            if 'prefLabel_comment' in result:\n",
    "                prefLabel_comment = result['prefLabel_comment']['value']\n",
    "            if 'altLabel_comment' in result:\n",
    "                altLabel_comment = result['altLabel_comment']['value']\n",
    "\n",
    "            result_dict[uri]['lang'] = lang\n",
    "            result_dict[uri]['prefLabel'] = result['prefLabel']['value']\n",
    "            result_dict[uri]['altLabels'] = altLabels\n",
    "            result_dict[uri]['prefLabel_comment'] = prefLabel_comment\n",
    "            result_dict[uri]['altLabel_comment'] = altLabel_comment\n",
    "            result_dict[uri]['scopeNote'] = scopeNote\n",
    "    print(query_string)    \n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NMVW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nmvw(term_ids:list) -> dict:\n",
    "    '''\n",
    "    Getting info about terms by their handle IDs in NMVW-thesaurus\n",
    "    term_ids: list of term IDs (str)\n",
    "    Returns a dict with query results: {'ID': {'prefLabel': '',\n",
    "                                               'altLabel': [],\n",
    "                                               'notes': [],\n",
    "                                               'exactMatch': '',\n",
    "                                               'scheme': ''}}\n",
    "    '''\n",
    "    \n",
    "    # nmvw: importing thesaurus\n",
    "    path_to_nmvw = 'https://github.com/cultural-ai/wordsmatter/raw/main/NMVW/nmvw_thesaurus.json.zip'\n",
    "    nmvw_raw = requests.get(path_to_nmvw).content\n",
    "    nmvw_zip = ZipFile(BytesIO(nmvw_raw))\n",
    "    nmvw_json = json.loads(nmvw_zip.read(nmvw_zip.infolist()[0]).decode())\n",
    "\n",
    "    results_nmvw = {}\n",
    "    \n",
    "    for term_id in term_ids:\n",
    "        handle = 'https://hdl.handle.net/20.500.11840/termmaster' + term_id\n",
    "        results_nmvw[handle] = nmvw_json.get(handle)\n",
    "        \n",
    "    return results_nmvw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Princeton WordNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pwn(synsets:list) -> dict:\n",
    "    '''\n",
    "    Getting lemmata, definition, examples of a synset\n",
    "    synsets: a list of synsets IDs (str)\n",
    "    Return a dict: {'synset_id': {'lemmata': '',\n",
    "                                   'definition': '',\n",
    "                                   'examples': []}\n",
    "    Requires NLTK, wordnet corpus version 3.1\n",
    "    '''\n",
    "    \n",
    "    results_pwn = {}\n",
    "    \n",
    "    for s in synsets:\n",
    "        synset = wn.synset(s)\n",
    "        lemmata = [l.name() for l in synset.lemmas()]\n",
    "        definition = synset.definition()\n",
    "        examples = synset.examples()\n",
    "        \n",
    "        # writing results \n",
    "        results_pwn[s] = {'lemmata': lemmata,\n",
    "                         'definition': definition,\n",
    "                         'examples': examples}\n",
    "        \n",
    "    return results_pwn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ODWN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def odwn(le_ids:list, path_odwn:str) -> dict:\n",
    "    '''\n",
    "    Getting Lemma, sense definitions, examples, synset ID, synset definitions of ODWN Lexical Entries\n",
    "    le_ids: list of Lexical Entries IDs (str)\n",
    "    path_odwn: str path to the directory with OpenDutchWordnet (not including the module itself, for example 'user/Downloads')\n",
    "    Returns a dict: {'le_id': {'lemma': '',\n",
    "                                'sense_def': '',\n",
    "                                'examples': [],\n",
    "                                'synset_ID': '',\n",
    "                                'synset_def': []}\n",
    "    '''\n",
    "    # importing ODWN\n",
    "    sys.path.insert(0,path_odwn)\n",
    "    from OpenDutchWordnet import Wn_grid_parser\n",
    "    # creating an instance\n",
    "    instance = Wn_grid_parser(Wn_grid_parser.odwn)\n",
    "    \n",
    "    # importing all synset definitions\n",
    "    path_to_glosses = \"https://raw.githubusercontent.com/cultural-ai/wordsmatter/main/ODWN/odwn_synset_glosses.json\"\n",
    "    synset_glosses = requests.get(path_to_glosses).json()\n",
    "    \n",
    "    results_odwn = {}\n",
    "\n",
    "    for le_id in le_ids:\n",
    "        le = instance.les_find_le(le_id)\n",
    "        synset_id = le.get_synset_id()\n",
    "\n",
    "        sense_def = le.get_definition()\n",
    "        if sense_def == '':\n",
    "            sense_def = None\n",
    "\n",
    "        results_odwn[le_id] = {'lemma': le.get_lemma(),\n",
    "                                'sense_def': sense_def,\n",
    "                                'examples': le.get_sense_example(),\n",
    "                                'synset_ID': synset_id,\n",
    "                                'synset_def': synset_glosses.get(synset_id)}\n",
    "    return results_odwn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### check where a query term is found per resource:\n",
    "* Wikidata: labels, aliases\n",
    "* AAT: prefLabel, altLabel\n",
    "* NMVW: prefLabel, altLabel\n",
    "* Princeton WordNet: no checking, only lemmata\n",
    "* ODWN: no checking, only lemma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting properties where cont terms are used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_literals(resource:str) -> str:\n",
    "    '''\n",
    "    Getting literal values of related matches\n",
    "    resource: str name of the resource 'aat', 'wikidata', 'pwn', 'odwn', 'nmvw' \n",
    "    path_odwn: str path to the directory with OpenDutchWordnet (not including the module itself, for example 'user/Downloads')\n",
    "    Saves json files with literal values:\n",
    "    aat: 'aat_rm_en.json','aat_rm_nl.json'\n",
    "    wikidata: 'wikidata_rm_en.json','wikidata_rm_nl.json'\n",
    "    pwn: 'pwn_rm.json'\n",
    "    odwn: 'odwn_rm.json'\n",
    "    nmvw: 'nmvw_rm.json'\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # importing related matches\n",
    "    path_to_rm = 'https://github.com/cultural-ai/wordsmatter/raw/main/rm.json'\n",
    "    related_matches = requests.get(path_to_rm).json()\n",
    "\n",
    "    if resource == 'aat':\n",
    "        # AAT in EN\n",
    "        aat_en = [v['related_matches']['aat'][0] for v in related_matches.values() if v['lang'] == 'en' \\\n",
    "                  and 'None' not in v['related_matches']['aat']]\n",
    "        aat_en_results = aat(aat_en,'en')\n",
    "        with open('aat_rm_en.json', 'w') as jf:\n",
    "            json.dump(aat_en_results, jf)\n",
    "        print(\"AAT EN is saved\")\n",
    "\n",
    "        # AAT in NL\n",
    "        aat_nl = [v['related_matches']['aat'][0] for v in related_matches.values() if v['lang'] == 'nl' \\\n",
    "                  and 'None' not in v['related_matches']['aat']]\n",
    "        aat_nl_results = aat(aat_nl,'nl')\n",
    "        with open('aat_rm_nl.json', 'w') as jf:\n",
    "            json.dump(aat_nl_results, jf)\n",
    "        print(\"AAT NL is saved\")\n",
    "    \n",
    "    if resource == 'wikidata':\n",
    "        user_agent = input(\"Enter your user agent details for Wikidata:\")\n",
    "        \n",
    "        # Wikidata in EN\n",
    "        wikidata_en = [v['related_matches']['wikidata'][0] for v in related_matches.values() if v['lang'] == 'en' \\\n",
    "                  and 'None' not in v['related_matches']['wikidata']]\n",
    "        wikidata_en_results = wd(wikidata_en,'en',user_agent)\n",
    "        with open('wikidata_rm_en.json', 'w') as jf:\n",
    "            json.dump(wikidata_en_results, jf)\n",
    "        print(\"Wikidata EN is saved\")\n",
    "\n",
    "        # Wikidata in NL\n",
    "        wikidata_nl = [v['related_matches']['wikidata'][0] for v in related_matches.values() if v['lang'] == 'nl' \\\n",
    "                  and 'None' not in v['related_matches']['wikidata']]\n",
    "        wikidata_nl_results = wd(wikidata_nl,'nl',user_agent)\n",
    "        with open('wikidata_rm_nl.json', 'w') as jf:\n",
    "            json.dump(wikidata_nl_results, jf)\n",
    "        print(\"Wikidata NL is saved\")\n",
    "        \n",
    "    if resource == 'pwn':\n",
    "        # PWN\n",
    "        pwn_synsets = []\n",
    "\n",
    "        for v in related_matches.values():\n",
    "            if v['lang'] == 'en' and 'None' not in v['related_matches']['pwn']:\n",
    "                pwn_synsets.extend(v['related_matches']['pwn'])\n",
    "\n",
    "        pwn_results = pwn(pwn_synsets)\n",
    "        with open('pwn_rm.json', 'w') as jf:\n",
    "            json.dump(pwn_results, jf)\n",
    "        print(\"PWN is saved\")\n",
    "        \n",
    "    if resource == 'odwn':\n",
    "        path_odwn = input(\"Path to your local OpenDutchWordNet directory:\")\n",
    "        #6 ODWN\n",
    "        odwn_synsets = []\n",
    "\n",
    "        for v in related_matches.values():\n",
    "            if v['lang'] == 'nl' and 'None' not in v['related_matches']['odwn']:\n",
    "                odwn_synsets.extend(v['related_matches']['odwn'])\n",
    "\n",
    "        odwn_results = odwn(odwn_synsets,path_odwn)\n",
    "        with open('odwn_rm.json', 'w') as jf:\n",
    "            json.dump(odwn_results, jf)\n",
    "        print(\"ODWN is saved\")\n",
    "        \n",
    "    if resource == 'nmvw':\n",
    "        #7 NMVW\n",
    "        nmvw_handles = [v['related_matches']['nmvw'][0] for v in related_matches.values() if v['lang'] == 'nl' \\\n",
    "                  and 'None' not in v['related_matches']['nmvw']]\n",
    "        nmvw_results = nmvw(nmvw_handles)\n",
    "        with open('nmvw_rm.json', 'w') as jf:\n",
    "            json.dump(nmvw_results, jf)\n",
    "        print(\"NMVW is saved\")\n",
    "    \n",
    "    return (\"All files are saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Identifying properties where cont terms appear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hits() -> dict:\n",
    "    '''\n",
    "    Adding the info about where contentious terms were found to the related matches\n",
    "    (applicable to the query results from AAT, Wikidata, and NMVW)\n",
    "    source: str with the path to the json file with related matches 'related_matches.json'\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing related matches\n",
    "with open('/Users/anesterov/reps/wordsmatter/rm.json','r') as jf:\n",
    "    related_matches = json.load(jf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from LODlitParser import wd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_agent = \"bot testing search\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'aboriginal': 7783}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wd.get_search_hits(\"aboriginal\",\"en\",user_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

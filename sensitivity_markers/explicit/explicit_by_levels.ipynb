{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lemma_by_term(query_term:str, lang:str) -> str:\n",
    "    '''\n",
    "    Getting a lemma of a query term\n",
    "    lang: str, 'en' or 'nl'\n",
    "    Returns str, 'not found' if lemma was not found\n",
    "    '''\n",
    "    \n",
    "    return_lemma = 'not found'\n",
    "    \n",
    "    # importing query terms with lemmas\n",
    "    # change path to GitHub\n",
    "    \n",
    "    with open('/Users/anesterov/reps/LODlit/query_terms.json','r') as jf:\n",
    "        query_terms = json.load(jf)\n",
    "        \n",
    "    for lemma, qt in query_terms[lang].items():\n",
    "        if query_term in qt:\n",
    "            return_lemma = lemma\n",
    "            \n",
    "    return return_lemma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wikidata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Check if QIDs have explicit markers by lang by markers -> df\n",
    "# 2. remove duplicates by QID, lemma, lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wd_explicit = pd.DataFrame(columns=[\"resource\",\"lang\",\"lemma\",\"entity_id\",\"explicit_marker\",\"value\",\"level\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang = \"nl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing all search results\n",
    "with open(f\"/Users/anesterov/LODlit_local/wd/jan31/results_clean_{lang}.json\",'r') as jf:\n",
    "    wd_all = json.load(jf)\n",
    "\n",
    "# import subset\n",
    "with open(f\"/Users/anesterov/reps/LODlit/Wikidata/wd_{lang}_subset.json\",'r') as jf:\n",
    "    wd_subset = json.load(jf)\n",
    "\n",
    "# get all QIDs in the subset\n",
    "subset_quids = []\n",
    "for hits in wd_subset.values():\n",
    "    for hit in hits:\n",
    "        subset_quids.append(hit[\"QID\"])\n",
    "\n",
    "# import rm\n",
    "wd_rm = pd.read_csv(\"/Users/anesterov/reps/LODlit/rm/rm_entities_unique.csv\")\n",
    "rm_quids = list(wd_rm[wd_rm[\"resource\"] == \"wikidata\"][wd_rm[\"lang\"] == lang][\"entity_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import P31 markers\n",
    "p31 = pd.read_csv(\"/Users/anesterov/reps/LODlit/sensitivity_markers/explicit/wikidata_P31_all.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a dict of entity and value\n",
    "p31_pairs = {}\n",
    "for row in p31.iterrows():\n",
    "    p31_pairs[row[1][\"item\"].lstrip(\"http://www.wikidata.org/entity/\")] = row[1][\"instance_of\"].lstrip(\"http://www.wikidata.org/entity/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import P2559 markers\n",
    "p2559 = pd.read_csv(\"/Users/anesterov/reps/LODlit/sensitivity_markers/explicit/wikidata_P2559_all.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p2559_pairs = {}\n",
    "for row in p2559.iterrows():\n",
    "    p2559_pairs[row[1][\"item\"].lstrip(\"http://www.wikidata.org/entity/\")] = row[1][\"usage_instructions\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for term, hits in wd_all.items():\n",
    "    lemma = get_lemma_by_term(term, lang)\n",
    "    \n",
    "    for hit in hits:\n",
    "        # if p31 markers \n",
    "        if hit[\"QID\"] in p31_pairs.keys():\n",
    "            \n",
    "            # check level\n",
    "            level = \"1\"\n",
    "            if hit[\"QID\"] in set(subset_quids):\n",
    "                level = \"2\"\n",
    "            if hit[\"QID\"] in set(rm_quids):\n",
    "                level = \"3\"\n",
    "                \n",
    "            row = [\"wikidata\",lang,lemma,hit[\"QID\"],\"P31\",p31_pairs[hit[\"QID\"]],level]\n",
    "            wd_explicit.loc[len(wd_explicit)] = row\n",
    "            \n",
    "        # if p2559 markers \n",
    "        if hit[\"QID\"] in p2559_pairs.keys():\n",
    "            \n",
    "            # check level\n",
    "            level = \"1\"\n",
    "            if hit[\"QID\"] in set(subset_quids):\n",
    "                level = \"2\"\n",
    "            if hit[\"QID\"] in set(rm_quids):\n",
    "                level = \"3\"\n",
    "                \n",
    "            row = [\"wikidata\",lang,lemma,hit[\"QID\"],\"P2559\",p2559_pairs[hit[\"QID\"]],level]\n",
    "            wd_explicit.loc[len(wd_explicit)] = row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wd_explicit.drop_duplicates([\"lang\",\"lemma\",\"entity_id\",\"explicit_marker\",\"value\"],ignore_index=True).to_csv(\"wd_explicit.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aat_explicit = pd.DataFrame(columns=[\"resource\",\"lang\",\"lemma\",\"entity_id\",\"explicit_marker\",\"value\",\"level\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang = \"en\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing all search results\n",
    "with open(f\"/Users/anesterov/reps/LODlit/AAT/aat_query_results_{lang}.json\",'r') as jf:\n",
    "    aat_all = json.load(jf)\n",
    "\n",
    "# import subset\n",
    "with open(f\"/Users/anesterov/reps/LODlit/AAT/aat_{lang}_subset.json\",'r') as jf:\n",
    "    aat_subset = json.load(jf)\n",
    "\n",
    "# get all QIDs in the subset\n",
    "subset = []\n",
    "for hits in aat_subset.values():\n",
    "    for hit in hits:\n",
    "        subset.append(hit[\"aat_uri\"])\n",
    "\n",
    "# import rm\n",
    "aat_rm = pd.read_csv(\"/Users/anesterov/reps/LODlit/rm/rm_entities_unique.csv\")\n",
    "rm = list(aat_rm[aat_rm[\"resource\"] == \"aat\"][aat_rm[\"lang\"] == lang][\"entity_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strip_part = \"http://vocab.getty.edu/aat/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import historicFlag markers\n",
    "historic_flag = pd.read_csv(\"/Users/anesterov/reps/LODlit/sensitivity_markers/explicit/aat_historicFlag_all.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "historic_flag_list = list(set([c.lstrip(strip_part) for c in historic_flag[\"concept\"]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import termKind markers\n",
    "termKind_flag = pd.read_csv(\"/Users/anesterov/reps/LODlit/sensitivity_markers/explicit/aat_termKind_all.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for group in wd_en_implicit.groupby([\"lemma\",\"entity_id\",\"value\"]):\n",
    "    if (len(group[1])) > 1:\n",
    "        grouped_markers = (list(group[1][\"implicit_marker\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a dict of concepts with flags\n",
    "# !NB several labels can be marked, make a list\n",
    "termKind_flag_pairs = {}\n",
    "for group in termKind_flag.groupby(\"concept\"):\n",
    "    grouped_flags = list(group[1][\"flag\"])\n",
    "    termKind_flag_pairs[str(group[0]).lstrip(strip_part)] = [f.lstrip(\"http://vocab.getty.edu/term/kind/\") for f in grouped_flags]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "termKind_flag_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for term, hits in aat_all.items():\n",
    "    lemma = get_lemma_by_term(term, lang)\n",
    "    \n",
    "    for hit in hits:\n",
    "        # if historic_flag markers\n",
    "        if hit[\"aat_uri\"] in historic_flag_list:\n",
    "            \n",
    "            # check level\n",
    "            level = \"1\"\n",
    "            if hit[\"aat_uri\"] in set(subset):\n",
    "                level = \"2\"\n",
    "            if hit[\"aat_uri\"] in set(rm):\n",
    "                level = \"3\"\n",
    "                \n",
    "            row = [\"aat\",lang,lemma,hit[\"aat_uri\"],\"historicFlag\",\"\",level]\n",
    "            aat_explicit.loc[len(aat_explicit)] = row\n",
    "            \n",
    "        # if termKind flag\n",
    "        if hit[\"aat_uri\"] in termKind_flag_pairs.keys():\n",
    "            \n",
    "            # check level\n",
    "            level = \"1\"\n",
    "            if hit[\"aat_uri\"] in set(subset):\n",
    "                level = \"2\"\n",
    "            if hit[\"aat_uri\"] in set(rm):\n",
    "                level = \"3\"\n",
    "                \n",
    "            row = [\"aat\",lang,lemma,hit[\"aat_uri\"],\"termKind\",termKind_flag_pairs[hit[\"aat_uri\"]],level]\n",
    "            aat_explicit.loc[len(aat_explicit)] = row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aat_explicit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aat_explicit.drop_duplicates([\"lang\",\"lemma\",\"entity_id\",\"explicit_marker\"],ignore_index=True).to_csv(\"aat_explicit.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PWN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwn_explicit = pd.DataFrame(columns=[\"resource\",\"lang\",\"lemma\",\"entity_id\",\"explicit_marker\",\"value\",\"level\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing all search results\n",
    "with open(\"/Users/anesterov/reps/LODlit/PWN/pwn31_query_results.json\",'r') as jf:\n",
    "    pwn_all = json.load(jf)\n",
    "\n",
    "# import subset\n",
    "with open(\"/Users/anesterov/reps/LODlit/PWN/pwn_subset.json\",'r') as jf:\n",
    "    pwn_subset = json.load(jf)\n",
    "\n",
    "# get all QIDs in the subset\n",
    "subset = []\n",
    "for hits in pwn_subset.values():\n",
    "    for hit in hits:\n",
    "        subset.append(hit[\"synset_id\"])\n",
    "\n",
    "# import rm\n",
    "pwn_rm = pd.read_csv(\"/Users/anesterov/reps/LODlit/rm/rm_entities_unique.csv\")\n",
    "rm = list(pwn_rm[pwn_rm[\"resource\"] == \"pwn\"][\"entity_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pwn markers\n",
    "usage_domains = pd.read_csv(\"/Users/anesterov/reps/LODlit/sensitivity_markers/explicit/pwn_usage_domain_synsets_all.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usage_domains_pairs = {}\n",
    "for row in usage_domains.iterrows():\n",
    "    usage_domains_pairs[row[1][\"synset_id\"]] = \\\n",
    "    row[1][\"usage_domains\"].replace(\"[\",\"\").replace(\"]\",\"\").replace(\"'\",\"\").replace(\" \",\"\").split(\",\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for term, hits in pwn_all.items():\n",
    "    lemma = get_lemma_by_term(term, \"en\")\n",
    "    \n",
    "    for hit in hits:\n",
    "        # if historic_flag markers\n",
    "        if hit[\"synset_id\"] in usage_domains_pairs.keys():\n",
    "            \n",
    "            # check level\n",
    "            level = \"1\"\n",
    "            if hit[\"synset_id\"] in set(subset):\n",
    "                level = \"2\"\n",
    "            if hit[\"synset_id\"] in set(rm):\n",
    "                level = \"3\"\n",
    "                \n",
    "            row = [\"pwn\",\"en\",lemma,hit[\"synset_id\"],\"usage_domain\",usage_domains_pairs[hit[\"synset_id\"]],level]\n",
    "            pwn_explicit.loc[len(pwn_explicit)] = row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwn_explicit.drop_duplicates([\"lemma\",\"entity_id\"],ignore_index=True).to_csv(\"pwn_explicit.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ODWN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "odwn_explicit = pd.DataFrame(columns=[\"resource\",\"lang\",\"lemma\",\"entity_id\",\"explicit_marker\",\"value\",\"level\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing all search results\n",
    "with open(\"/Users/anesterov/reps/LODlit/ODWN/odwn_query_results.json\",'r') as jf:\n",
    "    odwn_all = json.load(jf)\n",
    "\n",
    "# import subset\n",
    "with open(\"/Users/anesterov/reps/LODlit/ODWN/odwn_subset.json\",'r') as jf:\n",
    "    odwn_subset = json.load(jf)\n",
    "\n",
    "# get all ids in the subset\n",
    "subset = []\n",
    "for hits in odwn_subset.values():\n",
    "    for hit in hits:\n",
    "        if hit[\"synset_id\"] != \"\":\n",
    "            subset.append(hit[\"synset_id\"])\n",
    "        else:\n",
    "            subset.append(hit[\"sense_id\"])\n",
    "\n",
    "# import rm\n",
    "odwn_rm = pd.read_csv(\"/Users/anesterov/reps/LODlit/rm/rm_entities_unique.csv\")\n",
    "rm = list(odwn_rm[odwn_rm[\"resource\"] == \"odwn\"][\"entity_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import odwn markers\n",
    "with open(\"/Users/anesterov/reps/LODlit/sensitivity_markers/explicit/odwn_all_pragmatics.json\",'r') as jf:\n",
    "    odwn_markers = json.load(jf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for term, hits in odwn_all.items():\n",
    "    lemma = get_lemma_by_term(term, \"nl\")\n",
    "    \n",
    "    for hit in hits:\n",
    "        \n",
    "        if hit[\"synset_id\"] != \"\":\n",
    "            odwn_id = hit[\"synset_id\"]\n",
    "        else:\n",
    "            odwn_id = hit[\"sense_id\"]\n",
    "        \n",
    "        # check level\n",
    "        level = \"1\"\n",
    "        if odwn_id in set(subset):\n",
    "            level = \"2\"\n",
    "        if odwn_id in set(rm):\n",
    "            level = \"3\"\n",
    "        \n",
    "        # get markers values for each synset/sense\n",
    "        # NB! collects pragmatics values of all lemmas in a synset\n",
    "        chron = list(set([le[\"pragmatics\"][\"chronology\"] for le in odwn_markers[odwn_id] if le[\"pragmatics\"][\"chronology\"]]))\n",
    "        conn = list(set([le[\"pragmatics\"][\"connotation\"] for le in odwn_markers[odwn_id] if le[\"pragmatics\"][\"connotation\"]]))\n",
    "        \n",
    "        if len(chron) > 0:\n",
    "            row = [\"odwn\",\"nl\",lemma,odwn_id,\"pragmatics_chronology\",chron,level]\n",
    "            odwn_explicit.loc[len(odwn_explicit)] = row\n",
    "\n",
    "        if len(conn) > 0:\n",
    "            row = [\"odwn\",\"nl\",lemma,odwn_id,\"pragmatics_connotation\",conn,level]\n",
    "            odwn_explicit.loc[len(odwn_explicit)] = row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "odwn_explicit.drop_duplicates([\"lemma\",\"entity_id\",\"explicit_marker\"],ignore_index=True).to_csv(\"odwn_explicit.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

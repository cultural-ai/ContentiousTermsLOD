{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from thefuzz import process\n",
    "import itertools\n",
    "from itertools import groupby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# suggestion can be found in the same properties as query terms\n",
    "# {\"suggestions\":{\"pref\": [('intersex', 90)] }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_suggestions(resource:str, lang:str, resource_file_path:str) -> dict:\n",
    "    '''\n",
    "    Finding suggestions from WM in the literal values of entities based on fuzzy string matching (thefuzz library);\n",
    "    there can be an exact match (score = 100) or near match (score > 90), matches with lower scores are not included;\n",
    "    \n",
    "    resource: str, 'wikidata', 'aat', 'pwn', or 'odwn'\n",
    "    lang: str, 'en' or 'nl'\n",
    "    resource_file_path: str, path to the file with query results of a resoure (related matches, subset, all results)\n",
    "    Returns dict per resource per lang adding the found suggestions to every hit:\n",
    "        for example, {'suggestions':{'prefLabel':[(\"match\",100)]}};\n",
    "    '''\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # load suggestions per lang\n",
    "    if lang == \"en\":\n",
    "        # change path\n",
    "        with open('/Users/anesterov/reps/LODlit/RQ2/en_suggestions.json','r') as jf:\n",
    "            suggestions_dict = json.load(jf)\n",
    "    if lang == \"nl\":\n",
    "        with open('/Users/anesterov/reps/LODlit/RQ2/nl_suggestions.json','r') as jf:\n",
    "            suggestions_dict = json.load(jf)\n",
    "    \n",
    "    # load the resource data\n",
    "    with open(resource_file_path,'r') as jf:\n",
    "        resource_data = json.load(jf)\n",
    "\n",
    "    for term, hits in resource_data.items():\n",
    "        hits_with_suggestions = []\n",
    "        \n",
    "        for hit in hits:\n",
    "            # searhing suggestions in literals of every hit\n",
    "            # this will allow to compare in which properties the query terms and suggestions were found\n",
    "            suggestions = {}\n",
    "\n",
    "            # check resource\n",
    "            # AAT\n",
    "            if resource == \"aat\":\n",
    "\n",
    "                # prefLabel\n",
    "                # filtering matches by scores\n",
    "                # excluding suggestions that are equal to query terms\n",
    "                matches_pref = [tup for tup in process.extract(hit['prefLabel'], suggestions_dict[term]) if tup[1] > 90\\\n",
    "                                and tup[0].lower() != hit[\"query_term\"]]\n",
    "\n",
    "                # altLabels\n",
    "                matches_alt = []\n",
    "                for alt in hit[\"altLabel\"]:\n",
    "                    matches_alt.extend([tup for tup in process.extract(alt, suggestions_dict[term]) if tup[1] > 90 \\\n",
    "                                       and tup[0].lower() != hit[\"query_term\"]])\n",
    "\n",
    "                # scopeNotes\n",
    "                matches_scopeNote = [tup for tup in process.extract(hit[\"scopeNote\"], suggestions_dict[term]) if tup[1] > 90 \\\n",
    "                                    and tup[0].lower() != hit[\"query_term\"]]\n",
    "\n",
    "                # prefLabel comment\n",
    "                matches_pref_comment = [tup for tup in process.extract(hit['prefLabel_comment'], suggestions_dict[term]) if tup[1] > 90 \\\n",
    "                                       and tup[0].lower() != hit[\"query_term\"]]\n",
    "                \n",
    "                # altLabels comments\n",
    "                matches_alt_comment = []\n",
    "                for alt_comment in hit[\"altLabel_comment\"]:\n",
    "                    matches_alt_comment.extend([tup for tup in process.extract(alt_comment, suggestions_dict[term]) if tup[1] > 90 \\\n",
    "                                               and tup[0].lower() != hit[\"query_term\"]])\n",
    "                    \n",
    "                # group by suggestions, take unique sugestions with max score\n",
    "                suggestions['prefLabel'] = [max(score) for l,score in itertools.groupby(sorted(matches_pref), key=lambda t: t[0])]\n",
    "                suggestions['altLabel'] = [max(score) for l,score in itertools.groupby(sorted(matches_alt), key=lambda t: t[0])]\n",
    "                suggestions['scopeNote'] = [max(score) for l,score in itertools.groupby(sorted(matches_scopeNote), key=lambda t: t[0])]\n",
    "                suggestions['prefLabel_comment'] = [max(score) for l,score in itertools.groupby(sorted(matches_pref_comment), key=lambda t: t[0])]\n",
    "                suggestions['altLabel_comment'] = [max(score) for l,score in itertools.groupby(sorted(matches_alt_comment), key=lambda t: t[0])]\n",
    "\n",
    "                hit[\"suggestions\"] = suggestions\n",
    "\n",
    "                # if there are suggestions found, append it\n",
    "                if len(matches_pref) > 0 or len(matches_alt) > 0 or len(matches_scopeNote) > 0 \\\n",
    "                or len(matches_pref_comment) > 0 or len(matches_alt_comment) > 0:\n",
    "                    hits_with_suggestions.append(hit)\n",
    "            \n",
    "            # Wikidata        \n",
    "            if resource == \"wd\":\n",
    "                # prefLabel\n",
    "                if type(hit['prefLabel']) == str:\n",
    "                    matches_pref = [tup for tup in process.extract(hit['prefLabel'], suggestions_dict[term]) if tup[1] > 90 \\\n",
    "                                   and tup[0].lower() != hit[\"query_term\"]]\n",
    "\n",
    "                # aliases\n",
    "                matches_alt = []\n",
    "                if hit[\"aliases\"]:\n",
    "                    for alt in hit[\"aliases\"]:\n",
    "                        matches_alt.extend([tup for tup in process.extract(alt, suggestions_dict[term]) if tup[1] > 90 \\\n",
    "                                           and tup[0].lower() != hit[\"query_term\"]])\n",
    "\n",
    "                # description\n",
    "                matches_desc = []\n",
    "                # check desc type\n",
    "                if type(hit[\"description\"]) == list:\n",
    "                    for d in hit[\"description\"]:\n",
    "                        matches_desc.extend([tup for tup in process.extract(d, suggestions_dict[term]) if tup[1] > 90 \\\n",
    "                                            and tup[0].lower() != hit[\"query_term\"]])\n",
    "                if type(hit[\"description\"]) == str:\n",
    "                    matches_desc = [tup for tup in process.extract(hit[\"description\"], suggestions_dict[term]) if tup[1] > 90 \\\n",
    "                                   and tup[0].lower() != hit[\"query_term\"]]\n",
    "                    \n",
    "                # group by suggestions, take unique sugestions with max score\n",
    "                suggestions['prefLabel'] = [max(score) for l,score in itertools.groupby(sorted(matches_pref), key=lambda t: t[0])]\n",
    "                suggestions['aliases'] = [max(score) for l,score in itertools.groupby(sorted(matches_alt), key=lambda t: t[0])]\n",
    "                suggestions['description'] = [max(score) for l,score in itertools.groupby(sorted(matches_desc), key=lambda t: t[0])]\n",
    "                \n",
    "                hit[\"suggestions\"] = suggestions\n",
    "                \n",
    "                # if there are suggestions found, append it\n",
    "                if len(matches_pref) > 0 or len(matches_alt) > 0 or len(matches_desc) > 0:\n",
    "                    hits_with_suggestions.append(hit)\n",
    "            \n",
    "            # PWN\n",
    "            if resource == \"pwn\":\n",
    "                # lemmata\n",
    "                matches_lemma = []\n",
    "                for l in hit[\"lemmata\"]:\n",
    "                    matches_lemma.extend([tup for tup in process.extract(l, suggestions_dict[term]) if tup[1] > 90 \\\n",
    "                                         and tup[0].lower() != hit[\"query_term\"]])\n",
    "                    \n",
    "                # definitions\n",
    "                matches_def = [tup for tup in process.extract(hit['definition'], suggestions_dict[term]) if tup[1] > 90 \\\n",
    "                              and tup[0].lower() != hit[\"query_term\"]]\n",
    "                \n",
    "                # examples\n",
    "                matches_ex = []\n",
    "                for e in hit[\"examples\"]:\n",
    "                    matches_ex.extend([tup for tup in process.extract(e, suggestions_dict[term]) if tup[1] > 90 \\\n",
    "                                      and tup[0].lower() != hit[\"query_term\"]])\n",
    "                    \n",
    "                # group by suggestions, take unique sugestions with max score\n",
    "                suggestions['lemmata'] = [max(score) for l,score in itertools.groupby(sorted(matches_lemma), key=lambda t: t[0])]\n",
    "                suggestions['definition'] = [max(score) for l,score in itertools.groupby(sorted(matches_def), key=lambda t: t[0])]\n",
    "                suggestions['examples'] = [max(score) for l,score in itertools.groupby(sorted(matches_ex), key=lambda t: t[0])]\n",
    "                \n",
    "                hit[\"suggestions\"] = suggestions\n",
    "                \n",
    "                # if there are suggestions found, append it\n",
    "                if len(matches_lemma) > 0 or len(matches_def) > 0 or len(matches_ex) > 0:\n",
    "                    hits_with_suggestions.append(hit)\n",
    "                    \n",
    "            # ODWN\n",
    "            if resource == \"odwn\":\n",
    "                # le_written_form\n",
    "                # optional\n",
    "                matches_le = []\n",
    "                if hit.get('le_written_form'):\n",
    "                    matches_le = [tup for tup in process.extract(hit['le_written_form'], suggestions_dict[term]) if tup[1] > 90 \\\n",
    "                                 and tup[0].lower() != hit[\"query_term\"]]\n",
    "                \n",
    "                # sense_definition\n",
    "                # optional\n",
    "                matches_sense_def = []\n",
    "                if hit.get('sense_definition'):\n",
    "                    matches_sense_def = [tup for tup in process.extract(hit['sense_definition'], suggestions_dict[term]) if tup[1] > 90 \\\n",
    "                                        and tup[0].lower() != hit[\"query_term\"]]\n",
    "                \n",
    "                # synset def\n",
    "                matches_synset_def = []\n",
    "                for synset_def in hit['synset_definitions']:\n",
    "                    matches_synset_def.extend([tup for tup in process.extract(synset_def, suggestions_dict[term]) if tup[1] > 90 \\\n",
    "                                              and tup[0].lower() != hit[\"query_term\"]])\n",
    "            \n",
    "                # sense ex\n",
    "                # optional\n",
    "                matches_sense_ex = []\n",
    "                if hit.get(\"sense_examples\"):\n",
    "                    for sense_ex in hit[\"sense_examples\"]:\n",
    "                        matches_sense_ex.extend([tup for tup in process.extract(sense_ex, suggestions_dict[term]) if tup[1] > 90 \\\n",
    "                                                and tup[0].lower() != hit[\"query_term\"]])\n",
    "                \n",
    "                # group by suggestions, take unique sugestions with max score\n",
    "                suggestions['le_written_form'] = [max(score) for l,score in itertools.groupby(sorted(matches_le), key=lambda t: t[0])]\n",
    "                suggestions['sense_definition'] = [max(score) for l,score in itertools.groupby(sorted(matches_sense_def), key=lambda t: t[0])]\n",
    "                suggestions['synset_definitions'] = [max(score) for l,score in itertools.groupby(sorted(matches_synset_def), key=lambda t: t[0])]\n",
    "                suggestions['sense_examples'] = [max(score) for l,score in itertools.groupby(sorted(matches_sense_ex), key=lambda t: t[0])]\n",
    "\n",
    "                \n",
    "                hit[\"suggestions\"] = suggestions\n",
    "                \n",
    "                # if there are suggestions found, append it\n",
    "                if len(matches_le) > 0 or len(matches_sense_def) > 0 or len(matches_synset_def) > 0 \\\n",
    "                or len(matches_sense_ex) > 0:\n",
    "                    hits_with_suggestions.append(hit)\n",
    "                \n",
    "        results[term] = hits_with_suggestions\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sug_all_wd_nl = find_suggestions('wd','nl','/Users/anesterov/LODlit_local/wd/jan31/results_clean_nl.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exporting the json file\n",
    "with open('sug_all_wd_nl.json', 'w') as jf:\n",
    "    json.dump(sug_all_wd_nl, jf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Suggestions stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N suggestions per lang\n",
    "with open('/Users/anesterov/reps/LODlit/RQ2/en_suggestions.json','r') as jf:\n",
    "    en_suggestions = json.load(jf)\n",
    "\n",
    "with open('/Users/anesterov/reps/LODlit/RQ2/nl_suggestions.json','r') as jf:\n",
    "    nl_suggestions = json.load(jf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sug_unique = []\n",
    "for term, sug in nl_suggestions.items():\n",
    "    sug_unique.extend(sug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(sug_unique))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# related matches\n",
    "with open('/Users/anesterov/reps/LODlit/RQ2/sug_rm_wd_en.json','r') as jf:\n",
    "    sug_rm_wd_en = json.load(jf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_suggestions_count(resource:str, path_to_dataset:str) -> dict:\n",
    "    '''\n",
    "    Counting N of entities and hits with exact and near match suggestions\n",
    "    resource: str, \"wd\", \"aat\", \"pwn\", or \"odwn\"\n",
    "    path_to_dataset: str, path to a file with hits\n",
    "    Returns dict\n",
    "    '''\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    with open(path_to_dataset,'r') as jf:\n",
    "        data = json.load(jf)\n",
    "        \n",
    "    # check resource\n",
    "    if resource == \"wd\":\n",
    "        entity_id_key = \"QID\"\n",
    "    if resource == \"aat\":\n",
    "        entity_id_key = \"aat_uri\"\n",
    "    if resource == \"pwn\":\n",
    "        entity_id_key = \"synset_id\"\n",
    "        \n",
    "    # how many exact match suggestions?\n",
    "    entities_exact = []\n",
    "    entities_near = []\n",
    "    hits_exact = 0\n",
    "    hits_near = 0\n",
    "\n",
    "    for term, sug_hits in data.items():\n",
    "        for hit in sug_hits:\n",
    "            for prop, matches in hit[\"suggestions\"].items():\n",
    "                for m in matches:\n",
    "                    if m[1] == 100:\n",
    "                        hits_exact += 1\n",
    "                        entities_exact.append(hit)\n",
    "                    else:\n",
    "                        hits_near += 1\n",
    "                        entities_near.append(hit)\n",
    "    # ODWN\n",
    "    if resource == \"odwn\":\n",
    "        n_entities_exact_d = []\n",
    "        for hit in entities_exact:\n",
    "            if hit.get(\"synset_id\"):\n",
    "                n_entities_exact_d.append(hit[\"synset_id\"])\n",
    "            else:\n",
    "                n_entities_exact_d.append(hit[\"le_id\"])\n",
    "        n_entities_exact = len(set(n_entities_exact_d))\n",
    "\n",
    "        n_entities_near_d = []\n",
    "        for hit in entities_near:\n",
    "            if hit.get(\"synset_id\"):\n",
    "                n_entities_near_d.append(hit[\"synset_id\"])\n",
    "            else:\n",
    "                n_entities_near_d.append(hit[\"le_id\"])\n",
    "        n_entities_near = len(set(n_entities_near_d))\n",
    "        \n",
    "    else:\n",
    "        n_entities_exact = len(set([hit[entity_id_key] for hit in entities_exact]))\n",
    "        n_entities_near = len(set([hit[entity_id_key] for hit in entities_near]))\n",
    "\n",
    "    results[\"n_entities_exact\"] = n_entities_exact\n",
    "    results[\"n_entities_near\"] = n_entities_near\n",
    "    results[\"hits_exact\"] = hits_exact\n",
    "    results[\"hits_near\"] = hits_near\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_entities_exact': 7, 'n_entities_near': 2, 'hits_exact': 17, 'hits_near': 6}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_suggestions_count(\"wd\",\"sug_rm_wd_en.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check unique entities with suggestions in the subset and all search results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_entities_with_suggestions(resource:str,lang:str,to_compare:tuple) -> dict:\n",
    "    '''\n",
    "    to_compare: tuple; (\"rm\",\"subset\") or (\"subset\",\"all\")\n",
    "    '''\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    lang = \"_\" + lang\n",
    "    if resource == \"odwn\" or resource == \"pwn\":\n",
    "        lang = ''\n",
    "    \n",
    "    with open(f\"sug_{to_compare[0]}_{resource}{lang}.json\",'r') as jf:\n",
    "        resource_1 = json.load(jf)\n",
    "        \n",
    "    with open(f\"sug_{to_compare[1]}_{resource}{lang}.json\",'r') as jf:\n",
    "        resource_2 = json.load(jf)\n",
    "        \n",
    "    # check resource\n",
    "    # special conditions for ODWN\n",
    "    if resource == \"odwn\":\n",
    "        e_with_sug_1 = []\n",
    "        e_with_sug_2 = []\n",
    "        \n",
    "        for term, sug_hits in resource_1.items():\n",
    "            for hit in sug_hits:\n",
    "                if hit.get(\"synset_id\"):\n",
    "                    e_with_sug_1.append(hit[\"synset_id\"])\n",
    "                else:\n",
    "                    e_with_sug_1.append(hit[\"le_id\"])\n",
    "\n",
    "        for term, sug_hits in resource_2.items():\n",
    "            for hit in sug_hits:\n",
    "                if hit.get(\"synset_id\"):\n",
    "                    e_with_sug_2.append(hit[\"synset_id\"])\n",
    "                else:\n",
    "                    e_with_sug_2.append(hit[\"le_id\"])\n",
    "    else:\n",
    "    \n",
    "        if resource == \"wd\":\n",
    "            entity_id_key = \"QID\"\n",
    "        if resource == \"aat\":\n",
    "            entity_id_key = \"aat_uri\"\n",
    "        if resource == \"pwn\":\n",
    "            entity_id_key = \"synset_id\"\n",
    "\n",
    "        e_with_sug_1 = []\n",
    "        for term, sug_hits in resource_1.items():\n",
    "            for hit in sug_hits:\n",
    "                e_with_sug_1.append(hit[entity_id_key])\n",
    "\n",
    "        e_with_sug_2 = []\n",
    "        for term, sug_hits in resource_2.items():\n",
    "            for hit in sug_hits:\n",
    "                e_with_sug_2.append(hit[entity_id_key])\n",
    "            \n",
    "    unique_e = [e for e in list(set(e_with_sug_2)) if e not in list(set(e_with_sug_1))]\n",
    "    \n",
    "    results[f\"{resource}{lang}\"] = unique_e\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "ent = get_unique_entities_with_suggestions(\"wd\",\"en\",(\"subset\",\"all\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'wd_en': ['Q4117169',\n",
       "  'Q48801961',\n",
       "  'Q459387',\n",
       "  'Q12060728',\n",
       "  'Q475027',\n",
       "  'Q8060',\n",
       "  'Q11387167',\n",
       "  'Q112918934',\n",
       "  'Q97690709',\n",
       "  'Q3200179',\n",
       "  'Q1145774',\n",
       "  'Q6025468',\n",
       "  'Q4668353',\n",
       "  'Q9064330',\n",
       "  'Q37178',\n",
       "  'Q97703712',\n",
       "  'Q19860',\n",
       "  'Q100379957']}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ent[\"wd_en\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Are sugestions used when contentious terms are found in prefLabel?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# related matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/anesterov/reps/LODlit/RQ2/sug_rm_wd_en.json','r') as jf:\n",
    "    sug_rm_wd_en = json.load(jf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_suggestions(resource:str,lang:str,level:str,search_suggestions_in:str) -> dict:\n",
    "    '''\n",
    "    resource: 'wd', 'aat'\n",
    "    level: 'rm', 'subset', 'all'\n",
    "    search_suggestions_in: 'pref', 'alt'\n",
    "    '''\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # check resource\n",
    "    if resource == \"wd\":\n",
    "        pref = \"prefLabel\"\n",
    "        alt = \"aliases\"\n",
    "        entity_key = \"QID\"\n",
    "    if resource == \"aat\":\n",
    "        pref = \"prefLabel\"\n",
    "        alt = \"altLabel\"\n",
    "        entity_key = \"aat_uri\"\n",
    "    \n",
    "    with open(f\"sug_{level}_{resource}_{lang}.json\",'r') as jf:\n",
    "        hits_with_suggestions = json.load(jf)\n",
    "    \n",
    "    n_hits = 0\n",
    "    for term, sug_hits in hits_with_suggestions.items():\n",
    "        hits_per_term = []\n",
    "        for hit in sug_hits:\n",
    "            # query terms are found in aliases and suggestions are used as prefLabel\n",
    "            if search_suggestions_in == 'pref':\n",
    "                if hit[\"found_in\"] == alt and len(hit[\"suggestions\"][pref]) > 0:\n",
    "                    hits_per_term.append(hit)\n",
    "                    n_hits += 1\n",
    "                    \n",
    "            if search_suggestions_in == 'alt':\n",
    "                if hit[\"found_in\"] == pref and len(hit[\"suggestions\"][alt]) > 0:\n",
    "                    hits_per_term.append(hit)\n",
    "                    n_hits += 1\n",
    "                \n",
    "        if len(hits_per_term) > 0:\n",
    "            results[term] = hits_per_term\n",
    "    \n",
    "    # get N of unique entities\n",
    "    n_e_unique = []\n",
    "    for term, hits in results.items():\n",
    "        for hit in hits:\n",
    "            n_e_unique.append(hit[entity_key])\n",
    "    \n",
    "    string_to_return = f\"Suggestions in {search_suggestions_in}. N unique e: {len(set(n_e_unique))} {(set(n_e_unique))}, N hits all: {n_hits}\"\n",
    "        \n",
    "    return string_to_return,results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"Suggestions in pref. N unique e: 2 {'300018279', '300404576'}, N hits all: 2\",\n",
       " {'aboriginal': [{'query_term': 'aboriginal',\n",
       "    'aat_uri': '300404576',\n",
       "    'found_in': 'altLabel',\n",
       "    'prefLabel': 'indigenous art',\n",
       "    'prefLabel_comment': '',\n",
       "    'altLabel': ['art, indigenous', 'aboriginal art'],\n",
       "    'altLabel_comment': [],\n",
       "    'scopeNote': 'Art and cultural works produced by the original inhabitants of an area, as contrasted to works produced by descendants of colonists to the area. A primary usage of the term refers to cultures who pre-existed European colonialism in the Americas, Africa, and Oceania.',\n",
       "    'suggestions': {'prefLabel': [['Indigenous', 95]],\n",
       "     'altLabel': [],\n",
       "     'scopeNote': [],\n",
       "     'prefLabel_comment': [],\n",
       "     'altLabel_comment': []}}],\n",
       "  'oriental': [{'query_term': 'oriental',\n",
       "    'aat_uri': '300018279',\n",
       "    'found_in': 'altLabel',\n",
       "    'prefLabel': 'Asian',\n",
       "    'prefLabel_comment': '',\n",
       "    'altLabel': ['Oriental (Asian)'],\n",
       "    'altLabel_comment': [],\n",
       "    'scopeNote': 'Refers to the cultures of the continent of Asia, which is in the eastern hemisphere, and is bounded by the Pacific Ocean, the Indian Ocean, the Arctic Ocean, and is generally considered to be delimited on the west by the Ural Mountains. It also refers to the numerous islands off the coast of Asia.',\n",
       "    'suggestions': {'prefLabel': [['Asian', 100]],\n",
       "     'altLabel': [],\n",
       "     'scopeNote': [],\n",
       "     'prefLabel_comment': [],\n",
       "     'altLabel_comment': []}}]})"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_suggestions(\"aat\",\"en\",\"subset\",\"pref\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

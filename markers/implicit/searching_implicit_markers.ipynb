{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import csv\n",
    "import pandas as pd\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lemma_by_term(query_term:str, lang:str) -> str:\n",
    "    '''\n",
    "    Getting a lemma of a query term\n",
    "    lang: str, 'en' or 'nl'\n",
    "    Returns str, 'not found' if lemma was not found\n",
    "    '''\n",
    "    \n",
    "    return_lemma = 'not found'\n",
    "    \n",
    "    # importing query terms with lemmas\n",
    "    \n",
    "    with open('/LODlit/query_terms.json','r') as jf:\n",
    "        query_terms = json.load(jf)\n",
    "        \n",
    "    for lemma, qt in query_terms[lang].items():\n",
    "        if query_term in qt:\n",
    "            return_lemma = lemma\n",
    "            \n",
    "    return return_lemma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wikidata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_implicit_markers_wd(lang:str):\n",
    "    '''\n",
    "    lang: str, \"en\" or \"nl\"\n",
    "    Returns pandas DataFrame\n",
    "    '''\n",
    "    \n",
    "    wd_implicit = pd.DataFrame(columns=[\"resource\",\"lang\",\"lemma\",\"entity_id\",\"property\",\"value\",\"implicit_marker\",\"level\"])\n",
    "    \n",
    "    # importing implicit markers dict\n",
    "    with open('implicit_markers.json','r') as jf:\n",
    "        implicit_markers = json.load(jf)\n",
    "    \n",
    "    # importing all search results\n",
    "    # this file is gzipped on Github with the prefix \"_gzip\"\n",
    "    with open(f\"/LODlit/Wikidata/gzip_results_clean_{lang}.json\",'r') as jf:\n",
    "        wd_all = json.load(jf)\n",
    "        \n",
    "    # import subset\n",
    "    with open(f\"/LODlit/Wikidata/wd_{lang}_subset.json\",'r') as jf:\n",
    "        wd_subset = json.load(jf)\n",
    "        \n",
    "    # get all QIDs in the subset\n",
    "    subset_quids = []\n",
    "    for hits in wd_subset.values():\n",
    "        for hit in hits:\n",
    "            subset_quids.append(hit[\"QID\"])\n",
    "            \n",
    "    # import rm\n",
    "    wd_rm = pd.read_csv(\"/LODlit/rm/rm_entities_unique.csv\")\n",
    "    rm_quids = list(wd_rm[wd_rm[\"resource\"] == \"wikidata\"][wd_rm[\"lang\"] == lang][\"entity_id\"])\n",
    "    \n",
    "    # searching in descriptions\n",
    "    for term, hits in wd_all.items():\n",
    "        lemma = get_lemma_by_term(term, lang)\n",
    "        \n",
    "        for hit in hits:\n",
    "\n",
    "            level = \"1\"\n",
    "            # check entity level\n",
    "            if hit[\"QID\"] in set(subset_quids):\n",
    "                level = \"2\"\n",
    "            if hit[\"QID\"] in set(rm_quids):\n",
    "                level = \"3\"\n",
    "                \n",
    "            for marker in implicit_markers[\"wikidata\"][lang]:\n",
    "\n",
    "                # check descriptions type\n",
    "                if type(hit[\"description\"]) == list:\n",
    "                    for d in hit[\"description\"]:\n",
    "                        match = re.search(f\"\\\\b{marker}\\\\b\",d,flags=re.IGNORECASE)\n",
    "                        if match:\n",
    "                            row = [\"wikidata\",lang,lemma,hit[\"QID\"],\"description\",d,match[0],level]\n",
    "                            wd_implicit.loc[len(wd_implicit)] = row\n",
    "\n",
    "                if type(hit[\"description\"]) == str:\n",
    "                    match = re.search(f\"\\\\b{marker}\\\\b\",hit[\"description\"],flags=re.IGNORECASE)\n",
    "                    if match:\n",
    "                        row = [\"wikidata\",lang,lemma,hit[\"QID\"],\"description\",hit[\"description\"],match[0],level]\n",
    "                        wd_implicit.loc[len(wd_implicit)] = row\n",
    "                        \n",
    "    return wd_implicit.drop_duplicates([\"lemma\",\"entity_id\",\"value\",\"implicit_marker\"],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wd_implicit = search_implicit_markers_wd(\"nl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wd_implicit.to_csv(\"wd_nl_implicit.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_implicit_markers_aat(lang:str):\n",
    "    '''\n",
    "    lang: str, \"en\" or \"nl\"\n",
    "    Returns pandas DataFrame\n",
    "    '''\n",
    "    \n",
    "    aat_implicit = pd.DataFrame(columns=[\"resource\",\"lang\",\"lemma\",\"entity_id\",\"property\",\"value\",\"implicit_marker\",\"level\"])\n",
    "    \n",
    "    # importing implicit markers dict\n",
    "    with open('implicit_markers.json','r') as jf:\n",
    "        implicit_markers = json.load(jf)\n",
    "    \n",
    "    # importing all search results\n",
    "    with open(f\"/LODlit/AAT/aat_query_results_{lang}.json\",'r') as jf:\n",
    "        aat_all = json.load(jf)\n",
    "        \n",
    "    # import subset\n",
    "    with open(f\"/LODlit/AAT/aat_{lang}_subset.json\",'r') as jf:\n",
    "        aat_subset = json.load(jf)\n",
    "        \n",
    "    # get all QIDs in the subset\n",
    "    subset_uris = []\n",
    "    for hits in aat_subset.values():\n",
    "        for hit in hits:\n",
    "            subset_uris.append(hit[\"aat_uri\"])\n",
    "            \n",
    "    # import rm\n",
    "    aat_rm = pd.read_csv(\"/LODlit/rm/rm_entities_unique.csv\")\n",
    "    rms = list(aat_rm[aat_rm[\"resource\"] == \"aat\"][aat_rm[\"lang\"] == lang][\"entity_id\"])\n",
    "    \n",
    "    for term, hits in aat_all.items():\n",
    "        lemma = get_lemma_by_term(term, lang)\n",
    "        \n",
    "        for hit in hits:\n",
    "\n",
    "            level = \"1\"\n",
    "            # check entity level\n",
    "            if hit[\"aat_uri\"] in set(subset_uris):\n",
    "                level = \"2\"\n",
    "            if hit[\"aat_uri\"] in set(rms):\n",
    "                level = \"3\"\n",
    "                \n",
    "            for marker in implicit_markers[\"aat\"][lang]:\n",
    "                # searching in scopeNotes\n",
    "                match = re.search(f\"\\\\b{marker}\\\\b\",hit[\"scopeNote\"],flags=re.IGNORECASE)\n",
    "                if match:\n",
    "                    row = [\"aat\",lang,lemma,hit[\"aat_uri\"],\"scopeNote\",hit[\"scopeNote\"],match[0],level]\n",
    "                    aat_implicit.loc[len(aat_implicit)] = row\n",
    "                    \n",
    "                # searching in prefLabel comments\n",
    "                match = re.search(f\"\\\\b{marker}\\\\b\",hit[\"prefLabel_comment\"],flags=re.IGNORECASE)\n",
    "                if match:\n",
    "                    row = [\"aat\",lang,lemma,hit[\"aat_uri\"],\"prefLabel_comment\",hit[\"prefLabel_comment\"],match[0],level]\n",
    "                    aat_implicit.loc[len(aat_implicit)] = row\n",
    "                    \n",
    "                # searching in altLabel comments    \n",
    "                for c in hit[\"altLabel_comment\"]:\n",
    "                    match = re.search(f\"\\\\b{marker}\\\\b\",c,flags=re.IGNORECASE)\n",
    "                    if match:\n",
    "                        row = [\"aat\",lang,lemma,hit[\"aat_uri\"],\"altLabel_comment\",c,match[0],level]\n",
    "                        aat_implicit.loc[len(aat_implicit)] = row\n",
    "                        \n",
    "    return aat_implicit.drop_duplicates([\"lemma\",\"entity_id\",\"property\",\"implicit_marker\"],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aat_implicit = search_implicit_markers_aat(\"nl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export csv\n",
    "aat_implicit.to_csv(\"aat_nl_implicit.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PWN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwn_implicit = pd.DataFrame(columns=[\"resource\",\"lang\",\"lemma\",\"entity_id\",\"property\",\"value\",\"implicit_marker\",\"level\"])\n",
    "    \n",
    "# importing implicit markers dict\n",
    "with open('implicit_markers.json','r') as jf:\n",
    "    implicit_markers = json.load(jf)\n",
    "\n",
    "# importing all search results\n",
    "with open(f\"/LODlit/PWN/pwn31_query_results.json\",'r') as jf:\n",
    "    pwn_all = json.load(jf)\n",
    "\n",
    "# import subset\n",
    "with open(f\"/LODlit/PWN/pwn_subset.json\",'r') as jf:\n",
    "    pwn_subset = json.load(jf)\n",
    "\n",
    "# get all QIDs in the subset\n",
    "subset = []\n",
    "for hits in pwn_subset.values():\n",
    "    for hit in hits:\n",
    "        subset.append(hit[\"synset_id\"])\n",
    "\n",
    "# import rm\n",
    "pwn_rm = pd.read_csv(\"/LODlit/rm/rm_entities_unique.csv\")\n",
    "rms = list(pwn_rm[pwn_rm[\"resource\"] == \"pwn\"][\"entity_id\"])\n",
    "\n",
    "for term, hits in pwn_all.items():\n",
    "    lemma = get_lemma_by_term(term, \"en\")\n",
    "        \n",
    "    for hit in hits:\n",
    "\n",
    "        level = \"1\"\n",
    "        # check entity level\n",
    "        if hit[\"synset_id\"] in set(subset):\n",
    "            level = \"2\"\n",
    "        if hit[\"synset_id\"] in set(rms):\n",
    "            level = \"3\"\n",
    "                \n",
    "        for marker in implicit_markers[\"pwn\"][\"en\"]:\n",
    "            # searching in definitions\n",
    "            match = re.search(f\"\\\\b{marker}\\\\b\",hit[\"definition\"],flags=re.IGNORECASE)\n",
    "            if match:\n",
    "                row = [\"pwn\",\"en\",lemma,hit[\"synset_id\"],\"definition\",hit[\"definition\"],match[0],level]\n",
    "                pwn_implicit.loc[len(pwn_implicit)] = row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export csv\n",
    "pwn_implicit.drop_duplicates([\"lemma\",\"entity_id\",\"value\",\"implicit_marker\"],ignore_index=True).to_csv(\"pwn_implicit.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are no implicit markers in ODWN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### merge wikidata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wd_en_impl = pd.read_csv(\"wd_en_implicit.csv\")\n",
    "wd_nl_impl = pd.read_csv(\"wd_nl_implicit.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [wd_en_impl,wd_nl_impl]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wd_impl = pd.concat(frames,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wd_impl.to_csv(\"wd_implicit.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### merge aat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aat_en_impl = pd.read_csv(\"aat_en_implicit.csv\")\n",
    "aat_nl_impl = pd.read_csv(\"aat_nl_implicit.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [aat_en_impl,aat_nl_impl]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aat_impl = pd.concat(frames,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aat_impl.to_csv(\"aat_implicit.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import json\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_lg\", disable=[\"tagger\", \"attribute_ruler\", \"lemmatizer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_1 = nlp(\"black people colour skin african american race \")\n",
    "bow_2 = nlp(\"black right people organisation human\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_1.similarity(bow_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert bows to str\n",
    "# remove target term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing search results\n",
    "with open('/Users/anesterov/wd/jan31/wd_bows_en.json','r') as jf:\n",
    "    wd_en = json.load(jf)\n",
    "# importing related matches\n",
    "with open('/Users/anesterov/reps/LODlit/bg/rm_bows_all.json','r') as jf:\n",
    "    rm = json.load(jf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang = \"en\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wd_df = pd.DataFrame(columns=['term', 'QID', 'wd_bow', 'spacy_cs'])\n",
    "\n",
    "for term, hits in wd_en.items():\n",
    "    \n",
    "    if term in rm[lang].keys():\n",
    "    \n",
    "        # EN collecting related matches per term\n",
    "        rm_per_term = []\n",
    "        if rm[lang][term].get('aat'):\n",
    "            rm_per_term.extend(rm[lang][term]['aat'])\n",
    "        if rm[lang][term].get('wikidata'):\n",
    "            rm_per_term.extend(rm[lang][term]['wikidata'])\n",
    "        if rm[lang][term].get('pwn'):\n",
    "            rm_per_term.extend(rm[lang][term]['pwn'])\n",
    "\n",
    "        # remove target term from bow\n",
    "        rm_per_term_no_target = list(set([t for t in rm_per_term if t != term]))\n",
    "            \n",
    "        # if there are search results\n",
    "        if len(hits) > 0:\n",
    "            for hit in hits:\n",
    "                for i, bow in hit.items():\n",
    "                    # remove target term from bow\n",
    "                    wd_bow = list(set([t for t in bow if t != term]))\n",
    "                    if len(wd_bow) > 0:\n",
    "                        # calculate cs\n",
    "                        cs = _get_sim(rm_per_term_no_target,wd_bow,nlp)\n",
    "                        wd_df.loc[len(wd_df)] = [term,i,wd_bow,cs]\n",
    "                    # if there are no tokens except the target term, cs == None\n",
    "                    else:\n",
    "                        wd_df.loc[len(wd_df)] = [term,i,wd_bow,None]\n",
    "                    \n",
    "    # if there are no related matches, cs == None, wd_bow == None\n",
    "    else:\n",
    "        wd_df.loc[len(wd_df)] = [term,i,None,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subfunction for spacy similarity\n",
    "def _get_sim(bow_1:list, bow_2:list, nlp) -> float:\n",
    "    '''\n",
    "    bow_1 and bow_2: list, two bags of words; target term should be excluded\n",
    "    Concerts lists to str of tokens separated by spaces\n",
    "    Returns similarity score (float) rounded\n",
    "    '''\n",
    "    # converting list to str, spaces as a separator\n",
    "    bow_1_str = \"\"\n",
    "    for t in bow_1:\n",
    "        bow_1_str += f\"{t} \"\n",
    "        \n",
    "    bow_2_str = \"\"\n",
    "    for t in bow_2:\n",
    "        bow_2_str += f\"{t} \"\n",
    "        \n",
    "    bow_1 = nlp(bow_1_str)\n",
    "    bow_2 = nlp(bow_2_str)\n",
    "        \n",
    "    sim = bow_1.similarity(bow_2)\n",
    "    \n",
    "    return round(sim,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wd_df.to_csv('spacy_sim_experiment.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating AAT EN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cs(bow_1:list, bow_2:list, nlp) -> float:\n",
    "    '''\n",
    "    Calculates cosine similarity between two bags of words;\n",
    "    based on the spacy vectors\n",
    "    bow_1 and bow_2: list, two bags of words;\n",
    "    nlp: spacy nlp class loaded through spacy.load\n",
    "    '''\n",
    "\n",
    "    # converting list to str, spaces as a separator\n",
    "\n",
    "    bow_1_str = \"\"\n",
    "    for t in bow_1:\n",
    "        bow_1_str += f\"{t} \"\n",
    "        \n",
    "    bow_2_str = \"\"\n",
    "    for t in bow_2:\n",
    "        bow_2_str += f\"{t} \"\n",
    "        \n",
    "    bow_1 = nlp(bow_1_str)\n",
    "    bow_2 = nlp(bow_2_str)\n",
    "        \n",
    "    sim = round(bow_1.similarity(bow_2),3)\n",
    "    \n",
    "    return sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing AAT with scopeNotes TF-IDF\n",
    "with open('/Users/anesterov/reps/LODlit/AAT/aat_bows_tfidf_en.json','r') as jf:\n",
    "    aat_tfidf_en = json.load(jf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing background info\n",
    "with open(\"/Users/anesterov/reps/LODlit/bg/rm_bows_tfidf_all.json\",\"r\") as jf:\n",
    "    bg_info = json.load(jf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang = \"en\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "aat_df = pd.DataFrame(columns=['term', 'URI', 'aat_bow', 'cs_scopeNote_tfidf'])\n",
    "\n",
    "for term, hits in aat_tfidf_en.items():\n",
    "    \n",
    "    if term in bg_info[lang].keys():\n",
    "    \n",
    "        # collecting bg info bow\n",
    "        bg_combined = []\n",
    "        if bg_info[lang][term].get('aat'):\n",
    "            bg_combined.extend(bg_info[lang][term]['aat'])\n",
    "        if bg_info[lang][term].get('wikidata'):\n",
    "            bg_combined.extend(bg_info[lang][term]['wikidata'])\n",
    "        if bg_info[lang][term].get('pwn'):\n",
    "            bg_combined.extend(bg_info[lang][term]['pwn'])\n",
    "        if bg_info[lang][term].get('wm'):\n",
    "            bg_combined.extend(bg_info[lang][term]['wm'])\n",
    "\n",
    "        # making a set\n",
    "        bg_set = list(set(bg_combined))\n",
    "            \n",
    "        # if there are search results\n",
    "        if len(hits) > 0:\n",
    "            for hit in hits:\n",
    "                for i, bow in hit.items():\n",
    "                    # making a set\n",
    "                    aat_bow = list(set(bow))\n",
    "                    if len(aat_bow) > 0:\n",
    "                        \n",
    "                        # calculate cs\n",
    "                        cs_combined = calculate_cs(bg_set,aat_bow,nlp)\n",
    "                        \n",
    "                        aat_df.loc[len(aat_df)] = [term,i,aat_bow,cs_combined]\n",
    "                    \n",
    "                    # if there are no tokens, cs == None\n",
    "                    else:\n",
    "                        aat_df.loc[len(aat_df)] = [term,i,aat_bow,None]\n",
    "                    \n",
    "    # if there are no related matches, cs == None\n",
    "    else:\n",
    "        aat_df.loc[len(aat_df)] = [term,i,aat_bow,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "aat_df.to_csv('aat_cs_tfidf_en.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating PWN CS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing AAT\n",
    "with open('/Users/anesterov/reps/LODlit/PWN/pwn31_bows.json','r') as jf:\n",
    "    pwn = json.load(jf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwn_df = pd.DataFrame(columns=['term', 'synset_ID', 'pwn_bow', 'spacy_cs'])\n",
    "\n",
    "for term, hits in pwn.items():\n",
    "    \n",
    "    if term in rm[lang].keys():\n",
    "    \n",
    "        # EN collecting related matches per term\n",
    "        rm_per_term = []\n",
    "        if rm[lang][term].get('aat'):\n",
    "            rm_per_term.extend(rm[lang][term]['aat'])\n",
    "        if rm[lang][term].get('wikidata'):\n",
    "            rm_per_term.extend(rm[lang][term]['wikidata'])\n",
    "        if rm[lang][term].get('pwn'):\n",
    "            rm_per_term.extend(rm[lang][term]['pwn'])\n",
    "\n",
    "        # remove target term from bow\n",
    "        rm_per_term_no_target = list(set([t for t in rm_per_term if t != term]))\n",
    "        \n",
    "        rm_string = \"\"\n",
    "        for t in rm_per_term_no_target:\n",
    "            rm_string += f\"{t} \"\n",
    "            \n",
    "        rm_bow_nlp = nlp(rm_string)\n",
    "            \n",
    "        # if there are search results\n",
    "        if len(hits) > 0:\n",
    "            for hit in hits:\n",
    "                for i, bow in hit.items():\n",
    "                    # remove target term from bow\n",
    "                    aat_bow = list(set([t for t in bow if t != term]))\n",
    "                    if len(aat_bow) > 0:\n",
    "                        aat_str = \"\"\n",
    "                        for t in aat_bow:\n",
    "                            aat_str += f\"{t} \"\n",
    "                        aat_bow_nlp = nlp(aat_str)\n",
    "                        \n",
    "                        # calculate cs\n",
    "                        cs = round(rm_bow_nlp.similarity(aat_bow_nlp),3)\n",
    "                        aat_df.loc[len(aat_df)] = [term,i,aat_bow,cs]\n",
    "                    # if there are no tokens except the target term, cs == None\n",
    "                    else:\n",
    "                        aat_df.loc[len(aat_df)] = [term,i,aat_bow,None]\n",
    "                    \n",
    "    # if there are no related matches, cs == None\n",
    "    else:\n",
    "        aat_df.loc[len(aat_df)] = [term,i,aat_bow,None]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generates 2 csv files with background information for EN and NL terms\n",
    "\"bg_en.csv\", \"bg_nl.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. collect query terms\n",
    "#2. collect lemmas\n",
    "#3. get rm lit\n",
    "#4. get wm text\n",
    "#5. export csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_lemma_by_term(query_term:str, lang:str) -> str:\n",
    "    '''\n",
    "    Getting a lemma of a query term\n",
    "    lang: str, 'en' or 'nl'\n",
    "    Returns str, 'not found' if lemma was not found\n",
    "    '''\n",
    "    \n",
    "    return_lemma = 'not found'\n",
    "    \n",
    "    # importing query terms with lemmas\n",
    "    # change path to GitHub\n",
    "    \n",
    "    with open('/query_terms.json','r') as jf:\n",
    "        query_terms = json.load(jf)\n",
    "        \n",
    "    for lemma, qt in query_terms[lang].items():\n",
    "        if query_term in qt:\n",
    "            return_lemma = lemma\n",
    "            \n",
    "    return return_lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing subsets EN\n",
    "with open('/Wikidata/wd_en_subset.json','r') as jf:\n",
    "    wd_en_subset = json.load(jf)\n",
    "with open('/AAT/aat_en_subset.json','r') as jf:\n",
    "    aat_en_subset = json.load(jf)\n",
    "with open('/PWN/pwn_subset.json','r') as jf:\n",
    "    pwn_subset = json.load(jf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all unique query terms\n",
    "qt_en = []\n",
    "qt_en.extend(list(wd_en_subset))\n",
    "qt_en.extend(list(aat_en_subset))\n",
    "qt_en.extend(list(pwn_subset))\n",
    "qt_en_u = list(set(qt_en))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all unique lemmas\n",
    "en_lemmas = []\n",
    "for qt in qt_en_u:\n",
    "    en_lemmas.append(_get_lemma_by_term(qt,\"en\"))\n",
    "en_lemmas = list(set(en_lemmas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing rm \n",
    "with open('/rm/rm_wd_en.json','r') as jf:\n",
    "    rm_wd_en = json.load(jf)\n",
    "with open('/rm/rm_aat_en.json','r') as jf:\n",
    "    rm_aat_en = json.load(jf)\n",
    "with open('/rm/rm_pwn.json','r') as jf:\n",
    "    rm_pwn = json.load(jf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma_lit_en = {}\n",
    "\n",
    "for lemma in en_lemmas:\n",
    "    \n",
    "    source1 = []\n",
    "    source2 = []\n",
    "    source3 = []\n",
    "    \n",
    "    # source1: wikidata\n",
    "    for hit in rm_wd_en[lemma]:\n",
    "        source1.append(hit['prefLabel'])\n",
    "        if hit[\"aliases\"]:\n",
    "            lits.extend(hit[\"aliases\"])\n",
    "        if hit[\"description\"]:\n",
    "            lits.extend(hit[\"description\"])\n",
    "        source1.extend(hit['instance_of'])\n",
    "        source1.extend(hit['subclass_of'])\n",
    "    \n",
    "    # source2: aat\n",
    "    for hit in rm_aat_en[lemma]:\n",
    "        source2.append(hit['prefLabel'])\n",
    "        source2.extend(hit['altLabel'])\n",
    "        source2.append(hit['scopeNote'])\n",
    "        source2.append(hit['prefLabel_comment'])\n",
    "        source2.extend(hit['altLabel_comment'])\n",
    "    \n",
    "    # source3: pwn\n",
    "    for hit in rm_pwn[lemma]:\n",
    "        source3.extend(hit['lemmata'])\n",
    "        source3.append(hit['definition'])\n",
    "        source3.extend(hit['examples'])\n",
    "        \n",
    "    lemma_lit_en[lemma] = {\"source_1\":list(set(source1)),\\\n",
    "                           \"source_2\":list(set(source2)),\\\n",
    "                           \"source_3\":list(set(source3))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding wm text\n",
    "with open(\"/bg/en_wm_bows.json\",'r') as jf:\n",
    "    wm_en = json.load(jf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lemma, bg in lemma_lit_en.items():\n",
    "    bg[\"wm\"] = wm_en[lemma][\"wm_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove empty str\n",
    "for lemma, bg in lemma_lit_en.items():\n",
    "    for source, text_list in bg.items():\n",
    "        bg[source] = [t for t in text_list if t != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export a csv\n",
    "with open('bg_en.csv','w') as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "    header = [\"term\",\"source_1\",\"source_2\",\"source_3\",\"wm\"]\n",
    "    writer.writerow(header)\n",
    "    \n",
    "    for lemma, bg in lemma_lit_en.items():\n",
    "        data = [lemma, bg[\"source_1\"], bg[\"source_2\"], bg[\"source_3\"], bg[\"wm\"]]\n",
    "        writer.writerow(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing subsets NL\n",
    "with open('/Wikidata/wd_nl_subset.json','r') as jf:\n",
    "    wd_nl_subset = json.load(jf)\n",
    "with open('/AAT/aat_nl_subset.json','r') as jf:\n",
    "    aat_nl_subset = json.load(jf)\n",
    "with open('/ODWN/odwn_subset.json','r') as jf:\n",
    "    odwn_subset = json.load(jf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all unique query terms\n",
    "qt_nl = []\n",
    "qt_nl.extend(list(wd_nl_subset))\n",
    "qt_nl.extend(list(aat_nl_subset))\n",
    "qt_nl.extend(list(odwn_subset))\n",
    "qt_nl_u = list(set(qt_nl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all unique lemmas\n",
    "nl_lemmas = []\n",
    "for qt in qt_nl_u:\n",
    "    nl_lemmas.append(_get_lemma_by_term(qt,\"nl\"))\n",
    "nl_lemmas = list(set(nl_lemmas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing rm \n",
    "with open('/rm/rm_wd_nl.json','r') as jf:\n",
    "    rm_wd_nl = json.load(jf)\n",
    "with open('/rm/rm_aat_nl.json','r') as jf:\n",
    "    rm_aat_nl = json.load(jf)\n",
    "with open('/rm/rm_odwn.json','r') as jf:\n",
    "    rm_odwn = json.load(jf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma_lit_nl = {}\n",
    "\n",
    "for lemma in nl_lemmas:\n",
    "    \n",
    "    source1 = []\n",
    "    source2 = []\n",
    "    source3 = []\n",
    "    \n",
    "    # source1: wikidata\n",
    "    for hit in rm_wd_nl[lemma]:\n",
    "        source1.append(hit['prefLabel'])\n",
    "        if hit[\"aliases\"]:\n",
    "            lits.extend(hit[\"aliases\"])\n",
    "        if hit[\"description\"]:\n",
    "            lits.extend(hit[\"description\"])\n",
    "        source1.extend(hit['instance_of'])\n",
    "        source1.extend(hit['subclass_of'])\n",
    "    \n",
    "    # source2: aat\n",
    "    for hit in rm_aat_nl[lemma]:\n",
    "        source2.append(hit['prefLabel'])\n",
    "        source2.extend(hit['altLabel'])\n",
    "        source2.append(hit['scopeNote'])\n",
    "        source2.append(hit['prefLabel_comment'])\n",
    "        source2.extend(hit['altLabel_comment'])\n",
    "    \n",
    "    # source3: odwn\n",
    "    for hit in rm_odwn[lemma]:\n",
    "        source3.append(hit.get('le_written_form'))\n",
    "        source3.append(hit.get('sense_definition'))\n",
    "        source3.extend(hit.get('sense_examples'))\n",
    "        source3.extend(hit.get('synonyms'))\n",
    "        source3.extend(hit.get('synset_definitions'))\n",
    "        \n",
    "    lemma_lit_nl[lemma] = {\"source_1\":list(set(source1)),\\\n",
    "                           \"source_2\":list(set(source2)),\\\n",
    "                           \"source_3\":list(set(source3))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding wm text\n",
    "with open(\"/bg/nl_wm_bows.json\",'r') as jf:\n",
    "    wm_nl = json.load(jf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lemma, bg in lemma_lit_nl.items():\n",
    "    bg[\"wm\"] = wm_nl[lemma][\"wm_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove empty str\n",
    "for lemma, bg in lemma_lit_nl.items():\n",
    "    for source, text_list in bg.items():\n",
    "        bg[source] = [t for t in text_list if t != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export a csv\n",
    "with open('bg_nl.csv','w') as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "    header = [\"term\",\"source_1\",\"source_2\",\"source_3\",\"wm\"]\n",
    "    writer.writerow(header)\n",
    "    \n",
    "    for lemma, bg in lemma_lit_nl.items():\n",
    "        data = [lemma, bg[\"source_1\"], bg[\"source_2\"], bg[\"source_3\"], bg[\"wm\"]]\n",
    "        writer.writerow(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

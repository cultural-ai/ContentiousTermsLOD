{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1. For each resource, divide lemmas into quartiles based on N of entities they have\n",
    "### 2. In each quartile, randomly draw 10 entities excluding related match entities; take only unique entities (40 in total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_lemma_by_term(query_term:str, lang:str) -> str:\n",
    "    '''\n",
    "    Getting a lemma of a query term\n",
    "    lang: str, 'en' or 'nl'\n",
    "    Returns str, 'not found' if lemma was not found\n",
    "    '''\n",
    "    \n",
    "    return_lemma = 'not found'\n",
    "    \n",
    "    # importing query terms with lemmas\n",
    "    # change path to GitHub\n",
    "    \n",
    "    with open('/Users/anesterov/reps/LODlit/query_terms.json','r') as jf:\n",
    "        query_terms = json.load(jf)\n",
    "        \n",
    "    for lemma, qt in query_terms[lang].items():\n",
    "        if query_term in qt:\n",
    "            return_lemma = lemma\n",
    "            \n",
    "    return return_lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_lemma_quartiles(lang:str, path_to_n_hits_by_lemma:str) -> dict:\n",
    "    '''\n",
    "    Grouping lemmas in quartiles according to N of entities they have in resources\n",
    "    lang: str, 'en' or 'nl' (relevant only for wikidata and aat)\n",
    "    path_to_n_hits_by_lemma: str, a path to a csv file with N of hits by lemma (for example, '/LODlit/Wikidata/n_hits_by_lemma.csv')\n",
    "    Returns dict {'q1':['lemma']}\n",
    "    '''\n",
    "    \n",
    "    results = {}\n",
    "\n",
    "    resource_hits = pd.read_csv(path_to_n_hits_by_lemma)\n",
    "    \n",
    "    # taking entities with hits > 0\n",
    "    resource_hits = resource_hits[resource_hits['total_lemma'] > 0]\n",
    "    \n",
    "    if 'lang' in resource_hits.columns:\n",
    "        resource_hits_by_lang = resource_hits.loc[resource_hits['lang'] == lang]\n",
    "    # for PWN and ODWN\n",
    "    else:\n",
    "        resource_hits_by_lang = resource_hits\n",
    "    \n",
    "    # getting quantiles values\n",
    "    q_values = list(resource_hits_by_lang[\"total_lemma\"].quantile([0,0.25,0.5,0.75,1]))\n",
    "    \n",
    "    # getting list of lemmas by quartiles\n",
    "    # quartile 1\n",
    "    results['q1'] = [row[1]['lemma'] for row in resource_hits_by_lang.iterrows() if int(q_values[0]) <= row[1]['total_lemma'] <= int(q_values[1])]\n",
    "    # quartile 2\n",
    "    results['q2'] = [row[1]['lemma'] for row in resource_hits_by_lang.iterrows() if int(q_values[1]) <= row[1]['total_lemma'] <= int(q_values[2])]\n",
    "    # quartile 3\n",
    "    results['q3'] = [row[1]['lemma'] for row in resource_hits_by_lang.iterrows() if int(q_values[2]) <= row[1]['total_lemma'] <= int(q_values[3])]\n",
    "    # quartile 4\n",
    "    results['q4'] = [row[1]['lemma'] for row in resource_hits_by_lang.iterrows() if int(q_values[3]) <= row[1]['total_lemma'] <= int(q_values[4])]\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_unique_entities_per_quartile(lemma_quartiles:dict, entities_per_lemma:dict) -> dict:\n",
    "    \n",
    "    entity_quartiles = {}\n",
    "    entity_quartiles['q1'] = []\n",
    "    entity_quartiles['q2'] = []\n",
    "    entity_quartiles['q3'] = []\n",
    "    entity_quartiles['q4'] = []\n",
    "    \n",
    "    for lemma in lemma_quartiles['q1']:\n",
    "        entity_quartiles['q1'].extend(entities_per_lemma[lemma])\n",
    "        \n",
    "    for lemma in lemma_quartiles['q2']:\n",
    "        entity_quartiles['q2'].extend(entities_per_lemma[lemma])\n",
    "        \n",
    "    for lemma in lemma_quartiles['q3']:\n",
    "        entity_quartiles['q3'].extend(entities_per_lemma[lemma])\n",
    "    \n",
    "    for lemma in lemma_quartiles['q4']:\n",
    "        entity_quartiles['q4'].extend(entities_per_lemma[lemma])\n",
    "    \n",
    "    # take only unique entities\n",
    "    \n",
    "    unique_e = []\n",
    "    for q, entities in entity_quartiles.items():\n",
    "        unique_per_q = []\n",
    "        for e in entities:\n",
    "            if e not in unique_e:\n",
    "                unique_per_q.append(e)\n",
    "                unique_e.append(e)\n",
    "        entity_quartiles[q] = unique_per_q\n",
    "                \n",
    "    return entity_quartiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _draw_random_entities_per_q(entities_per_q:dict) -> dict:\n",
    "    \n",
    "    random_per_q = {}\n",
    "    \n",
    "    # check len\n",
    "    if len(entities_per_q['q1']) < 10:\n",
    "        add_k = 10 - len(entities_per_q['q1'])\n",
    "        random_per_q['q1'] = entities_per_q['q1']\n",
    "    else:\n",
    "        random_per_q['q1'] = random.sample(entities_per_q['q1'], k=10)\n",
    "        \n",
    "    random_per_q['q2'] = random.sample(entities_per_q['q2'], k=10)\n",
    "    random_per_q['q3'] = random.sample(entities_per_q['q3'], k=10)\n",
    "    random_per_q['q4'] = random.sample(entities_per_q['q4'], k=10)\n",
    "\n",
    "    return random_per_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_resource_properties(resource:str,lang:str) -> dict:\n",
    "    \n",
    "    resource_props = {}\n",
    "    \n",
    "    if resource == 'wikidata':\n",
    "        resource_props[\"path_to_n_hits_by_lemma\"] = \"/Users/anesterov/reps/LODlit/Wikidata/n_hits_by_lemma.csv\"\n",
    "        resource_props[\"subset_path\"] = f\"/Users/anesterov/reps/LODlit/Wikidata/wd_{lang}_subset.json\"\n",
    "        resource_props[\"entity_id_key\"] = [\"QID\"]\n",
    "        resource_props[\"lit_1\"] = \"prefLabel\"\n",
    "        resource_props[\"lit_2\"] = \"aliases\"\n",
    "        resource_props[\"lit_3\"] = \"description\"\n",
    "        resource_props[\"lit_4\"] = \"instance_of\"\n",
    "        resource_props[\"lit_5\"] = \"subclass_of\"\n",
    "        \n",
    "    if resource == 'aat':\n",
    "        resource_props[\"path_to_n_hits_by_lemma\"] = \"/Users/anesterov/reps/LODlit/AAT/n_hits_by_lemma.csv\"\n",
    "        resource_props[\"subset_path\"] = f\"/Users/anesterov/reps/LODlit/AAT/aat_{lang}_subset.json\"\n",
    "        resource_props[\"entity_id_key\"] = [\"aat_uri\"]\n",
    "        resource_props[\"lit_1\"] = \"prefLabel\"\n",
    "        resource_props[\"lit_2\"] = \"altLabel\"\n",
    "        resource_props[\"lit_3\"] = \"scopeNote\"\n",
    "        resource_props[\"lit_4\"] = \"prefLabel_comment\"\n",
    "        resource_props[\"lit_5\"] = \"altLabel_comment\"\n",
    "    \n",
    "    if resource == 'pwn':\n",
    "        resource_props[\"path_to_n_hits_by_lemma\"] = \"/Users/anesterov/reps/LODlit/PWN/pwn31_hits_by_lemma.csv\"\n",
    "        resource_props[\"subset_path\"] = \"/Users/anesterov/reps/LODlit/PWN/pwn_subset.json\"\n",
    "        resource_props[\"entity_id_key\"] = [\"synset_id\"]\n",
    "        resource_props[\"lit_1\"] = \"lemmata\"\n",
    "        resource_props[\"lit_2\"] = \"definition\"\n",
    "        resource_props[\"lit_3\"] = \"examples\"\n",
    "        \n",
    "    if resource == 'odwn':\n",
    "        resource_props[\"path_to_n_hits_by_lemma\"] = \"/Users/anesterov/reps/LODlit/ODWN/odwn_hits_by_lemma.csv\"\n",
    "        resource_props[\"subset_path\"] = \"/Users/anesterov/reps/LODlit/ODWN/odwn_subset.json\"\n",
    "        resource_props[\"entity_id_key\"] = [\"synset_id\",\"le_id\"]\n",
    "        resource_props[\"lit_1\"] = \"le_written_form\"\n",
    "        resource_props[\"lit_2\"] = \"sense_definition\"\n",
    "        resource_props[\"lit_3\"] = \"sense_examples\"\n",
    "        resource_props[\"lit_4\"] = \"synonyms\"\n",
    "        resource_props[\"lit_5\"] = \"synset_definitions\"\n",
    "        \n",
    "    return resource_props"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_by_resource(resource:str,lang:str):\n",
    "    '''\n",
    "    lang: str, 'en' or 'nl'\n",
    "    '''\n",
    "    subset_df = pd.DataFrame()\n",
    "    \n",
    "    # load the resource props\n",
    "    resource_props = _get_resource_properties(resource,lang)\n",
    "        \n",
    "    # import query terms\n",
    "    with open('/Users/anesterov/reps/LODlit/query_terms.json','r') as jf:\n",
    "        query_terms = json.load(jf)\n",
    "    \n",
    "    # load the subset\n",
    "    with open(resource_props['subset_path'],'r') as jf:\n",
    "        subset = json.load(jf)\n",
    "        \n",
    "    # load related match entities\n",
    "    rm_e = pd.read_csv(\"/Users/anesterov/reps/LODlit/rm/rm_entities_unique.csv\")\n",
    "    # selecting rm entities per lemma by lang and resource; rewrite rm_e\n",
    "    rm_e = rm_e[(rm_e[\"lang\"] == lang) & (rm_e[\"resource\"] == resource)]\n",
    "        \n",
    "    # get unique entities per lemma\n",
    "    unique_e_per_lemma = {}\n",
    "\n",
    "    for lemma, terms in query_terms[lang].items():\n",
    "        e_per_lemma = []\n",
    "        for query_term, subset_hits in subset.items():\n",
    "            if query_term in terms:\n",
    "                # special condition for ODWN\n",
    "                if resource == 'odwn':\n",
    "                    for hit in subset_hits:\n",
    "                        if resource_props['entity_id_key'][0] not in hit.keys():\n",
    "                            e_per_lemma.append(hit[resource_props['entity_id_key'][1]])\n",
    "                        else:\n",
    "                            e_per_lemma.append(hit[resource_props['entity_id_key'][0]])\n",
    "                else:\n",
    "                    e_per_lemma.extend([hit[resource_props['entity_id_key'][0]] for hit in subset_hits])\n",
    "        unique_e_per_lemma[lemma] = list(set(e_per_lemma))\n",
    "        \n",
    "    # get lemma quartiles\n",
    "    lemma_quartiles = _get_lemma_quartiles(lang,resource_props['path_to_n_hits_by_lemma'])\n",
    "    # divide entities into quartiles\n",
    "    entities_per_q = _get_unique_entities_per_quartile(lemma_quartiles,unique_e_per_lemma)\n",
    "    # get 10 random entities per quartile\n",
    "    random_per_q = _draw_random_entities_per_q(entities_per_q)\n",
    "    # generate a df sample\n",
    "\n",
    "    for q, entities in random_per_q.items():\n",
    "        for query_term, hits in subset.items():\n",
    "            lemma = _get_lemma_by_term(query_term,lang)\n",
    "            for hit in hits:\n",
    "                # special condition for ODWN\n",
    "                if resource == 'odwn' and hit.get(resource_props['entity_id_key'][0]) == None:\n",
    "                    if hit[resource_props['entity_id_key'][1]] in entities and lemma in lemma_quartiles[q]:\n",
    "                        row = {\"term\":lemma,\"entity_id\":hit.get(resource_props['entity_id_key'][1]),\\\n",
    "                               \"text_1\":hit.get(resource_props[\"lit_1\"]),\"text_2\":hit.get(resource_props[\"lit_2\"]),\\\n",
    "                              \"text_3\":hit.get(resource_props[\"lit_3\"]),\"text_4\":hit.get(resource_props[\"lit_4\"]),\\\n",
    "                               \"text_5\":hit.get(resource_props[\"lit_5\"])}\n",
    "                        subset_df = subset_df.append(row,ignore_index=True)\n",
    "                else:\n",
    "                    # special condition for PWN\n",
    "                    if resource == 'pwn':\n",
    "                        if hit[resource_props['entity_id_key'][0]] in entities and lemma in lemma_quartiles[q]:\n",
    "                            row = {\"term\":lemma,\"entity_id\":hit[resource_props['entity_id_key'][0]],\\\n",
    "                               \"text_1\":hit[resource_props.get(\"lit_1\")],\"text_2\":hit[resource_props.get(\"lit_2\")],\\\n",
    "                              \"text_3\":hit[resource_props.get(\"lit_3\")]}\n",
    "                            subset_df = subset_df.append(row,ignore_index=True)\n",
    "                    else:\n",
    "                        if hit[resource_props['entity_id_key'][0]] in entities and lemma in lemma_quartiles[q]:\n",
    "                            row = {\"term\":lemma,\"entity_id\":hit.get(resource_props['entity_id_key'][0]),\\\n",
    "                               \"text_1\":hit.get(resource_props.get(\"lit_1\")),\"text_2\":hit.get(resource_props.get(\"lit_2\")),\\\n",
    "                              \"text_3\":hit.get(resource_props.get(\"lit_3\")),\"text_4\":hit.get(resource_props.get(\"lit_4\")),\\\n",
    "                               \"text_5\":hit.get(resource_props.get(\"lit_5\"))}\n",
    "                            subset_df = subset_df.append(row,ignore_index=True)\n",
    "    \n",
    "    subset_df.drop_duplicates(subset=['entity_id'],inplace=True)\n",
    "    \n",
    "    return subset_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generating sample files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = get_sample_by_resource('odwn','nl')\n",
    "sample.to_csv(\"/Users/anesterov/reps/LODlit/samples/odwn.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Annotated samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from nltk.metrics.agreement import AnnotationTask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AnnotationTask(data=[('annotator', 'entity_id', 'response'),('annotator', 'entity_id', 'response'),...])\n",
    "# there can be 3 responses: 'match', 'no_match', 'cannot decide'\n",
    "# or 2 responses: 'match', 'no_match' ('cannot decide' are excluded; there might be missing values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pairwise_a(resource:str,lang:str,annotator_id:list,binary=False)-> float:\n",
    "    '''\n",
    "    Calculate pairwise agreement using Krippendorff's alpha \n",
    "    resource: str, only \"wd\", \"aat\", \"pwn\" or \"odwn\";\n",
    "    lang: str, \"wd\" and \"aat\" are \"en\" or \"nl\", \"pwn\" is only \"en\", \"odwn\" is only \"nl\";\n",
    "    annotator_id: list of annotators' ids, for example [\"1\",\"2\"]\n",
    "    binary: boolean, True – take only the \"match\" column; False – incl. \"cannot_decide\"; default is False;\n",
    "    Returns float, rounded K alpha\n",
    "    '''\n",
    "    \n",
    "    # importing annotated samples\n",
    "    annotator_one = pd.read_csv(f\"/Users/anesterov/reps/LODlit/samples/ann_{resource}_{lang}_0{annotator_id[0]}.csv\")\n",
    "    annotator_two = pd.read_csv(f\"/Users/anesterov/reps/LODlit/samples/ann_{resource}_{lang}_0{annotator_id[1]}.csv\")\n",
    "    \n",
    "    responses_one = []\n",
    "    responses_two = []\n",
    "    \n",
    "    # adding a column with formatted responses\n",
    "    \n",
    "    if binary:\n",
    "        # do not include \"cannot_decide\" responses\n",
    "        for row in annotator_one.iterrows():\n",
    "            if row[1][\"check\"] == True:\n",
    "                response = \"match\"\n",
    "            if row[1][\"check\"] == False and row[1][\"cannot_decide\"] == False:\n",
    "                response = \"no_match\"\n",
    "            responses_one.append(response)\n",
    "            \n",
    "        for row in annotator_two.iterrows():\n",
    "            if row[1][\"check\"] == True:\n",
    "                response = \"match\"\n",
    "            if row[1][\"check\"] == False and row[1][\"cannot_decide\"] == False:\n",
    "                response = \"no_match\"\n",
    "            responses_two.append(response)\n",
    "        \n",
    "    else:\n",
    "        for row in annotator_one.iterrows():\n",
    "            if row[1][\"check\"] == True:\n",
    "                response = \"match\"\n",
    "            if row[1][\"check\"] == False and row[1][\"cannot_decide\"] == False:\n",
    "                response = \"no_match\"\n",
    "            if row[1][\"check\"] == False and row[1][\"cannot_decide\"] == True:\n",
    "                response = \"cannot_decide\"\n",
    "            responses_one.append(response)\n",
    "\n",
    "        for row in annotator_two.iterrows():\n",
    "            if row[1][\"check\"] == True:\n",
    "                response = \"match\"\n",
    "            if row[1][\"check\"] == False and row[1][\"cannot_decide\"] == False:\n",
    "                response = \"no_match\"\n",
    "            if row[1][\"check\"] == False and row[1][\"cannot_decide\"] == True:\n",
    "                response = \"cannot_decide\"\n",
    "            responses_two.append(response)\n",
    "            \n",
    "    annotator_one[\"response\"] = responses_one\n",
    "    # add the annotator id column\n",
    "    annotator_one.insert(0,\"annotator_id\",[f\"a{annotator_id[0]}\" for i in range(0,len(responses_one))])\n",
    "    \n",
    "    annotator_two[\"response\"] = responses_two\n",
    "    # add the annotator id column\n",
    "    annotator_two.insert(0,\"annotator_id\",[f\"a{annotator_id[1]}\" for i in range(0,len(responses_two))])\n",
    "    \n",
    "    # taking only annotator_id, entity_id, and respose\n",
    "    a1 = annotator_one[[\"annotator_id\",\"entity_id\",\"response\"]]\n",
    "    a2 = annotator_two[[\"annotator_id\",\"entity_id\",\"response\"]]\n",
    "    \n",
    "    # getting a list of tuples of all responses to calculate alpha\n",
    "    all_responses = [tuple(l) for l in a1.values.tolist()]\n",
    "    all_responses.extend([tuple(l) for l in a2.values.tolist()])\n",
    "    \n",
    "    # calculating K aplha\n",
    "    t = AnnotationTask(data=all_responses)\n",
    "    k_alpha = round(t.alpha(),3)\n",
    "    \n",
    "    return k_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Geenrate an agreement overview\n",
    "\n",
    "with open('annotators_agreement.csv','w') as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "    header = [\"sample\",\"annotators\",\"K_alpha_3_options\",\"K_alpha_2_options\"]\n",
    "    writer.writerow(header)\n",
    "    \n",
    "    # WD EN\n",
    "    data = [\"wd_en\",\"1,2\",get_pairwise_a(\"wd\",\"en\",[\"1\",\"2\"]),get_pairwise_a(\"wd\",\"en\",[\"1\",\"2\"],True)]\n",
    "    writer.writerow(data)\n",
    "    # AAT EN\n",
    "    data = [\"aat_en\",\"1,2\",get_pairwise_a(\"aat\",\"en\",[\"1\",\"2\"]),get_pairwise_a(\"aat\",\"en\",[\"1\",\"2\"],True)]\n",
    "    writer.writerow(data)\n",
    "    # PWN\n",
    "    data = [\"pwn\",\"1,2\",get_pairwise_a(\"pwn\",\"en\",[\"1\",\"2\"]),get_pairwise_a(\"pwn\",\"en\",[\"1\",\"2\"],True)]\n",
    "    writer.writerow(data)\n",
    "    # WD NL\n",
    "    data = [\"wd_nl\",\"1,3\",get_pairwise_a(\"wd\",\"nl\",[\"1\",\"3\"]),get_pairwise_a(\"wd\",\"nl\",[\"1\",\"3\"],True)]\n",
    "    writer.writerow(data)\n",
    "    # AAT NL\n",
    "    data = [\"aat_nl\",\"1,3\",get_pairwise_a(\"aat\",\"nl\",[\"1\",\"3\"]),get_pairwise_a(\"aat\",\"nl\",[\"1\",\"3\"],True)]\n",
    "    writer.writerow(data)\n",
    "    # ODWN\n",
    "    data = [\"odwn\",\"1,3\",get_pairwise_a(\"odwn\",\"nl\",[\"1\",\"3\"]),get_pairwise_a(\"odwn\",\"nl\",[\"1\",\"3\"],True)]\n",
    "    writer.writerow(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.103"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# WD EN\n",
    "get_pairwise_a(\"wd\",\"en\",[\"1\",\"2\"],False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.149"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# AAT EN\n",
    "get_pairwise_a(\"aat\",\"en\",[\"1\",\"2\"],False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

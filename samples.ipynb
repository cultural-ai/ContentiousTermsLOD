{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1. For each resource, divide lemmas into quartiles based on N of entities they have\n",
    "### 2. In each quartile, randomly draw 10 entities excluding related match entities; take only unique entities (40 in total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_lemma_by_term(query_term:str, lang:str) -> str:\n",
    "    '''\n",
    "    Getting a lemma of a query term\n",
    "    lang: str, 'en' or 'nl'\n",
    "    Returns str, 'not found' if lemma was not found\n",
    "    '''\n",
    "    \n",
    "    return_lemma = 'not found'\n",
    "    \n",
    "    # importing query terms with lemmas\n",
    "    # change path to GitHub\n",
    "    \n",
    "    with open('/Users/anesterov/reps/LODlit/query_terms.json','r') as jf:\n",
    "        query_terms = json.load(jf)\n",
    "        \n",
    "    for lemma, qt in query_terms[lang].items():\n",
    "        if query_term in qt:\n",
    "            return_lemma = lemma\n",
    "            \n",
    "    return return_lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_lemma_quartiles(lang:str, path_to_n_hits_by_lemma:str) -> dict:\n",
    "    '''\n",
    "    Grouping lemmas in quartiles according to N of entities they have in resources\n",
    "    lang: str, 'en' or 'nl' (relevant only for wikidata and aat)\n",
    "    path_to_n_hits_by_lemma: str, a path to a csv file with N of hits by lemma (for example, '/LODlit/Wikidata/n_hits_by_lemma.csv')\n",
    "    Returns dict {'q1':['lemma']}\n",
    "    '''\n",
    "    \n",
    "    results = {}\n",
    "\n",
    "    resource_hits = pd.read_csv(path_to_n_hits_by_lemma)\n",
    "    \n",
    "    # taking entities with hits > 0\n",
    "    resource_hits = resource_hits[resource_hits['total_lemma'] > 0]\n",
    "    \n",
    "    if 'lang' in resource_hits.columns:\n",
    "        resource_hits_by_lang = resource_hits.loc[resource_hits['lang'] == lang]\n",
    "    # for PWN and ODWN\n",
    "    else:\n",
    "        resource_hits_by_lang = resource_hits\n",
    "    \n",
    "    # getting quantiles values\n",
    "    q_values = list(resource_hits_by_lang[\"total_lemma\"].quantile([0,0.25,0.5,0.75,1]))\n",
    "    \n",
    "    # getting list of lemmas by quartiles\n",
    "    # quartile 1\n",
    "    results['q1'] = [row[1]['lemma'] for row in resource_hits_by_lang.iterrows() if int(q_values[0]) <= row[1]['total_lemma'] <= int(q_values[1])]\n",
    "    # quartile 2\n",
    "    results['q2'] = [row[1]['lemma'] for row in resource_hits_by_lang.iterrows() if int(q_values[1]) <= row[1]['total_lemma'] <= int(q_values[2])]\n",
    "    # quartile 3\n",
    "    results['q3'] = [row[1]['lemma'] for row in resource_hits_by_lang.iterrows() if int(q_values[2]) <= row[1]['total_lemma'] <= int(q_values[3])]\n",
    "    # quartile 4\n",
    "    results['q4'] = [row[1]['lemma'] for row in resource_hits_by_lang.iterrows() if int(q_values[3]) <= row[1]['total_lemma'] <= int(q_values[4])]\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_unique_entities_per_quartile(lemma_quartiles:dict, entities_per_lemma:dict) -> dict:\n",
    "    \n",
    "    entity_quartiles = {}\n",
    "    entity_quartiles['q1'] = []\n",
    "    entity_quartiles['q2'] = []\n",
    "    entity_quartiles['q3'] = []\n",
    "    entity_quartiles['q4'] = []\n",
    "    \n",
    "    for lemma in lemma_quartiles['q1']:\n",
    "        entity_quartiles['q1'].extend(entities_per_lemma[lemma])\n",
    "        \n",
    "    for lemma in lemma_quartiles['q2']:\n",
    "        entity_quartiles['q2'].extend(entities_per_lemma[lemma])\n",
    "        \n",
    "    for lemma in lemma_quartiles['q3']:\n",
    "        entity_quartiles['q3'].extend(entities_per_lemma[lemma])\n",
    "    \n",
    "    for lemma in lemma_quartiles['q4']:\n",
    "        entity_quartiles['q4'].extend(entities_per_lemma[lemma])\n",
    "    \n",
    "    # take only unique entities\n",
    "    \n",
    "    unique_e = []\n",
    "    for q, entities in entity_quartiles.items():\n",
    "        unique_per_q = []\n",
    "        for e in entities:\n",
    "            if e not in unique_e:\n",
    "                unique_per_q.append(e)\n",
    "                unique_e.append(e)\n",
    "        entity_quartiles[q] = unique_per_q\n",
    "                \n",
    "    return entity_quartiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _draw_random_entities_per_q(entities_per_q:dict) -> dict:\n",
    "    \n",
    "    random_per_q = {}\n",
    "    \n",
    "    # check len\n",
    "    if len(entities_per_q['q1']) < 10:\n",
    "        add_k = 10 - len(entities_per_q['q1'])\n",
    "        random_per_q['q1'] = entities_per_q['q1']\n",
    "    else:\n",
    "        random_per_q['q1'] = random.sample(entities_per_q['q1'], k=10)\n",
    "        \n",
    "    random_per_q['q2'] = random.sample(entities_per_q['q2'], k=10)\n",
    "    random_per_q['q3'] = random.sample(entities_per_q['q3'], k=10)\n",
    "    random_per_q['q4'] = random.sample(entities_per_q['q4'], k=10)\n",
    "\n",
    "    return random_per_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_resource_properties(resource:str,lang:str) -> dict:\n",
    "    \n",
    "    resource_props = {}\n",
    "    \n",
    "    if resource == 'wikidata':\n",
    "        resource_props[\"path_to_n_hits_by_lemma\"] = \"/Users/anesterov/reps/LODlit/Wikidata/n_hits_by_lemma.csv\"\n",
    "        resource_props[\"subset_path\"] = f\"/Users/anesterov/reps/LODlit/Wikidata/wd_{lang}_subset.json\"\n",
    "        resource_props[\"entity_id_key\"] = [\"QID\"]\n",
    "        resource_props[\"lit_1\"] = \"prefLabel\"\n",
    "        resource_props[\"lit_2\"] = \"aliases\"\n",
    "        resource_props[\"lit_3\"] = \"description\"\n",
    "        resource_props[\"lit_4\"] = \"instance_of\"\n",
    "        resource_props[\"lit_5\"] = \"subclass_of\"\n",
    "        \n",
    "    if resource == 'aat':\n",
    "        resource_props[\"path_to_n_hits_by_lemma\"] = \"/Users/anesterov/reps/LODlit/AAT/n_hits_by_lemma.csv\"\n",
    "        resource_props[\"subset_path\"] = f\"/Users/anesterov/reps/LODlit/AAT/aat_{lang}_subset.json\"\n",
    "        resource_props[\"entity_id_key\"] = [\"aat_uri\"]\n",
    "        resource_props[\"lit_1\"] = \"prefLabel\"\n",
    "        resource_props[\"lit_2\"] = \"altLabel\"\n",
    "        resource_props[\"lit_3\"] = \"scopeNote\"\n",
    "        resource_props[\"lit_4\"] = \"prefLabel_comment\"\n",
    "        resource_props[\"lit_5\"] = \"altLabel_comment\"\n",
    "    \n",
    "    if resource == 'pwn':\n",
    "        resource_props[\"path_to_n_hits_by_lemma\"] = \"/Users/anesterov/reps/LODlit/PWN/pwn31_hits_by_lemma.csv\"\n",
    "        resource_props[\"subset_path\"] = \"/Users/anesterov/reps/LODlit/PWN/pwn_subset.json\"\n",
    "        resource_props[\"entity_id_key\"] = [\"synset_id\"]\n",
    "        resource_props[\"lit_1\"] = \"lemmata\"\n",
    "        resource_props[\"lit_2\"] = \"definition\"\n",
    "        resource_props[\"lit_3\"] = \"examples\"\n",
    "        \n",
    "    if resource == 'odwn':\n",
    "        resource_props[\"path_to_n_hits_by_lemma\"] = \"/Users/anesterov/reps/LODlit/ODWN/odwn_hits_by_lemma.csv\"\n",
    "        resource_props[\"subset_path\"] = \"/Users/anesterov/reps/LODlit/ODWN/odwn_subset.json\"\n",
    "        resource_props[\"entity_id_key\"] = [\"synset_id\",\"le_id\"]\n",
    "        resource_props[\"lit_1\"] = \"le_written_form\"\n",
    "        resource_props[\"lit_2\"] = \"sense_definition\"\n",
    "        resource_props[\"lit_3\"] = \"sense_examples\"\n",
    "        resource_props[\"lit_4\"] = \"synonyms\"\n",
    "        resource_props[\"lit_5\"] = \"synset_definitions\"\n",
    "        \n",
    "    return resource_props"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_by_resource(resource:str,lang:str):\n",
    "    '''\n",
    "    lang: str, 'en' or 'nl'\n",
    "    '''\n",
    "    subset_df = pd.DataFrame()\n",
    "    \n",
    "    # load the resource props\n",
    "    resource_props = _get_resource_properties(resource,lang)\n",
    "        \n",
    "    # import query terms\n",
    "    with open('/Users/anesterov/reps/LODlit/query_terms.json','r') as jf:\n",
    "        query_terms = json.load(jf)\n",
    "    \n",
    "    # load the subset\n",
    "    with open(resource_props['subset_path'],'r') as jf:\n",
    "        subset = json.load(jf)\n",
    "        \n",
    "    # load related match entities\n",
    "    rm_e = pd.read_csv(\"/Users/anesterov/reps/LODlit/rm/rm_entities_unique.csv\")\n",
    "    # selecting rm entities per lemma by lang and resource; rewrite rm_e\n",
    "    rm_e = rm_e[(rm_e[\"lang\"] == lang) & (rm_e[\"resource\"] == resource)]\n",
    "        \n",
    "    # get unique entities per lemma\n",
    "    unique_e_per_lemma = {}\n",
    "\n",
    "    for lemma, terms in query_terms[lang].items():\n",
    "        e_per_lemma = []\n",
    "        for query_term, subset_hits in subset.items():\n",
    "            if query_term in terms:\n",
    "                # special condition for ODWN\n",
    "                if resource == 'odwn':\n",
    "                    for hit in subset_hits:\n",
    "                        if resource_props['entity_id_key'][0] not in hit.keys():\n",
    "                            e_per_lemma.append(hit[resource_props['entity_id_key'][1]])\n",
    "                        else:\n",
    "                            e_per_lemma.append(hit[resource_props['entity_id_key'][0]])\n",
    "                else:\n",
    "                    e_per_lemma.extend([hit[resource_props['entity_id_key'][0]] for hit in subset_hits])\n",
    "        unique_e_per_lemma[lemma] = list(set(e_per_lemma))\n",
    "        \n",
    "    # get lemma quartiles\n",
    "    lemma_quartiles = _get_lemma_quartiles(lang,resource_props['path_to_n_hits_by_lemma'])\n",
    "    # divide entities into quartiles\n",
    "    entities_per_q = _get_unique_entities_per_quartile(lemma_quartiles,unique_e_per_lemma)\n",
    "    # get 10 random entities per quartile\n",
    "    random_per_q = _draw_random_entities_per_q(entities_per_q)\n",
    "    # generate a df sample\n",
    "\n",
    "    for q, entities in random_per_q.items():\n",
    "        for query_term, hits in subset.items():\n",
    "            lemma = _get_lemma_by_term(query_term,lang)\n",
    "            for hit in hits:\n",
    "                # special condition for ODWN\n",
    "                if resource == 'odwn' and hit.get(resource_props['entity_id_key'][0]) == None:\n",
    "                    if hit[resource_props['entity_id_key'][1]] in entities and lemma in lemma_quartiles[q]:\n",
    "                        row = {\"term\":lemma,\"entity_id\":hit.get(resource_props['entity_id_key'][1]),\\\n",
    "                               \"text_1\":hit.get(resource_props[\"lit_1\"]),\"text_2\":hit.get(resource_props[\"lit_2\"]),\\\n",
    "                              \"text_3\":hit.get(resource_props[\"lit_3\"]),\"text_4\":hit.get(resource_props[\"lit_4\"]),\\\n",
    "                               \"text_5\":hit.get(resource_props[\"lit_5\"])}\n",
    "                        subset_df = subset_df.append(row,ignore_index=True)\n",
    "                else:\n",
    "                    # special condition for PWN\n",
    "                    if resource == 'pwn':\n",
    "                        if hit[resource_props['entity_id_key'][0]] in entities and lemma in lemma_quartiles[q]:\n",
    "                            row = {\"term\":lemma,\"entity_id\":hit[resource_props['entity_id_key'][0]],\\\n",
    "                               \"text_1\":hit[resource_props.get(\"lit_1\")],\"text_2\":hit[resource_props.get(\"lit_2\")],\\\n",
    "                              \"text_3\":hit[resource_props.get(\"lit_3\")]}\n",
    "                            subset_df = subset_df.append(row,ignore_index=True)\n",
    "                    else:\n",
    "                        if hit[resource_props['entity_id_key'][0]] in entities and lemma in lemma_quartiles[q]:\n",
    "                            row = {\"term\":lemma,\"entity_id\":hit.get(resource_props['entity_id_key'][0]),\\\n",
    "                               \"text_1\":hit.get(resource_props.get(\"lit_1\")),\"text_2\":hit.get(resource_props.get(\"lit_2\")),\\\n",
    "                              \"text_3\":hit.get(resource_props.get(\"lit_3\")),\"text_4\":hit.get(resource_props.get(\"lit_4\")),\\\n",
    "                               \"text_5\":hit.get(resource_props.get(\"lit_5\"))}\n",
    "                            subset_df = subset_df.append(row,ignore_index=True)\n",
    "    \n",
    "    subset_df.drop_duplicates(subset=['entity_id'],inplace=True)\n",
    "    \n",
    "    return subset_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generating sample files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = get_sample_by_resource('odwn','nl')\n",
    "sample.to_csv(\"/Users/anesterov/reps/LODlit/samples/odwn.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'q1': ['allochtoon',\n",
       "  'baboo',\n",
       "  'bush negro',\n",
       "  'coolie',\n",
       "  'developing nations',\n",
       "  'footmen',\n",
       "  'full blood',\n",
       "  'half-blood',\n",
       "  'half-breed',\n",
       "  'hottentot',\n",
       "  'kaffir',\n",
       "  'lilliputian',\n",
       "  'low-income countries',\n",
       "  'mestizo',\n",
       "  'mohammedan',\n",
       "  'mongoloid',\n",
       "  'mulatto',\n",
       "  'primitivism',\n",
       "  'transvestite'],\n",
       " 'q2': ['barbarian',\n",
       "  'batavia',\n",
       "  'berber',\n",
       "  'caucasian',\n",
       "  'discover',\n",
       "  'eskimo',\n",
       "  'exotic',\n",
       "  'handicap',\n",
       "  'headhunter',\n",
       "  'hermaphrodite',\n",
       "  'homosexual',\n",
       "  'inuit',\n",
       "  'maroon',\n",
       "  'medicine man',\n",
       "  'métis',\n",
       "  'pygmy',\n",
       "  'retarded',\n",
       "  'southern rhodesia',\n",
       "  'third world'],\n",
       " 'q3': ['aboriginal',\n",
       "  'bombay',\n",
       "  'burma',\n",
       "  'calcutta',\n",
       "  'caucasian',\n",
       "  'disabled',\n",
       "  'ethnicity',\n",
       "  'first world',\n",
       "  'gay',\n",
       "  'gypsy',\n",
       "  'immigrant',\n",
       "  'madras',\n",
       "  'negro',\n",
       "  'page',\n",
       "  'primitive',\n",
       "  'queer',\n",
       "  'roots',\n",
       "  'second world',\n",
       "  'slave'],\n",
       " 'q4': ['black',\n",
       "  'colored',\n",
       "  'descent',\n",
       "  'dwarf',\n",
       "  'ethnic groups',\n",
       "  'homo',\n",
       "  'indian',\n",
       "  'indigenous',\n",
       "  'indo',\n",
       "  'moor',\n",
       "  'native',\n",
       "  'oriental',\n",
       "  'race',\n",
       "  'servant',\n",
       "  'traditional',\n",
       "  'trans',\n",
       "  'tribe',\n",
       "  'western',\n",
       "  'white']}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_get_lemma_quartiles(\"en\",\"/Users/anesterov/reps/LODlit/Wikidata/n_hits_by_lemma.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
